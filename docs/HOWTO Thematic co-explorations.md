# HOWTO Thematic co-exploration

Next, the set of steps that should be in carried in a logical sequence to co-deliver a thematic co-exploration are listed. Importantly, there could be different iterations over some steps. The process indicated below should not be understood as a one way top down process. 

## #0: Define research questions/hypothesis to be validated to enable policy-making

#### Responsible
Pilot Owners

#### Objective
The primary objective of this step is to formulate precise research questions and hypotheses that are anchored in the needs and priorities of policy-making. These questions and hypotheses should be actionable, measurable, and capable of being empirically tested within the project's scope. They should be designed to fill knowledge gaps, address current challenges, and generate evidence to translate into effective policy interventions. 

The process will involve a thorough analysis of the current policy landscape, consultation with stakeholders, and a review of the existing literature to ensure that the research is targeted and relevant. This foundational step sets the stage for the subsequent research and data collection efforts, ensuring that the project outcomes align with the strategic goals of influencing and enabling policy-making.

#### How can we achieve it?
Utilising tools like the Data Quality Dashboard for data analysis and the CE(Collaborative Environment), we can identify gaps in current research and align our inquiries with the pressing policy development needs. In this way, the pilot ensures a standardised approach to gathering comprehensive feedback.

After that, an interdisciplinary team will review the stakeholder input alongside a thorough literature review conducted using our project's repository of resources. This team will employ analytical tools mentioned in our guidelines, like data visualisation software and documentation tools, to synthesise insights and frame them into research questions and hypotheses. The usability and evaluation exercises documented in our repository will be instrumental in refining these questions to ensure they are clear, measurable, and ethically sound.

#### Resources
 - [Collaborative Environment](https://greengage-project.github.io/Documentation/tools/collaborativeEnvironment) --> to have a centralised repository of resources and a collaborative space for the team to work together in the first steps of the thematic co-exploration
 - [Thematic Co-Exploration for Citizen Observatory (COb) Specification Template](https://aitonline.sharepoint.com/:w:/r/sites/HEUGREENGAGE337/_layouts/15/Doc.aspx?sourcedoc=%7BB2395027-626C-44CC-98DC-124E9DC5750B%7D&file=ThematicCoExplorationSpec.docx&action=default&mobileredirect=true) --> to specify the thematic co-exploration
 - Metrics and insights foresight (TBD) --> 


## #1: Preparation of the co-production process, including the configuration of COb's community teams

#### Responsible
Pilot Owners

#### Objective
The primary objective of this step is to establish a robust and effective co-production process that fosters collaborative engagement between various stakeholders within the Citizen Observatory (COb). This involves the strategic configuration of community teams to ensure a diverse representation of perspectives, skills, and expertise. The aim is to create an environment where community members, experts, and stakeholders can collectively contribute to the project, leveraging their unique insights and experiences. This process is designed to facilitate the seamless integration of community input into the project and empower participants by providing them with the tools, training, and support necessary for meaningful participation. The ultimate goal is to enhance the quality, relevance, and impact of the project outcomes through a participatory approach that values and utilises the collective knowledge and capabilities of the COb community teams.

#### How can we achieve it?
To facilitate discussions and community engagement, Discourse will be integrated, providing a modern and user-friendly forum for exchange and collaboration. In parallel, WordPress will serve as our content management system, offering a versatile platform for information dissemination and interaction with the broader community. Tools like MODE, Apache Superset, Apache Druid, and Apache NiFi will be crucial for data collection and analysis, enabling us to handle vast amounts of data efficiently and generate actionable insights. DataHub will act as our centralised metadata platform, democratising data discovery and management within the project. Additionally, the integration of Libelium MQTT sensors will bolster our data collection capabilities, particularly in environmental monitoring.

#### Resources
 - [Discourse](https://greengage-project.github.io/Documentation/tools/Discourse/) --> necessary to foster discussions among CObs and allow them to request help and support from each other 
 - [WordPress](https://greengage-project.github.io/Documentation/tools/wordpress/) --> necessary to disseminate information and interact with the broader community
 - [Problem Statement Questionnaire](https://aitonline.sharepoint.com/:x:/r/sites/HEUGREENGAGE337/_layouts/15/Doc.aspx?sourcedoc=%7B2A0A5854-8F2F-41A8-821B-4ED300A86BE2%7D&file=ProblemStatementQuestionnaire.xlsx&action=default&mobileredirect=true) --> to identify the problems that the pilot will address


## #2: Inventory of available datasets and data gaps identification to explore a problem domain

#### Responsible:
Pilot Owners

#### Objective
The primary goal of this step is to establish a comprehensive and organised system for managing datasets relevant to our project. By requiring pilot owners to register existing and forthcoming datasets, we aim to create a centralised metadata repository that will serve as a foundational resource for the technical team. This registration process will facilitate the subsequent uploading and integration of these datasets into the project's infrastructure. The overarching objective is to develop a detailed and accessible catalogue of data, which will streamline project operations and enhance the efficacy of data analysis and decision-making processes. This initiative is instrumental in ensuring that all stakeholders have a clear and consistent understanding of the available resources, thereby optimising the utilisation and impact of the data within the project's scope.

#### How can we achieve it?  
To achieve the following objective, Pilot Owners should comprehensively identify all existing and potential datasets within the scope of the project. This step aims to create an inventory of datasets, categorising them based on relevance, data type, source, and intended use. Furthermore, it is essential to specify how each dataset may be retrieved and accessed. This initial inventory will serve as the foundation for the subsequent registration process in DataHub. 

#### Resources
 - [DataHub](https://greengage-project.github.io/Documentation/tools/datahub/) --> to see what datasets are already available in the catalogue of such Citizen Observatory
 - [Dataset Inventory](https://aitonline.sharepoint.com/:x:/r/sites/HEUGREENGAGE337/_layouts/15/Doc.aspx?sourcedoc=%7B632ec461-1851-463e-87de-01eedb9d2263%7D&action=edit&wdinitialsession=c2406669-913f-4a32-b5b5-3334b3429bd4&wdrldsc=2&wdrldc=1&wdrldr=OnSaveAsWebMethodComplete) --> to identify the datasets that will be needed for the thematic co-exploration. Make a copy of the template and fill it.


## #3: Co-design of thematic co-exploration to address the defined research questions
#### Responsible
Pilot Support Team

#### Objective 
(TODO: indicate the different questions that need to be answered base on the template of the resource, including even METRICS)
The core objective of co-designing the thematic co-exploration is to establish a structured and engaging framework that directly aligns with the defined research questions and hypotheses. This approach aims to actively involve a diverse citizen group in data collection, observation, and analytical processes, ensuring that their contributions are impactful and directly address the core research queries. Central to this initiative is the implementation of cutting-edge digital tools, facilitating efficient data collection, management, and analysis, thereby aligning citizen contributions with the overarching goals of the project. Furthermore, the campaign is designed to foster a community-driven approach, significantly enhancing public understanding and participation in scientific research while promoting a collaborative ethos in problem-solving. This strategy is intended to leverage collective efforts and insights, thereby facilitating a comprehensive and inclusive approach to addressing the project's research objectives.

#### How can we achieve it?
To realize these objectives, we will integrate the Collaborative Environment (CE) for enhanced interaction and management of the campaign's activities. Platforms like Discourse will be used for community engagement and open communication, facilitating a structured and inclusive dialogue among participants. For data collection, tools such as MODE for GPS-based data and Libelium MQTT sensors for environmental monitoring will be employed. The data collected will be organized and analyzed using DataHub, Apache Superset, and Apache Druid for advanced data visualization and analytics. Apache NiFi will be instrumental in ensuring a streamlined and efficient data processing pipeline. The synergy of these tools and platforms, as specified in our technical guide, will ensure a comprehensive, secure, and effective Citizen Science campaign, addressing the research questions while empowering citizen participants.

#### Resources
 - [Collaborative Environment](https://greengage-project.github.io/Documentation/tools/collaborativeEnvironment/) --> to follow the process defined, have a centralised repository of resources and a collaborative space for the team to work together in the first steps of the thematic co-exploration
 - [CS-CO-schema-process-specification](https://aitonline.sharepoint.com/:x:/r/sites/HEUGREENGAGE337/_layouts/15/Doc.aspx?sourcedoc=%7B3D2E1BC5-96E9-4C2F-AC70-DAD6DD88D83C%7D&file=CS-CO-schema-process-specification.xlsx&action=default&mobileredirect=true&DefaultItemOpen=1) --> to specify the process of the thematic co-exploration
 - [Thematic Co-Exploration for Citizen Observatory (CO) Specification Template](https://aitonline.sharepoint.com/:w:/r/sites/HEUGREENGAGE337/_layouts/15/Doc.aspx?sourcedoc=%7BB2395027-626C-44CC-98DC-124E9DC5750B%7D&file=ThematicCoExplorationSpec.docx&action=default&mobileredirect=true) --> to specify the process of the thematic co-exploration


## #4: Invitation, onboarding and training of selected users to take part in aCitizen Observatory’s thematic co-exploration

#### Responsible
Pilot Support Team

#### Objective
The objective of this stage is the collaborative design of a thematic co-exploration aimed at effectively addressing the research questions identified earlier. This campaign is structured to engage and empower citizens, enabling their active involvement in data collection, observation, and analysis. It strives to ensure that contributions from citizens are meaningful and pivotal in addressing key research questions. Central to this initiative is the utilisation of innovative digital tools and platforms, which are crucial for efficient data collection, management, and analysis. A significant goal is to foster a community spirit and shared purpose among participants, thereby enhancing their understanding and engagement in scientific research. This approach is expected to yield reliable and high-quality data, instrumental in influencing policy-making, supporting community initiatives, and furthering scientific inquiries. Additionally, the campaign is designed to be scalable, replicable, and adaptable to a variety of contexts and research domains, setting a model for future citizen science endeavours.


#### How can we achieve it?
To achieve the objectives, the Collaborative Environment will be the key to enhanced collaboration and asset management and will be instrumental in co-ordinating the efforts of different stakeholders and managing campaign assets effectively. For community engagement and discussion, Discourse will serve as our primary platform, fostering a space for open, structured communication among participants.

Data collection and analysis will be powered by MODE, enabling us to gather detailed GPS-based transportation data and Libelium MQTT sensors for environmental data capture. The data thus collected will be organized and made accessible through DataHub, while Apache Superset and Apache Druid will be utilized for their advanced data visualization and real-time analytics capabilities, ensuring that insights drawn from the data are both meaningful and actionable. Apache NiFi will streamline data ingestion and processing, ensuring a smooth flow of data through our systems. The integration of these tools, as outlined in our technical guide, will ensure a seamless, efficient, and secure approach to co-designing a thematic co-exploration that effectively addresses the research questions.

#### Resources
 - Folder to where trainings are currently uploaded, in time they will be part of GREENGAGE Academy (TBD)
 - Knowledge base in GREENGAGE web page: https://www.greengage-project.eu/ (TBD)


## #5: Management of data workflow process for the thematic co-exploration

#### Responsible
Technical Team & PST

#### Objective
This step aims to establish a robust and efficient data management process for the Citizen Science campaign. This process will involve the planning of data collection based on the data identified in *Step 2*. The data management plan should depict the data collection process for each dataset, the stakeholders involved in their collection, and the tools and platforms used. Furthermore, the plan should include the workflow that will be followed for each dataset, from collection to analysis and visualisation. Through this process, the technical team and the PST will have a well-defined and structured plan for each dataset, facilitating the subsequent steps.

#### How can we achieve it?
PST and the technical team should work together to fulfil a template that defines the data management plan for each dataset in the data inventory realised in *Step 2*. 

#### Resources
 - [Data Management Plan Template](https://aitonline.sharepoint.com/:x:/r/sites/HEUGREENGAGE337/Shared%20Documents/WP4%20CO%20enabling%20infrastructure%20and%20interoperable/D4.1%20GREEN%20Engine%20and%20manuals/Academy/resources/Libro.xlsx?d=w0a19b821efd94d55ba8f50d10bd6fd17&csf=1&web=1&e=XEUvCF) --> to define the data management plan for each dataset
 - Data Management plan of GREENGAGE (TBD)


## #5a: Gather Copernicus and Open Data from government datasets to gain bird's eye view for thematic co-exploration

#### Responsible
Technical Team

#### Objective
This step aims to retrieve the datasets identified in *Step 2* that are already available in open data portals. This process will involve the creation of ingestion sources for each dataset in DataHub, which will allow us to retrieve the data from the source and store it in the centralised repository. The technical team should create the ingestion sources with the pilot owners' support.

#### How can we achieve it?
This step can be achieved in many ways depending on the data source to be retrieved. The ingestion sources can be created using Apache NiFi, allowing us to retrieve data from APIs, FTP servers, and other sources. Alternatively, other ingestion sources, such as Python scripts, may require programmatic solutions. Finally, the ingestion sources can be created using Apache Druid, allowing us to load data directly into the project's database. The technical team will be responsible for creating the ingestion sources, while the pilot owners will be responsible for providing the necessary information.

#### Resources
 - HOWTOs to gather data from Copernicus and from Open Data portals considered  in the project (TBD)
 - [Apache NiFi official documentation](https://nifi.apache.org/docs.html) --> to create ingestion sources
 - [Apache NiFi GREENGAGE documentation](https://greengage-project.github.io/Documentation/tools/nifi/) --> to create ingestion sources
 - [Apache Druid official documentation](https://druid.apache.org/docs/latest/) --> to load data directly into the project's database
 - [Apache Druid GREENGAGE documentation](https://greengage-project.github.io/Documentation/tools/druid/) --> to load data directly into the project's database


## #5b: Organise and gather crowdsourcing of data by citizens through data capture apps

#### Responsible
Pilot Owners

#### Objective
This step aims to design and implement a citizen science campaign to collect public data identified in the *Step 2* that is not already available. This process will involve the creation of campaigns that include missions (tasks) to be completed by the public. The missions will be designed by the pilot owners, with the support of the technical team, to retrieve specific data. The missions, thus, the data capture process, will involve using specific tools designed for this task, such as MODE, GREENGAGE app or MindEarth. 

#### How can we achieve it?
First, the pilot owners should identify and design the missions that citizens will have to complete to collect the data. Then, together with the technical team, they will identify the tools that will be used for the data capture process. Once the tools are identified, configured and citizens trained to use them (allegedly carried out in *Step X*), the campaigns should be launched and promoted. During the operation of the campaigns, the PST should monitor their progress and performance, and the technical team should ensure that the data is being collected and stored correctly. Apps in the GREEN Engine, such as MODE, GREENGAGE app or MindEarth, will be used to collect the data. 

#### Resources
 - [COb Resources' List ](https://aitonline.sharepoint.com/:w:/r/sites/HEUGREENGAGE337/_layouts/15/Doc.aspx?sourcedoc=%7B7C6E2319-112E-498C-A90E-8286CC147E9D%7D&file=ListResourcesNeeded.docx&action=default&mobileredirect=true) --> to identify the resources needed 
 - [MODEs GREENGAGE documentation](https://greengage-project.github.io/Documentation/tools/mode/) --> to collect commuting data
 - [MindViews GREENGAGE documentation](https://greengage-project.github.io/Documentation/tools/mindview/) --> to collect geo-tagged street-level imagery
 - [GRENGAGE apps GREENGAGE documentation](https://greengage-project.github.io/Documentation/tools/greengage-app-api/) --> to collect data via campaigns
 - AtmoTube page HOPU and Noisetube (TBD)


## #6: Cataloguing of captured data

#### Responsible
Technical Team

### Objective
This step aims to create a centralised repository of metadata for all datasets within the scope of the CO. This repository will serve as a foundational resource for the stakeholders working in the CO, enabling them to manage and analyse the data efficiently. The process will involve the registration of datasets in DataHub, a centralised metadata platform that will act as a single source of truth for all project data. Furthermore, DataHub allows us to add documentation and metadata for each dataset catalogued this way.

### How can we achieve it?  
Once the datasets are identified, the next step is registering them in DataHub. The technical team will conduct this process with the support of the pilot owners. The registration process will involve the creation of a metadata file for each dataset, which will be uploaded to DataHub. The metadata file will include information such as the dataset's name, description, source, and intended use. The technical team will be responsible for creating the ingestion sources for DataHub, while the pilot owners will be responsible for providing the necessary information.

#### Resources
 - [Documentation of DataHub](https://datahubproject.io/docs/) --> to register datasets in DataHub
 - Video of how to create datahub sources (TBD)
 - Druid (TBD)


## #6: Generation of data analysis workflows (Data cleaning, pipelining and analysis)

#### Responsible: 
Technical Team

#### Objective
The objective of this step is a critical phase in the data management process, primarily focused on ensuring the quality and usability of datasets within the project's framework. This stage involves a series of preprocessing steps designed to cleanse the datasets, removing any inconsistencies, errors, or irrelevant information that might hinder analysis. Since some datasets are updated daily, it's essential to establish automated data ingestion workflows, using tools like Apache NiFi, to streamline the continuous flow of data into the main database, Apache Druid. This approach facilitates the efficient and reliable updating of data and ensures that the datasets are in an optimal format and structure for analysis. The ultimate goal of this process is to enhance data accuracy and integrity, thereby enabling a deeper and more accurate understanding of the data, which is vital for informed decision-making and insightful analysis within the project.

#### How can we achieve it?
To successfully create workflows that can process the data into the desired format we have several ways to do it: creating a NiFi Flow, creating a python script or manually.
 - **NiFi flow:** Apache NiFi is a powerful tool that allows us to create data pipelines that can be scheduled to run periodically. This tool is very useful for data ingestion, as it allows us to fetch data from APIs, FTP servers, and other sources. Additionally, NiFi can be used to clean and transform data, ensuring that it is in the desired format and structure. Finally, NiFi can be used to load data into Apache Druid, ensuring that the data is correctly ingested into the database.
 - **Python script:** Develop Python scripts for more complex data transformations that might not be directly feasible within NiFi. This can include data cleaning, normalization, or feature engineering tasks. It can also be integrated with Apache NiFi to run as part of the flow. Furthermore, by using the Apache Druid API we can directly load data into the database.
 - **Manually:** For simple data transformations, it might be more efficient to perform them manually. This can include tasks such as renaming columns, changing data types, or removing columns. However, this approach is not scalable and should only be used for simple tasks.

 Once the data is preprocessed and cleaned the analysis may be conducted by combining and exploring the data using Apache Superset and Apache Druid. Apache Superset is a powerful tool that allows us to create interactive dashboards and visualizations that will be explained in the next step. Apache Druid allows us to perform real-time analytics on large datasets. These tools will be used to explore the data and generate insights that will be used to answer the research questions.

### Resources
 - Data quality VRVIS (TBD)
 - [Apache NiFi documentation](https://nifi.apache.org/docs.html) --> to create preprocessing flows
 - [Apache Druid documentation](https://druid.apache.org/docs/latest/) --> to load data into the project's database
 - [Apache Superset documentation](https://superset.apache.org/docs/intro) --> to create dashboards and visualizations


## #7: Generation of thematic co-exploration dashboards

#### Responsible
Technical Team

#### Objective
The objective of generating thematic co-exploration dashboards in the project is to develop interactive and insightful tools that transform complex data into accessible visual formats, enhancing comprehension and insight. These dashboards are designed to enable stakeholders, including policymakers, researchers, and citizens, to delve into various data themes and patterns, fostering an environment of collaborative exploration and dialogue. By providing real-time data monitoring and analysis, these dashboards will be instrumental in supporting quick and informed decision-making and policy development. The overarching goal is to ensure transparency and open access to project data, thereby promoting inclusive participation and a shared understanding among all project participants.

#### How can we achieve it?
To accomplish this, we will leverage tools like Apache Superset and Apache Druid, as outlined in our project documentation. Apache Superset will allow us to create and share rich data visualizations and dashboards, providing an intuitive interface for users to interact with the data. Apache Druid will be utilized for its real-time data analytics capabilities, ensuring that our dashboards can display the most current data efficiently. Additionally, the integration of Apache NiFi will streamline the data flow into these systems, ensuring a consistent and reliable data pipeline. The combination of these tools, supported by thorough documentation and examples of integration, will enable us to create dynamic and insightful thematic co-exploration dashboards that serve the diverse needs of our project stakeholders.

#### Resources
 - [ThematicCoExplorationSpec](https://aitonline.sharepoint.com/:w:/r/sites/HEUGREENGAGE337/_layouts/15/Doc.aspx?sourcedoc=%7BB2395027-626C-44CC-98DC-124E9DC5750B%7D&file=ThematicCoExplorationSpec.docx&action=default&mobileredirect=true) --> to revisit the objectives of the thematic co-exploration and generate dashboards that address them
 - Data quality VRVIS (TBD)
 - [Apache Superset documentation](https://superset.apache.org/docs/intro) --> to create dashboards and visualizations
 - HOWTO GISAT tool to do these visualizatons (TBD)


## #8: Generation of storylines for wide dissemination

#### Responsible
Pilot Coordinators

#### Objective
The objective in the "Generation of storylines for wide dissemination" phase is to craft engaging and accessible narratives that convey the project's key findings and insights to a broad audience. These storylines are designed to translate complex data and research outcomes into formats that are both engaging and easily understandable by the public. The aim is to underscore the project's significance and its potential impact on policy-making, community initiatives, and scientific research. By doing so, the goal is to foster increased public awareness and interest in the project, thereby enhancing its overall reach and influence. To achieve wide visibility and impact, these storylines will be disseminated through various media channels, tailored to resonate with diverse audience segments, thereby ensuring the project's findings are communicated effectively and broadly.

#### How can we achieve it?
To effectively generate storylines for wide dissemination, we will utilize a range of tools specified in our project documentation. The Communications and Outreach Team will collaborate closely with the Data Analysis Team, utilizing Apache Superset for creating compelling data visualizations that bring complex data to life in an easily digestible format. These visual elements will be integral in crafting storylines that are both informative and engaging to the public. Additionally, we will leverage WordPress and Discourse platforms to draft, refine, and share these narratives, ensuring they are tailored to resonate with diverse audience segments. The integration of these tools will enable us to effectively communicate the project's impact and findings through various media channels, from social media to press releases, thereby maximizing reach and fostering greater public engagement with the project's outcomes.

#### Resources
 - Revise if there are storyline guidelines in https://zenodo.org/records/10119036 (https://aitonline.sharepoint.com/sites/HEUGREENGAGE337/Shared%20Documents/Forms/AllItems.aspx?id=%2Fsites%2FHEUGREENGAGE337%2FShared%20Documents%2FWP4%20CO%20enabling%20infrastructure%20and%20interoperable%2FD4%2E1%20GREEN%20Engine%20and%20manuals%2FAcademy%2Fresources%2FVUB%5FScience%5FStarterkit%5FFinaal%202%2Epdf&viewid=8d5e58b2%2Da28d%2D4eae%2Dbac9%2D69afab2f27ff&parent=%2Fsites%2FHEUGREENGAGE337%2FShared%20Documents%2FWP4%20CO%20enabling%20infrastructure%20and%20interoperable%2FD4%2E1%20GREEN%20Engine%20and%20manuals%2FAcademy%2Fresources) create new template to upload into REPO


## #9: Generation of policy briefs' inputs for policy makers

#### Responsible
Policy Analysis Team

#### Objective
The generation of policy briefs for policymakers, a key objective of this project step, involves synthesizing research findings and data into clear, concise, and actionable recommendations. These briefs are crafted to effectively communicate pivotal insights and findings to policymakers, providing them with evidence-based guidance for policy interventions. By directly aligning with current policy debates and challenges, these briefs aim to bridge the gap between research and practical policymaking, offering solutions that are grounded in thorough data analysis. This approach not only enhances the relevancy of the research to ongoing policy discussions but also aims to significantly influence policy decisions and strategies, thus amplifying the overall impact of the project.

#### How can we achieve it?
To achieve this, we will first aggregate and analyze the data collected and insights generated throughout the project, using tools like Apache Druid for real-time analytics and Apache Superset for data visualization. These tools will help in identifying key trends and findings that are most relevant to policy-making. The policy analysis team will then work closely with research teams to distill these insights into policy briefs, ensuring that they are both data-driven and aligned with current policy needs. Communication specialists will refine these briefs to be clear and compelling, making them accessible to non-specialist policymakers. Throughout this process, project coordinators will ensure that the briefs align with the strategic objectives of the project and effectively convey its potential impact on policy and governance.


#### Resources
 - [Collaborative Environment](https://greengage-project.github.io/Documentation/tools/collaborativeEnvironment/) --> to collaborate in the policy briefs' generation in a centralised repository
 - Revise if there are POLICY BRIEFS guidelines in https://zenodo.org/records/10119036  (TBD)
