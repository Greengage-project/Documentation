{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Technical Guide for GREENGAGE Project Integration Project The GREENGAGE project is a three-year-long pan-European initiative aimed at strengthening urban governance through citizen-based co-creation and crowd-sourcing. Funded under the Horizon Europe Framework Programme and co-funded by the UK government and Switzerland's State Secretariat for Education, Research and Innovation, this project is led by the Austrian Institute of Technology and comprises 17 research and industry partners from the EU and the UK. The project seeks to encourage innovative governance processes and assist public authorities in formulating climate mitigation and adaptation policies. By leveraging citizen participation and providing innovative digital solutions, GREENGAGE lays the groundwork for co-creating and co-designing innovative solutions to monitor environmental issues at the grassroots level. The developed Citizen Observatories (CO) are intended to be further integrated into urban planning decision-making processes. The project encompasses campaigns in four pilot cities across Europe, focusing on mobility, air quality, and healthy living, with citizens encouraged to observe and co-create data solutions for urban environments through Citizen Observatories. Tools The tools directory contains subdirectories for each tool utilized in the project. Each subdirectory includes a detailed description of the tool, its functionality, and its role within the project. Keycloak Directory : /tools Link : tools/keycloak Deployed in : https://auth1.demo.greengage-project.eu Description : Keycloak is an open-source Identity and Access Management solution aimed at modern applications and services. It facilitates the secure transmission of identity information between applications and services, even across different domains. Examples of programmatic integration/usage : Node Python Collaborative Environment Directory : /tools Link : tools/collaborativeEnvironment Deployed in : https://demo.greengage-project.eu Description : The Collaborative Environment is a comprehensive platform designed to facilitate co-production, asset management, and enhanced collaboration among users. It incorporates various interlinkers and modules to streamline workflows and improve organizational processes, supporting integration with different technologies and frameworks. Examples of programmatic integration/usage : Deploy example MODE Directory : /tools Link : tools/mode Description : This module allows to determine the transport mode of a user based on GPS data provided. Examples of programmatic integration/usage : iOS example Android example Backend example Discourse Directory : /tools Link : tools/Discourse Deployed in : https://discourse.greengage-project.eu Description : Discourse is a modern, open-source discussion platform designed for building and managing community forums. Known for its user-friendly interface and robust features, it supports real-time discussions, multimedia integration, and comprehensive moderation tools. Ideal for a wide range of communities, from small groups to large enterprises, Discourse fosters engagement, knowledge sharing, and collaboration. Examples of programmatic integration/usage : Deploy example DataHub Directory : /tools Link : tools/DataHub Deployed in : https://datahub.greengage-project.eu/ Description : DataHub is an open-source metadata platform designed to democratise data discovery. It acts as a centralised system for companies and teams to track, manage, and find data within their organisation. Its primary aim is to streamline the process of accessing and understanding data assets, making it easier for teams to derive insights and make informed decisions. Examples of programmatic integration : Integration example Apache Superset Directory : /tools Link : tools/Superset Deployed in : https://superset.greengage-project.eu/ Description : Apache Superset is a sophisticated, open-source data exploration and visualisation platform designed to transform the way analysts and businesses interact with their data. As a versatile web application, it empowers users with tools for deep data exploration and the creation of beautifully rendered visualisations and dashboards. Superset is renowned for its user-friendly interface, democratising data analysis by enabling users of all skill levels to engage with complex datasets, design compelling visual narratives, and derive actionable insights. Offering a rich array of features, including a variety of chart types, interactive dashboards, and an integrated SQL editor, Apache Superset stands out as a comprehensive solution for modern data analytics and business intelligence needs. Examples of integration of sources with Superset : Integration example Apache Druid Directory : /tools Link : tools/Druid Deployed in : https://druid.greengage-project.eu Description : Apache Druid is a high-performance, distributed data store designed for real-time analytics on large-scale datasets. It excels in scenarios where fast queries and ingest are crucial, such as in business intelligence applications, operational analytics, and real-time monitoring. Druid is known for its ability to handle high concurrency, its columnar storage format, and its ability to scale horizontally. It supports streaming and batch data ingestion, and its architecture allows for efficient data summarisation, fast aggregation queries, and low-latency data access. Druid is often used in conjunction with big data technologies like Hadoop and Kafka, making it a popular choice for organisations looking to implement real-time analytics solutions. Examples of integration of sources with Druid : Integration example Apache NiFi Directory : /tools Link : tools/NiFi Deployed in : https://nifi.greengage-project.eu/nifi Description : Apache NiFi is a robust, open-source software solution designed for efficient and reliable data ingestion, processing, and distribution. It is particularly noted for its user-friendly graphical interface that simplifies the design, management, and monitoring of data flows. NiFi supports highly configurable and flexible data routing, system mediation, and transformation capabilities, enabling the handling of diverse data formats and systems. Key features include its scalability to accommodate large volumes of data, built-in data security measures, and extensive audit trails for data provenance. Primarily used in scenarios requiring complex data pipelines, such as real-time data processing and integration across varied systems, Apache NiFi stands out for its ease of use and comprehensive data management capabilities. Examples of integration of sources with NiFi : Integration example Libelium MQTT sensor integration Directory : /tools Link : tools/sensorsIntegration Description : This documentation contains all the information necessary to connect and configure the IoT sensors provided by Libelium to MQTT broker and integrate it in other platforms or architectures. The above README provides a structured outline to document the technical aspects of the GREENGAGE project, offering a solid starting point to further elaborate on the tools and processes involved in the project. Greengage APP Directory : /tools Link : tools/greengage-app-api Deployed in : api-stage.greengage.dev/graphql Description : The greengage app api is mesh of multiple services which are provided via a single gateway. This underlying docuemntation will show you in a simple way how you can interact with application and how you can adopt your application to fit in the ecosystem. UrbanTEP / VISAT Directory : /tools Link : tools/urbantep-visat Deployed in : visat-sdg.urban-tep.eu Description : VISAT (Visualization and Analytical Toolbox) is a powerful tool provided by GISAT within the Urban Thematic and Exploration Platform (UrbanTEP). It is designed for the seamless integration, exploration, visualization, and analysis of diverse datasets, particularly geospatial data. As part of the GREENGAGE project, VISAT is integral to supporting pilot solutions with its data integration, visualization, and analysis capabilities, enhancing the efficiency of Citizen Observatory projects. Wordpress Directory : /tools Link : tools/wordpress Description : WordPress is a highly popular and versatile Content Management System (CMS) renowned for its ease of use and flexibility. It's an ideal platform for creating a wide range of websites, from personal blogs to large corporate portals. WordPress is known for its extensive theme and plugin ecosystem, which allows users to customize their sites to fit their specific needs and enhance functionality. Examples of programmatic integration/usage : With Keycloak With Discourse MindEarth Directory : /tools Link : tools/mindview Deployed in : Google Play Description : The MindEarth platform is an advanced system designed for real-time data collection and analysis, particularly suited to urban planning, environmental monitoring, and commercial research projects. It allows third-party organizations to commission missions for users to perform, which may involve taking photographs, traveling specific routes, or staying in a certain location for a set amount of time. The platform supports seamless integration with mobile devices and backend services, ensuring high scalability and flexibility. Examples of programmatic integration/usage : With Third-Party Services Data Quality Dashboard Directory : /tools Link : tools/DataQualityDashboard Deployed in : qualitydashboard.greengage-project.eu Description : The Data Quality Dashboard is a web-based platform designed to monitor and visualize the quality of collected data. It provides users with powerful tools to identify, analyze, and resolve data quality issues. By offering both visual and numeric representations of key metrics such as completeness, accuracy, and consistency, the dashboard helps users quickly pinpoint potential issues in their datasets. The platform supports various data exploration features, including filtering by time, location, and other parameters, making it ideal for applications like environmental monitoring, urban planning, and more. Source code in: https://github.com/Greengage-project/Documentation","title":"Home"},{"location":"#technical-guide-for-greengage-project-integration","text":"","title":"Technical Guide for GREENGAGE Project Integration"},{"location":"#project","text":"The GREENGAGE project is a three-year-long pan-European initiative aimed at strengthening urban governance through citizen-based co-creation and crowd-sourcing. Funded under the Horizon Europe Framework Programme and co-funded by the UK government and Switzerland's State Secretariat for Education, Research and Innovation, this project is led by the Austrian Institute of Technology and comprises 17 research and industry partners from the EU and the UK. The project seeks to encourage innovative governance processes and assist public authorities in formulating climate mitigation and adaptation policies. By leveraging citizen participation and providing innovative digital solutions, GREENGAGE lays the groundwork for co-creating and co-designing innovative solutions to monitor environmental issues at the grassroots level. The developed Citizen Observatories (CO) are intended to be further integrated into urban planning decision-making processes. The project encompasses campaigns in four pilot cities across Europe, focusing on mobility, air quality, and healthy living, with citizens encouraged to observe and co-create data solutions for urban environments through Citizen Observatories.","title":"Project"},{"location":"#tools","text":"The tools directory contains subdirectories for each tool utilized in the project. Each subdirectory includes a detailed description of the tool, its functionality, and its role within the project.","title":"Tools"},{"location":"#keycloak","text":"Directory : /tools Link : tools/keycloak Deployed in : https://auth1.demo.greengage-project.eu Description : Keycloak is an open-source Identity and Access Management solution aimed at modern applications and services. It facilitates the secure transmission of identity information between applications and services, even across different domains. Examples of programmatic integration/usage : Node Python","title":"Keycloak"},{"location":"#collaborative-environment","text":"Directory : /tools Link : tools/collaborativeEnvironment Deployed in : https://demo.greengage-project.eu Description : The Collaborative Environment is a comprehensive platform designed to facilitate co-production, asset management, and enhanced collaboration among users. It incorporates various interlinkers and modules to streamline workflows and improve organizational processes, supporting integration with different technologies and frameworks. Examples of programmatic integration/usage : Deploy example","title":"Collaborative Environment"},{"location":"#mode","text":"Directory : /tools Link : tools/mode Description : This module allows to determine the transport mode of a user based on GPS data provided. Examples of programmatic integration/usage : iOS example Android example Backend example","title":"MODE"},{"location":"#discourse","text":"Directory : /tools Link : tools/Discourse Deployed in : https://discourse.greengage-project.eu Description : Discourse is a modern, open-source discussion platform designed for building and managing community forums. Known for its user-friendly interface and robust features, it supports real-time discussions, multimedia integration, and comprehensive moderation tools. Ideal for a wide range of communities, from small groups to large enterprises, Discourse fosters engagement, knowledge sharing, and collaboration. Examples of programmatic integration/usage : Deploy example","title":"Discourse"},{"location":"#datahub","text":"Directory : /tools Link : tools/DataHub Deployed in : https://datahub.greengage-project.eu/ Description : DataHub is an open-source metadata platform designed to democratise data discovery. It acts as a centralised system for companies and teams to track, manage, and find data within their organisation. Its primary aim is to streamline the process of accessing and understanding data assets, making it easier for teams to derive insights and make informed decisions. Examples of programmatic integration : Integration example","title":"DataHub"},{"location":"#apache-superset","text":"Directory : /tools Link : tools/Superset Deployed in : https://superset.greengage-project.eu/ Description : Apache Superset is a sophisticated, open-source data exploration and visualisation platform designed to transform the way analysts and businesses interact with their data. As a versatile web application, it empowers users with tools for deep data exploration and the creation of beautifully rendered visualisations and dashboards. Superset is renowned for its user-friendly interface, democratising data analysis by enabling users of all skill levels to engage with complex datasets, design compelling visual narratives, and derive actionable insights. Offering a rich array of features, including a variety of chart types, interactive dashboards, and an integrated SQL editor, Apache Superset stands out as a comprehensive solution for modern data analytics and business intelligence needs. Examples of integration of sources with Superset : Integration example","title":"Apache Superset"},{"location":"#apache-druid","text":"Directory : /tools Link : tools/Druid Deployed in : https://druid.greengage-project.eu Description : Apache Druid is a high-performance, distributed data store designed for real-time analytics on large-scale datasets. It excels in scenarios where fast queries and ingest are crucial, such as in business intelligence applications, operational analytics, and real-time monitoring. Druid is known for its ability to handle high concurrency, its columnar storage format, and its ability to scale horizontally. It supports streaming and batch data ingestion, and its architecture allows for efficient data summarisation, fast aggregation queries, and low-latency data access. Druid is often used in conjunction with big data technologies like Hadoop and Kafka, making it a popular choice for organisations looking to implement real-time analytics solutions. Examples of integration of sources with Druid : Integration example","title":"Apache Druid"},{"location":"#apache-nifi","text":"Directory : /tools Link : tools/NiFi Deployed in : https://nifi.greengage-project.eu/nifi Description : Apache NiFi is a robust, open-source software solution designed for efficient and reliable data ingestion, processing, and distribution. It is particularly noted for its user-friendly graphical interface that simplifies the design, management, and monitoring of data flows. NiFi supports highly configurable and flexible data routing, system mediation, and transformation capabilities, enabling the handling of diverse data formats and systems. Key features include its scalability to accommodate large volumes of data, built-in data security measures, and extensive audit trails for data provenance. Primarily used in scenarios requiring complex data pipelines, such as real-time data processing and integration across varied systems, Apache NiFi stands out for its ease of use and comprehensive data management capabilities. Examples of integration of sources with NiFi : Integration example","title":"Apache NiFi"},{"location":"#libelium-mqtt-sensor-integration","text":"Directory : /tools Link : tools/sensorsIntegration Description : This documentation contains all the information necessary to connect and configure the IoT sensors provided by Libelium to MQTT broker and integrate it in other platforms or architectures. The above README provides a structured outline to document the technical aspects of the GREENGAGE project, offering a solid starting point to further elaborate on the tools and processes involved in the project.","title":"Libelium MQTT sensor integration"},{"location":"#greengage-app","text":"Directory : /tools Link : tools/greengage-app-api Deployed in : api-stage.greengage.dev/graphql Description : The greengage app api is mesh of multiple services which are provided via a single gateway. This underlying docuemntation will show you in a simple way how you can interact with application and how you can adopt your application to fit in the ecosystem.","title":"Greengage APP"},{"location":"#urbantep-visat","text":"Directory : /tools Link : tools/urbantep-visat Deployed in : visat-sdg.urban-tep.eu Description : VISAT (Visualization and Analytical Toolbox) is a powerful tool provided by GISAT within the Urban Thematic and Exploration Platform (UrbanTEP). It is designed for the seamless integration, exploration, visualization, and analysis of diverse datasets, particularly geospatial data. As part of the GREENGAGE project, VISAT is integral to supporting pilot solutions with its data integration, visualization, and analysis capabilities, enhancing the efficiency of Citizen Observatory projects.","title":"UrbanTEP / VISAT"},{"location":"#wordpress","text":"Directory : /tools Link : tools/wordpress Description : WordPress is a highly popular and versatile Content Management System (CMS) renowned for its ease of use and flexibility. It's an ideal platform for creating a wide range of websites, from personal blogs to large corporate portals. WordPress is known for its extensive theme and plugin ecosystem, which allows users to customize their sites to fit their specific needs and enhance functionality. Examples of programmatic integration/usage : With Keycloak With Discourse","title":"Wordpress"},{"location":"#mindearth","text":"Directory : /tools Link : tools/mindview Deployed in : Google Play Description : The MindEarth platform is an advanced system designed for real-time data collection and analysis, particularly suited to urban planning, environmental monitoring, and commercial research projects. It allows third-party organizations to commission missions for users to perform, which may involve taking photographs, traveling specific routes, or staying in a certain location for a set amount of time. The platform supports seamless integration with mobile devices and backend services, ensuring high scalability and flexibility. Examples of programmatic integration/usage : With Third-Party Services","title":"MindEarth"},{"location":"#data-quality-dashboard","text":"Directory : /tools Link : tools/DataQualityDashboard Deployed in : qualitydashboard.greengage-project.eu Description : The Data Quality Dashboard is a web-based platform designed to monitor and visualize the quality of collected data. It provides users with powerful tools to identify, analyze, and resolve data quality issues. By offering both visual and numeric representations of key metrics such as completeness, accuracy, and consistency, the dashboard helps users quickly pinpoint potential issues in their datasets. The platform supports various data exploration features, including filtering by time, location, and other parameters, making it ideal for applications like environmental monitoring, urban planning, and more.","title":"Data Quality Dashboard"},{"location":"#source-code-in-httpsgithubcomgreengage-projectdocumentation","text":"","title":"Source code in: https://github.com/Greengage-project/Documentation"},{"location":"CS-loop-GREENGAGE/","text":"Completion of a Citizen Science Loop with GREENGAGE Citizen Science loop in thematic co-explorations within Citizen Observatories Citizen Observatories (COs) empower individuals to actively participate in data collection and environmental monitoring to address local challenges. The GREENGAGE project, under the Horizon Europe framework, aims to enhance the efficacy and more widespread adoption of COs by providing a structured Citizen Science Loop methodology operationalized by a co-production process which is enabled by its GREEN Engine infrastructure. One core contribution brought forward by GREENGAGE is the \u201cthematic co-exploration\u201d concept. A thematic co-exploration, in the context of COs, refers to a collaborative approach where citizens actively participate alongside scientists and other stakeholders in exploring specific themes or topics related to environmental monitoring and observation. Through them, COs are made purposeful by leveraging the collective efforts of individuals, often non-scientists, to gather, share, and analyse environmental data, typically facilitated by digital tools and technologies. This documentation describes the validation of the GREENGAGE co-creation process for thematic co-explorations, through a university campus based thematic co-exploration, which results in the execution of the following 6 steps of a Citizen Science loop, namely: 1. Problem identification \u2013 recognizing research questions or societal challenges suitable for public engagement; 2. Campaign design \u2013 developing participatory protocols, data collection methodologies, and toolkits for citizens\u2019 engagement; 3. Data crowdsourcing \u2013 enabling citizen scientists to gather good quality observations via digital applications, sensors, and surveys, through data crowdsourcing activities; 4. Data analysis & interpretation \u2013 employing AI-driven tools for insight extraction and thus making humanly meaningful the data modelled; 5. Feedback & collective learning \u2013 validating findings with humans and providing participants with actionable feedback; and 6. Action & impact - informing policies, creating solutions, and refining methodologies for future CS campaigns exploring similar or complementary thematic co-explorations. GREENGAGE platform to enable Citizen Observatories These CS loop stages shown above are aligned to the main phases established by GREENGAGE\u2019s co-creation process for thematic co-explorations, which has been devised to organize, execute and exploit the results of CS campaigns. These main phases which compose the GREENGAGE co-creation process, supported by the Collaborative Environment (fully described at HOWTO Thematic co-exploration documentation page, are: - Phase 1 - preparing: fully aligned with the \u201cproblem identification\u201d stage of a Citizen Science loop, comprises the following aspects: a) theme selection; b) pilot owners training; c) core team onboarding and d) core team training. - Phase 2 \u2013 designing: aligned with the \u201ccampaign design\u201d stage of a CS loop, comprising: a) experiment specification; b) tools\u2019 resources selection; c) tools resources customization and d) tools resources testing. - Phase 3 \u2013 experimenting: aligned with both the \u201cdata crowdsourcing\u201d and \u201cdata analysis & interpretation\u201d steps of a CS loop. It comprises the following activities: observers onboarding, observers training support, data collection, data combination, data analysis, data visualization and evaluation. - Phase 4 \u2013 sharing: aligned with \u201cfeedback & learning\u201d and \u201caction & impact\u201d stages of CS loop, comprising the following tasks defined in the GREENGAGE thematic co-exploration process, namely storytelling, policy advocacy and sustainability. Each phase is supported by GREENGAGE\u2019s GREEN Engine infrastructure, named GREEN Engine, fully described at page Citizen Observer journey , which integrates various digital tools and knowledge assets to streamline the co-production process. The tools and knowledge assets created in GREENGAGE are categorized in the following areas of concern, where the names of the tools defined for each layer is indicated: 1. Community and Co-production Process Management: In it, the emphasis is on building a strong, informed, and active community which collaborates through a co-production process by defining a hypothesis, research questions formulation or datasets selection, among others. 2. Data Crowdsourcing and Capture: Based on the groundwork of the previous area of concern, this materialises into concrete data collection activities. It is characterized by active participation, leveraging technology to gather vital curated high quality environmental data. 3. Data Analysis and Insights Generation: In this latter area of concern, the collected data is transformed into actionable insights. This is where the data, once transformed in actionable information, becomes a powerful tool for understanding and influencing environmental policy. The purpose of this documentation is to exemplify how the CS loop is enabled through the suite of tools and knowledge assets defined by GREENGAGE and shown in the below figure. Thus, next section describes how GREENGAGE validates the Citizen Science Loop through a real-world thematic co-exploration at the University of Deusto. Community and Co-production Process Management: Throughout this phase, the emphasis is on building a strong, informed, and active community which collaborates through a co-production process. Data Crowdsourcing and Capture: Based on the groundwork of the previous phase, e.g. definition of hypothesis, research questions formulation or datasets selection, among others, this phase materialises into concrete data collection activities. It is characterized by active participation, leveraging technology to gather vital environmental data. Data Analysis and Insights Generation: In this phase the collected data is transformed into actionable insights. This phase is where the data, once transformed in actionable information, becomes a powerful tool for understanding and influencing environmental policy. Citizen Science loop step-wised co-creation This section showcases the process, tools and results obtained when applying the GREENGAGE CO-enabling approach to a real use-case, namely, \u201creflection on the suitability and air quality of important points of interest (POIs) within the campus of the University of Deusto in Bilbao, Spain\u201d. The following subsections describe the different steps completed towards fulfilling the CS loop for this thematic co-exploration. Notice that to coordinate the execution of this co-creation process, a new process was defined in the Collaborative Environment as shown in Fig. 2. A. CS campaign specification First thing in the organization of a thematic co-exploration, within a Citizen Observatory, is to decide what is the socio-economic and/or environmental challenge that wants to be addressed. There are different aspects that need to be decided at this stage. A useful knowledge asset or CO enabler, as they are called in GREENGAGE, is the thematic co-exploration specification template which is a Word template which guides the organizers of a thematic co-exploration through the following questions: 1. WHY \u2013 reason why this Citizen Observatory\u2019 thematic co-exploration is needed; 2. WHO \u2013 stakeholders involved and affected who need to be recruited for the co-exploration to take place and for the outcomes to be disseminated to; 3. WHAT \u2013 actual endeavours/activities of the Citizen Observatory\u2019s thematic co-exploration towards validating a defined hypothesis and populate a given set of metrics; 4. WHEN \u2013 planning of activities (resources&time) when undertaking the Citizen Observatory\u2019s thematic co-exploration, e.g. crowd-sourcing and data analysis sessions needed; 5. WHERE \u2013 geographical area where Citizen Observatory\u2019s thematic co-exploration will take place, i.e. area to cover and specific points and frequency of measurements which are needed to ensure valuable crowdsourced data; 6. WHICH \u2013 materials and resources, i.e. actual materials, devices and tools needed to execute the Citizen Observatory\u2019s thematic co-exploration, coming either from GREENGAGE or other publicly available tools and assets; 7. HOW \u2013 specification of data analysis processes/workflows to be able to capture, analyse and generate indicators and visualizations sought in Citizen Observatory\u2019s thematic co-exploration. In this stage the needed visualizations and possible storytelling approaches to be eventually adopted are co-specified too. As result, a thematic co-exploration specification for University of Deusto's campus has been produced, where the following decisions were taken: - Set up the observers\u2019 team, in this case 10 researchers associated to the MORElab research group were recruited and committed to be engaged in the whole co-creation process. - Definition of different places (POIs) at the University\u2019s campus (4 were selected) where to gather air quality and place suitability perceptions. These POIs are in the surroundings of the Faculty of Engineering\u2019s building at University of Deusto. - Definition of metrics to estimate air quality and campus space suitability. For that, two new metrics were co-defined by the team of observers in a meeting, namely, a Perception of Air Quality Index (PAQI) was made up, where on a scale from 1 to 5, people have to indicate their perception from very clean (no noticeable pollution effects) to highly polluted (major health concerns, unlivable conditions), and, the Public Space Suitability Index (PSSI) was also made up, where again in a 1 to 5 scale, volunteers have to express their perception regarding aaccessibility & connectivity (20%), safety & security (15%), environmental quality (15%), functionality & comfort (20%), sociability & inclusivity (15%) or maintenance & management (15%) aspects. Again, 5 ranges of suitability were defined ranging from excellent suitability (average answer values >4) to poor suitability (<1). Apart from perceptions of air quality, those volunteers who counted with an Atmotube Pro device also collected air quality data through it. - Definition of tasks to be performed at each selected POI. Firstly, gather a visual perception (photo) and secondly, complete a short 3 question survey for volunteers to express their perception about place suitability and air quality, plus having the chance to leave some feedback about the visited POI. Notice that for the sake of simplicity, one single question per metric was provided to feed the above-mentioned metrics. In a more extensive and thorough real-world thematic co-exploration, one question for each of the factors feeding the devised PSSI should have been included, for instance. Next, the questions designed for the questionnaires to be completed by volunteers at each of the selected POIs are listed: 1. Do you consider that the Air Quality in this spot is? a) very bad; b) bad; c) normal; d) good; e) very good 2. Do you consider that this POI is suited to facilitate campus life and activities? a) not at all; b) not significantly; c) it is OK; d) good enough; e) very good 3. Provide feedback about this POI in terms of its suitability and/or air quality perception. Any suggestions, improvements that you would add? B. Design of the crowdsourcing campaign Once the specification of the thematic co-exploration is ready, the next critical stage is to co-design the CS crowdsourcing campaign \u2013 based on the hypothesis, objectives and data gaps previously identified in thematic co-exploration specification for University of Deusto's campus . In GREENGAGE, the following parameters must be completed in order to set up a CS crowdsourcing campaign: 1. A new instance of an observatory entity is defined, specifying a name for it and the geographical area it covers; 2. A set of POIs within the defined area are defined, for each POI the following fields are defined: type (e.g. culture, nature and so on), description, latitude & longitude, and optional photo; 3. In each POI several tasks can be defined, usually in a campaign the same tasks are required at every measurement point, for each task the following fields are defined: POI associated, topic (e.g. air quality, safety and so on), type (survey, photo, walk and so on), title, description and geocoordinates 4. For tasks of type survey, a survey has be created or an already existing one linked, providing the following fields: title and a set of questions, where for each question a title, type (single choice, multiple choice, text request), and options as pairs of id and values must be defined. The next figure exemplifies the crowdsourcing campaign defined for the thematic co-exploration at Deusto\u2019s campus. Next, it is shown the dashboard that has been defined as back-end of the GREENGAGE app and which can be used to configure CS crowdsourcing campaigns in GREENGAGE. For more details on how to use this interface check GREENGAGE back-end's crowdsourcing campaign configuration dashboard's documentation . C. Collect, extract, transform and load campaign data On Friday 14th March 2025, from 11:30am to 12:30pm CET, a crowdsourcing campaign was executed, where 10 people took part. Notice that volunteers before starting the crowdsourcing campaign they were requested to complete the following actions: They had to log into https://me.greengage-project.eu/ and complete there a sociodemographic form as the one shown in the figure below. Notice that this form requires that each participant specifies her role in the thematic co-exploration, gender, age range, work status, education level and so on. Besides and, very importantly, volunteers must accept a consent form at the bottomo of this page by means of which GREENGAGE is allowed to process their supplied data and aggregate with other participant's sociodemographic data, still preserving the privacy of participants at all time. Only, after they have completed this form, users are allowed to log in into the GREENGAGE app devised to capture data. They had to complete a PRE Impact evaluation questionnaire as the one shown below. Its PDF printout showcases how volunteers in a CS campaign held within GREENGAGE are questionned about environmental, political, scientific and social impact perception before they take part in a thematic co-exploration. They had to download and install the GREENGAGE app from either Android's Google play store or Apple's app store in their smartphones. Notice that the GREENGAGE app also allows users to register as done in step 1, if they do not have credentials, so that they can access into the app. After, these preparation activities, volunteers were ready to launch the GREENGAGE app, as shown below, and use it to collect data. Such campaign was executed by 10 volunteers on Friday 14th March 2025, from 11:30am to 12:30pm CET. Check into POI Answer questionnaire Submit responses Right after concluding the crowdsourcing campaign, volunteers were also requested to complete a POST Impact evaluation questionnaire ) as the one shown below. Its PDF printout showcases how volunteers in a CS campaign held within GREENGAGE are questionned about environmental, political, scientific and social impact perception after they take part in a thematic co-exploration. The crowdsourced campaign data was collected by applying an ETL process. Whilst, the PRE and POST impact and satisfaction questionnaires are hosted in Google Forms, the GREENGAGE app\u2019s collected data is hosted at Apollo Server which exhibits a GraphQL API. This API allows clients (such as mobile apps or web frontends) to efficiently query and interact with campaign data. This interface was used to retrieve all data crowdsourced associated to the POIs and spots generated in the above described thematic co-exploration\u2019s crowdsourcing campaign. Through the Apollo Server front-end, shown below, details in JSON about tasks performed in the crowdsourcing campaign are retrieved by means of a GraphQL query. Notice that GraphQL is a modern API query language and runtime that allows clients to request precisely the data they need in a single request, unlike RESTful APIs which rely on multiple endpoints and fixed data structures, often leading to over-fetching or under-fetching of data. The ETL process corresponding to the crowdsourcing campaign was implemented as a Python script utilizing asynchronous programming patterns to efficiently extract, transform, and load GREENGAGE app\u2019s data. For data extraction, the script interfaces with GREENGAGE app\u2019s GraphQL API endpoint to retrieve mission data, including various mission types (e.g. SURVEY, WALK or DATASET). Additional observatory-related data is collected to provide geographical context for each mission during transform stage of the process. Finally, the script also interfaces with a Keycloak-based authentication service customized for GREENGAGE , which allows extracting participants\u2019 socioeconomic user data. Internally, this extension of KeyCloak has a PostgreSQL database which can be queried through SQL. Such service enables anonymization while preserving demographic information. Through GREENGAGE's identify manager's interface , 10 volunteers completed their sociodemographic details and were granted credential to access to GREEN Engine's tools, including the GREENGAGE app. During transformation, mission data is processed according to type-specific rules. For example, a SURVEY type task, requires additional API calls to retrieve associated quantitative survey values, or GEOTRACKING type mission requires additional API call to retrieve associated GeoJSON object. The transformation phase maps tasks to observatory information, enriches records with anonymized user demographic data, and converts all data to a standardized CSV format with appropriate fields for analysis. This step is crucial because Apache Druid , the real-time analytics database used in the loading phase, requires data to be ingested in a structured format. The deployment of Druid performed for GREENGAGE stores data internally in a columnar format, which optimizes it for fast aggregations and queries. Finally, the load phase utilizes Apache Druid's ingestion API to load the transformed data. The script generates a comprehensive ingestion specification defining data types, dimensions, and granularity settings to optimize subsequent analytical queries. Once ingested, the data becomes immediately queryable through Druid's SQL API, which enables seamless integration with visualization platforms like Apache Superset and other analytics tools. Notice that 3 ETL processes were set up to extract, transform and load data from: 1. Photos gathered through task 2 (see crowdsourcing campaign's spec ) 2. Survey answers associated to the different users and POIs where surveys were responded through task 1 (see crowdsourcing campaign's spec 3. Socio demographic data completed by volunteers when they signed up to take part in the observatory, by means of the https://me.greengage-project.eu page shown at identify manager's interface . As result of these ETL processes, data was stored in Apache Druid infrastructure (see Apache Druid's interface ), which is the storage solution chosen within GREEN Engine. As result of such campaign the following number of measurements were gathered: - 10 people completed a sociodemographic questionnaire which granted them access GREENGAGE app, after a consent form was signed. - 10 people also completed the PRE Impact evaluation questionnaire, as part of the ex-ante and ex-post evaluation approach adopted by the project. - 10 people completed the POST Impact evaluation questionnaire - 21 photos were gathered at spots defined near the 4 POIs visited by the 10 volunteers, where potential issues were identified. - 90 answers to the 3-question survey associated to each of the 4 POIs defined in the campaign were gathered. - 180 air quality measurements were gathered by the 4 Atmotube devices carried by volunteers (39 PM2.5 measurements). D. Analyse data throuh visualisations & reflection Once data had been collected into Druid, helped by Apache Superset tool , a range of visualizations were created at GREENGAGE's deployment . A superset dashboard was created with Superset analyse survey answers at each POI . The figure shows analysis of answers to the question \u201cHow do you rate air quality at POI4?\u201d. In the top left-hand side in a pie chart we notice that volunteers\u2019 perception was good or very good in 70% of the cases, i.e. for 7 out of the 10 volunteers that took part. The table below the pie chart shows the answers\u2019 distribution out of the 4 categories, there are 5 (no answer for \u201cbad\u201d was received). Interestingly, the PM2.5 concentration gathered by the 4 volunteers that also carried out an Atmotube device whilst they went around with the GREENGAGE app is shown at the bottom right side. During the crowdsourcing campaign\u2019s time, the concentration of PM2.5 was in the range 1 to 7.8. Most studies indicate PM2.5 at or below 12 \u03bcg/m3 is considered healthy with little to no risk from exposure. If the level goes to or above 35 \u03bcg/m3 during a 24-hour period, the air is considered unhealthy and can cause issues for people with existing breathing issues such as asthma. Hence, users\u2019 perceptions regarding air quality match what the Atmobube devices reflected in their measurements. Additionally, results from all questions and all POIs were aggregated in other charts. For instance, figure about survey results in the whole campus depicts that in above 55% of the cases, volunteers considered that the air quality at the all the POIs, i.e. 4 points, selected was good or very good.","title":"Completion of a Citizen Science Loop with GREENGAGE"},{"location":"CS-loop-GREENGAGE/#completion-of-a-citizen-science-loop-with-greengage","text":"","title":"Completion of a Citizen Science Loop with GREENGAGE"},{"location":"CS-loop-GREENGAGE/#citizen-science-loop-in-thematic-co-explorations-within-citizen-observatories","text":"Citizen Observatories (COs) empower individuals to actively participate in data collection and environmental monitoring to address local challenges. The GREENGAGE project, under the Horizon Europe framework, aims to enhance the efficacy and more widespread adoption of COs by providing a structured Citizen Science Loop methodology operationalized by a co-production process which is enabled by its GREEN Engine infrastructure. One core contribution brought forward by GREENGAGE is the \u201cthematic co-exploration\u201d concept. A thematic co-exploration, in the context of COs, refers to a collaborative approach where citizens actively participate alongside scientists and other stakeholders in exploring specific themes or topics related to environmental monitoring and observation. Through them, COs are made purposeful by leveraging the collective efforts of individuals, often non-scientists, to gather, share, and analyse environmental data, typically facilitated by digital tools and technologies. This documentation describes the validation of the GREENGAGE co-creation process for thematic co-explorations, through a university campus based thematic co-exploration, which results in the execution of the following 6 steps of a Citizen Science loop, namely: 1. Problem identification \u2013 recognizing research questions or societal challenges suitable for public engagement; 2. Campaign design \u2013 developing participatory protocols, data collection methodologies, and toolkits for citizens\u2019 engagement; 3. Data crowdsourcing \u2013 enabling citizen scientists to gather good quality observations via digital applications, sensors, and surveys, through data crowdsourcing activities; 4. Data analysis & interpretation \u2013 employing AI-driven tools for insight extraction and thus making humanly meaningful the data modelled; 5. Feedback & collective learning \u2013 validating findings with humans and providing participants with actionable feedback; and 6. Action & impact - informing policies, creating solutions, and refining methodologies for future CS campaigns exploring similar or complementary thematic co-explorations.","title":"Citizen Science loop in thematic co-explorations within Citizen Observatories"},{"location":"CS-loop-GREENGAGE/#greengage-platform-to-enable-citizen-observatories","text":"These CS loop stages shown above are aligned to the main phases established by GREENGAGE\u2019s co-creation process for thematic co-explorations, which has been devised to organize, execute and exploit the results of CS campaigns. These main phases which compose the GREENGAGE co-creation process, supported by the Collaborative Environment (fully described at HOWTO Thematic co-exploration documentation page, are: - Phase 1 - preparing: fully aligned with the \u201cproblem identification\u201d stage of a Citizen Science loop, comprises the following aspects: a) theme selection; b) pilot owners training; c) core team onboarding and d) core team training. - Phase 2 \u2013 designing: aligned with the \u201ccampaign design\u201d stage of a CS loop, comprising: a) experiment specification; b) tools\u2019 resources selection; c) tools resources customization and d) tools resources testing. - Phase 3 \u2013 experimenting: aligned with both the \u201cdata crowdsourcing\u201d and \u201cdata analysis & interpretation\u201d steps of a CS loop. It comprises the following activities: observers onboarding, observers training support, data collection, data combination, data analysis, data visualization and evaluation. - Phase 4 \u2013 sharing: aligned with \u201cfeedback & learning\u201d and \u201caction & impact\u201d stages of CS loop, comprising the following tasks defined in the GREENGAGE thematic co-exploration process, namely storytelling, policy advocacy and sustainability. Each phase is supported by GREENGAGE\u2019s GREEN Engine infrastructure, named GREEN Engine, fully described at page Citizen Observer journey , which integrates various digital tools and knowledge assets to streamline the co-production process. The tools and knowledge assets created in GREENGAGE are categorized in the following areas of concern, where the names of the tools defined for each layer is indicated: 1. Community and Co-production Process Management: In it, the emphasis is on building a strong, informed, and active community which collaborates through a co-production process by defining a hypothesis, research questions formulation or datasets selection, among others. 2. Data Crowdsourcing and Capture: Based on the groundwork of the previous area of concern, this materialises into concrete data collection activities. It is characterized by active participation, leveraging technology to gather vital curated high quality environmental data. 3. Data Analysis and Insights Generation: In this latter area of concern, the collected data is transformed into actionable insights. This is where the data, once transformed in actionable information, becomes a powerful tool for understanding and influencing environmental policy. The purpose of this documentation is to exemplify how the CS loop is enabled through the suite of tools and knowledge assets defined by GREENGAGE and shown in the below figure. Thus, next section describes how GREENGAGE validates the Citizen Science Loop through a real-world thematic co-exploration at the University of Deusto. Community and Co-production Process Management: Throughout this phase, the emphasis is on building a strong, informed, and active community which collaborates through a co-production process. Data Crowdsourcing and Capture: Based on the groundwork of the previous phase, e.g. definition of hypothesis, research questions formulation or datasets selection, among others, this phase materialises into concrete data collection activities. It is characterized by active participation, leveraging technology to gather vital environmental data. Data Analysis and Insights Generation: In this phase the collected data is transformed into actionable insights. This phase is where the data, once transformed in actionable information, becomes a powerful tool for understanding and influencing environmental policy.","title":"GREENGAGE platform to enable Citizen Observatories"},{"location":"CS-loop-GREENGAGE/#citizen-science-loop-step-wised-co-creation","text":"This section showcases the process, tools and results obtained when applying the GREENGAGE CO-enabling approach to a real use-case, namely, \u201creflection on the suitability and air quality of important points of interest (POIs) within the campus of the University of Deusto in Bilbao, Spain\u201d. The following subsections describe the different steps completed towards fulfilling the CS loop for this thematic co-exploration. Notice that to coordinate the execution of this co-creation process, a new process was defined in the Collaborative Environment as shown in Fig. 2.","title":"Citizen Science loop step-wised co-creation"},{"location":"CS-loop-GREENGAGE/#a-cs-campaign-specification","text":"First thing in the organization of a thematic co-exploration, within a Citizen Observatory, is to decide what is the socio-economic and/or environmental challenge that wants to be addressed. There are different aspects that need to be decided at this stage. A useful knowledge asset or CO enabler, as they are called in GREENGAGE, is the thematic co-exploration specification template which is a Word template which guides the organizers of a thematic co-exploration through the following questions: 1. WHY \u2013 reason why this Citizen Observatory\u2019 thematic co-exploration is needed; 2. WHO \u2013 stakeholders involved and affected who need to be recruited for the co-exploration to take place and for the outcomes to be disseminated to; 3. WHAT \u2013 actual endeavours/activities of the Citizen Observatory\u2019s thematic co-exploration towards validating a defined hypothesis and populate a given set of metrics; 4. WHEN \u2013 planning of activities (resources&time) when undertaking the Citizen Observatory\u2019s thematic co-exploration, e.g. crowd-sourcing and data analysis sessions needed; 5. WHERE \u2013 geographical area where Citizen Observatory\u2019s thematic co-exploration will take place, i.e. area to cover and specific points and frequency of measurements which are needed to ensure valuable crowdsourced data; 6. WHICH \u2013 materials and resources, i.e. actual materials, devices and tools needed to execute the Citizen Observatory\u2019s thematic co-exploration, coming either from GREENGAGE or other publicly available tools and assets; 7. HOW \u2013 specification of data analysis processes/workflows to be able to capture, analyse and generate indicators and visualizations sought in Citizen Observatory\u2019s thematic co-exploration. In this stage the needed visualizations and possible storytelling approaches to be eventually adopted are co-specified too. As result, a thematic co-exploration specification for University of Deusto's campus has been produced, where the following decisions were taken: - Set up the observers\u2019 team, in this case 10 researchers associated to the MORElab research group were recruited and committed to be engaged in the whole co-creation process. - Definition of different places (POIs) at the University\u2019s campus (4 were selected) where to gather air quality and place suitability perceptions. These POIs are in the surroundings of the Faculty of Engineering\u2019s building at University of Deusto. - Definition of metrics to estimate air quality and campus space suitability. For that, two new metrics were co-defined by the team of observers in a meeting, namely, a Perception of Air Quality Index (PAQI) was made up, where on a scale from 1 to 5, people have to indicate their perception from very clean (no noticeable pollution effects) to highly polluted (major health concerns, unlivable conditions), and, the Public Space Suitability Index (PSSI) was also made up, where again in a 1 to 5 scale, volunteers have to express their perception regarding aaccessibility & connectivity (20%), safety & security (15%), environmental quality (15%), functionality & comfort (20%), sociability & inclusivity (15%) or maintenance & management (15%) aspects. Again, 5 ranges of suitability were defined ranging from excellent suitability (average answer values >4) to poor suitability (<1). Apart from perceptions of air quality, those volunteers who counted with an Atmotube Pro device also collected air quality data through it. - Definition of tasks to be performed at each selected POI. Firstly, gather a visual perception (photo) and secondly, complete a short 3 question survey for volunteers to express their perception about place suitability and air quality, plus having the chance to leave some feedback about the visited POI. Notice that for the sake of simplicity, one single question per metric was provided to feed the above-mentioned metrics. In a more extensive and thorough real-world thematic co-exploration, one question for each of the factors feeding the devised PSSI should have been included, for instance. Next, the questions designed for the questionnaires to be completed by volunteers at each of the selected POIs are listed: 1. Do you consider that the Air Quality in this spot is? a) very bad; b) bad; c) normal; d) good; e) very good 2. Do you consider that this POI is suited to facilitate campus life and activities? a) not at all; b) not significantly; c) it is OK; d) good enough; e) very good 3. Provide feedback about this POI in terms of its suitability and/or air quality perception. Any suggestions, improvements that you would add?","title":"A. CS campaign specification"},{"location":"CS-loop-GREENGAGE/#b-design-of-the-crowdsourcing-campaign","text":"Once the specification of the thematic co-exploration is ready, the next critical stage is to co-design the CS crowdsourcing campaign \u2013 based on the hypothesis, objectives and data gaps previously identified in thematic co-exploration specification for University of Deusto's campus . In GREENGAGE, the following parameters must be completed in order to set up a CS crowdsourcing campaign: 1. A new instance of an observatory entity is defined, specifying a name for it and the geographical area it covers; 2. A set of POIs within the defined area are defined, for each POI the following fields are defined: type (e.g. culture, nature and so on), description, latitude & longitude, and optional photo; 3. In each POI several tasks can be defined, usually in a campaign the same tasks are required at every measurement point, for each task the following fields are defined: POI associated, topic (e.g. air quality, safety and so on), type (survey, photo, walk and so on), title, description and geocoordinates 4. For tasks of type survey, a survey has be created or an already existing one linked, providing the following fields: title and a set of questions, where for each question a title, type (single choice, multiple choice, text request), and options as pairs of id and values must be defined. The next figure exemplifies the crowdsourcing campaign defined for the thematic co-exploration at Deusto\u2019s campus. Next, it is shown the dashboard that has been defined as back-end of the GREENGAGE app and which can be used to configure CS crowdsourcing campaigns in GREENGAGE. For more details on how to use this interface check GREENGAGE back-end's crowdsourcing campaign configuration dashboard's documentation .","title":"B. Design of the crowdsourcing campaign"},{"location":"CS-loop-GREENGAGE/#c-collect-extract-transform-and-load-campaign-data","text":"On Friday 14th March 2025, from 11:30am to 12:30pm CET, a crowdsourcing campaign was executed, where 10 people took part. Notice that volunteers before starting the crowdsourcing campaign they were requested to complete the following actions: They had to log into https://me.greengage-project.eu/ and complete there a sociodemographic form as the one shown in the figure below. Notice that this form requires that each participant specifies her role in the thematic co-exploration, gender, age range, work status, education level and so on. Besides and, very importantly, volunteers must accept a consent form at the bottomo of this page by means of which GREENGAGE is allowed to process their supplied data and aggregate with other participant's sociodemographic data, still preserving the privacy of participants at all time. Only, after they have completed this form, users are allowed to log in into the GREENGAGE app devised to capture data. They had to complete a PRE Impact evaluation questionnaire as the one shown below. Its PDF printout showcases how volunteers in a CS campaign held within GREENGAGE are questionned about environmental, political, scientific and social impact perception before they take part in a thematic co-exploration. They had to download and install the GREENGAGE app from either Android's Google play store or Apple's app store in their smartphones. Notice that the GREENGAGE app also allows users to register as done in step 1, if they do not have credentials, so that they can access into the app. After, these preparation activities, volunteers were ready to launch the GREENGAGE app, as shown below, and use it to collect data. Such campaign was executed by 10 volunteers on Friday 14th March 2025, from 11:30am to 12:30pm CET. Check into POI Answer questionnaire Submit responses Right after concluding the crowdsourcing campaign, volunteers were also requested to complete a POST Impact evaluation questionnaire ) as the one shown below. Its PDF printout showcases how volunteers in a CS campaign held within GREENGAGE are questionned about environmental, political, scientific and social impact perception after they take part in a thematic co-exploration. The crowdsourced campaign data was collected by applying an ETL process. Whilst, the PRE and POST impact and satisfaction questionnaires are hosted in Google Forms, the GREENGAGE app\u2019s collected data is hosted at Apollo Server which exhibits a GraphQL API. This API allows clients (such as mobile apps or web frontends) to efficiently query and interact with campaign data. This interface was used to retrieve all data crowdsourced associated to the POIs and spots generated in the above described thematic co-exploration\u2019s crowdsourcing campaign. Through the Apollo Server front-end, shown below, details in JSON about tasks performed in the crowdsourcing campaign are retrieved by means of a GraphQL query. Notice that GraphQL is a modern API query language and runtime that allows clients to request precisely the data they need in a single request, unlike RESTful APIs which rely on multiple endpoints and fixed data structures, often leading to over-fetching or under-fetching of data. The ETL process corresponding to the crowdsourcing campaign was implemented as a Python script utilizing asynchronous programming patterns to efficiently extract, transform, and load GREENGAGE app\u2019s data. For data extraction, the script interfaces with GREENGAGE app\u2019s GraphQL API endpoint to retrieve mission data, including various mission types (e.g. SURVEY, WALK or DATASET). Additional observatory-related data is collected to provide geographical context for each mission during transform stage of the process. Finally, the script also interfaces with a Keycloak-based authentication service customized for GREENGAGE , which allows extracting participants\u2019 socioeconomic user data. Internally, this extension of KeyCloak has a PostgreSQL database which can be queried through SQL. Such service enables anonymization while preserving demographic information. Through GREENGAGE's identify manager's interface , 10 volunteers completed their sociodemographic details and were granted credential to access to GREEN Engine's tools, including the GREENGAGE app. During transformation, mission data is processed according to type-specific rules. For example, a SURVEY type task, requires additional API calls to retrieve associated quantitative survey values, or GEOTRACKING type mission requires additional API call to retrieve associated GeoJSON object. The transformation phase maps tasks to observatory information, enriches records with anonymized user demographic data, and converts all data to a standardized CSV format with appropriate fields for analysis. This step is crucial because Apache Druid , the real-time analytics database used in the loading phase, requires data to be ingested in a structured format. The deployment of Druid performed for GREENGAGE stores data internally in a columnar format, which optimizes it for fast aggregations and queries. Finally, the load phase utilizes Apache Druid's ingestion API to load the transformed data. The script generates a comprehensive ingestion specification defining data types, dimensions, and granularity settings to optimize subsequent analytical queries. Once ingested, the data becomes immediately queryable through Druid's SQL API, which enables seamless integration with visualization platforms like Apache Superset and other analytics tools. Notice that 3 ETL processes were set up to extract, transform and load data from: 1. Photos gathered through task 2 (see crowdsourcing campaign's spec ) 2. Survey answers associated to the different users and POIs where surveys were responded through task 1 (see crowdsourcing campaign's spec 3. Socio demographic data completed by volunteers when they signed up to take part in the observatory, by means of the https://me.greengage-project.eu page shown at identify manager's interface . As result of these ETL processes, data was stored in Apache Druid infrastructure (see Apache Druid's interface ), which is the storage solution chosen within GREEN Engine. As result of such campaign the following number of measurements were gathered: - 10 people completed a sociodemographic questionnaire which granted them access GREENGAGE app, after a consent form was signed. - 10 people also completed the PRE Impact evaluation questionnaire, as part of the ex-ante and ex-post evaluation approach adopted by the project. - 10 people completed the POST Impact evaluation questionnaire - 21 photos were gathered at spots defined near the 4 POIs visited by the 10 volunteers, where potential issues were identified. - 90 answers to the 3-question survey associated to each of the 4 POIs defined in the campaign were gathered. - 180 air quality measurements were gathered by the 4 Atmotube devices carried by volunteers (39 PM2.5 measurements).","title":"C. Collect, extract, transform and load campaign data"},{"location":"CS-loop-GREENGAGE/#d-analyse-data-throuh-visualisations-reflection","text":"Once data had been collected into Druid, helped by Apache Superset tool , a range of visualizations were created at GREENGAGE's deployment . A superset dashboard was created with Superset analyse survey answers at each POI . The figure shows analysis of answers to the question \u201cHow do you rate air quality at POI4?\u201d. In the top left-hand side in a pie chart we notice that volunteers\u2019 perception was good or very good in 70% of the cases, i.e. for 7 out of the 10 volunteers that took part. The table below the pie chart shows the answers\u2019 distribution out of the 4 categories, there are 5 (no answer for \u201cbad\u201d was received). Interestingly, the PM2.5 concentration gathered by the 4 volunteers that also carried out an Atmotube device whilst they went around with the GREENGAGE app is shown at the bottom right side. During the crowdsourcing campaign\u2019s time, the concentration of PM2.5 was in the range 1 to 7.8. Most studies indicate PM2.5 at or below 12 \u03bcg/m3 is considered healthy with little to no risk from exposure. If the level goes to or above 35 \u03bcg/m3 during a 24-hour period, the air is considered unhealthy and can cause issues for people with existing breathing issues such as asthma. Hence, users\u2019 perceptions regarding air quality match what the Atmobube devices reflected in their measurements. Additionally, results from all questions and all POIs were aggregated in other charts. For instance, figure about survey results in the whole campus depicts that in above 55% of the cases, volunteers considered that the air quality at the all the POIs, i.e. 4 points, selected was good or very good.","title":"D. Analyse data throuh visualisations &amp; reflection"},{"location":"HOWTO%20Thematic%20co-explorations/","text":"HOWTO Thematic co-exploration This document describes the collaborative process, i.e. set of steps that should be carried out in a logical sequence to co-deliver a thematic co-exploration. Notice that co-production, alternatively known as co-creation, consists of two phases CO-DESIGN and CO-DELIVERY . Importantly, these steps are indicative and there could be different journeys through them, i.e. some may be skipped, and some new steps added if the co-producers of a thematic co-exploration so consider. Besides, a thematic co-exploration is a collaborative and iterative process, i.e. several iterations over some of the steps of the collaborative process may be needed. There are four main phases that the thematic co-exploration might cover. Probably you already performed some of these phases, or at least some steps of those phases, of this workplan, you can take advantage on that and go for the next one. Phase 1: Preparing 1.1 Theme Selection 1.2 Internal (Pilot Owner) training & preparing 1.3 Core Team Onboarding 1.4 Training of the core team Phase 2: Designing 2.1 Experiment specification 2.2 Tools and resources selection 2.3 Tools and resources customization 2.4 Testing tools and resources Phase 3: Experimenting 3.1 Observers onboarding 3.2 Observers Training & support 3.3 Data collection 3.4 Data combination, analysis & visualization 3.5 Experiment evaluation Phase 4: Sharing 4.1 Storytelling (from findings) 4.2 Translating into policies & assess impact 4.3 Experiment sustainability Phase 1: Preparing 1.1. Theme Selection Responsible Pilot owner and PST members. Objective The primary objective of this step is to formulate precise research questions and hypotheses that are anchored in the needs and priorities of policymaking for a given area where thematic co-exploration could be advisable. Guided by pilot owners who are part of the PST team and know well what the priorities of a given Citizen Observatory are, an area of interest to research through a thematic co-exploration is chosen. It might be necessary to get additional stakeholders on board who e.g. are relevant as they provide specific local knowledge, or they should have a meaningful role in the innovation of governance processes. The devised research questions and hypotheses should be actionable, measurable, and capable of being empirically tested within the thematic co-exploration\u2019s scope. They should be designed to fill knowledge gaps, address current challenges, and generate evidence to translate into effective policy interventions. This step will involve a thorough analysis of the current policy landscape, consultation with stakeholders of the GREENGAGE Observatory, and a review of the existing literature to ensure that the research is targeted and relevant. This foundational step sets the stage for the subsequent research and data collection efforts, ensuring that the thematic co-exploration\u2019s outcomes align with the strategic goals of influencing and enabling policymaking. How can we achieve it? After having chosen an area of high priority to tackle a thematic co-exploration, guided by the Thematic Co-Exploration for Citizen Observatory (COb) Specification document, a group of GREENGAGE Observers (GObs), citizens, civil servants and other stakeholders, who will govern / organise the pilot and moderate the thematic co-exploration, answer, in collaboration with owners of the Citizen Observatory, the following core questions: WHY \u2013 Reason why this Citizen Observatory\u2019 thematic co-exploration is needed (arguments for promoting the execution of this Citizen Observatory\u2019s campaign) WHO \u2013 different and affected stakeholders\u2019 groups in Citizen Observatory\u2019s thematic co-exploration (describe the target groups and their possible motivation) WHAT \u2013 Actual endeavours of the Citizen Observatory\u2019s thematic co-exploration (describe what will exactly be done in this Citizen Observatory) WHEN \u2013 Planning of activities and period when Citizen Observatory\u2019s thematic co-exploration will be executed (indicate for how long, what activities, where and for whom will be realized) WHERE \u2013 Geographical locations where Citizen Observatory\u2019s thematic co-exploration will take place (actual geographical areas where data collection and analysis will be carried out) WHICH \u2013 Materials and resources (actual materials and resources needed to execute the Citizen Observatory\u2019s thematic co-exploration) HOW \u2013 Data analysis process to be able to capture, analyse and generate indicators and visualizations sought in Citizen Observatory\u2019s thematic co-exploration The Thematic Co-Exploration for Citizen Observatory (COb) Specification document does not need to be completed thoroughly in this step. It is paramount that the first questions of this document are filled in. It is the WHAT question the one that addresses the following aspects essential to start with the co-production of a thematic co-exploration: The problem (describe the problem statement and the challenge that you are addressing) Objectives (describe the intended result) Added value (describe the potential benefits for the Citizen Observatory\u2019s thematic co-exploration\u2019s stakeholders) Current and desired situation (describe the current approach including existing practices, and the desired situation) CS hypothesis and research questions (to be validated by the outcomes of the execution of the Citizen Observatory\u2019s thematic co-exploration) Metrics definition (of indicators of success for the Citizen Observatory\u2019s thematic co-exploration) Notice that some additional optional resources are listed below which could be used before filling in the thematic co-exploration\u2019s specification, to help narrowing the problem and reflecting about the suitability of the problem to be addressed through a thematic co-exploration. Resources Citizen Science problem statement analysis \u2013 this resource allows to perform a minimal viability analysis to determine whether a problem that wants to be tackled is approachable through Citizen Science or not. Questionnaire to analyse suitability of the problem for co-production of CS experiment - this second resource can help you figure out whether a collaborative process can be organized or not, to address a given problem, through a Citizen Science experiment. Thematic Co-Exploration for GREENGAGE Observatory (GO) Specification Template \u2013 this resource is the most important one in this stage since it allows to answer the WHY, WHO, WHAT, WHEN, WHERE, with WHICH and HOW questions associated to the organization and planning of a thematic co-exploration. It is designed to compile all the information needed to get started with a thematic co-exploration. It gets information from the CONCEPTUALISING and PREPARING phases of the GREENGAGE methodology for Citizen Observatories. 1.2. Pilot Owner\u2019s training & preparing Responsible Pilot owners with the support of the PST. Objective The objective of this task is to empower pilot owners with a deep understanding of the capabilities and resources provided by the GREENGAGE platform (GREEN engine toolbox + knowledge assets in AcadeMe), facilitating the strategic planning and execution of thematic co-explorations. Besides, in this task, a comprehensive and organized inventory of datasets relevant to the thematic area of exploration is compiled. This inventory will not only catalogue existing datasets but also identify critical data gaps that cannot be filled by existing resources. Addressing these gaps is crucial for the thorough examination of the thematic area and for generating actionable insights. To fill these gaps, the strategy includes mobilizing citizen participation in data gathering campaigns. By leveraging the collective effort and unique perspectives of citizens, the initiative aims to enrich the dataset inventory with valuable, real-world data. How can we achieve it? To optimize the integration of technology in thematic co-explorations, pilot owners will first identify relevant technologies offered by the GREEN Engine that align with their exploration objectives. Subsequently, tailored practical training sessions will be conducted for each selected technology, ensuring pilot owners are equipped with the necessary operational skills and knowledge. These sessions are designed to be hands-on, focusing on the functionalities, data management, and application of technologies pertinent to their specific needs. This approach ensures efficient learning and preparation, enabling pilot owners to effectively leverage these technologies to enhance data collection, analysis, and the overall success of their thematic explorations, thereby contributing to impactful insights and policymaking. Furthermore, PST members should comprehensively identify all existing and potential datasets within the scope of the Citizen Observatory and, particularly, those needed for the thematic co-exploration considered. This step aims to create an inventory of datasets, categorising them based on the current thematic co-exploration\u2019s ID, relevance, data type, source, and intended use. Furthermore, it is essential to specify how each dataset may be retrieved and accessed. This inventory serves as the foundation for the dataset metadata registration process in DataHub. Population of Dataset with the newly created inventory is the final action within this step. Resources Training materials folder Dataset Inventory template \u2013 knowledge asset with fields to identify every dataset\u2019s metadata that will be needed for the thematic co-explorations in each Citizen Observatory. It is recommended that for each thematic co-exploration a a copy of this template is performed and filled in. DataHub \u2013 HOWTO for tool used to register, search and view metadata about what datasets of a given thematic area are already available in the catalogue of such Citizen Observatory. The dataset inventory template completed for a given thematic co-exploration should be used as input to complete the datasets metadata in DataHub. GREENGAGE documentation \u2013 Overall documentation of the tools that are available in the GREEN Engine. 1.3. Core Team Onboarding Responsible Pilot leader with the support of the PST. Objective To build a solid foundation for thematic co-explorations, the first stage involves creating a comprehensive map of potential stakeholders who could contribute significantly to the Core Team or join the ranks of the GREENGAGE Observers. This mapping will identify key individuals and organizations whose expertise, interests, and resources align with the goals of the exploration, ensuring a diverse and skilled team that can address the project's multifaceted challenges. Following this strategic identification, the second stage focuses on the onboarding of approximately 15 individuals who will play a crucial role in Phase 2 of the exploration. This group will be meticulously selected based on their potential to contribute to the project's success, ensuring they possess the necessary skills, knowledge, and enthusiasm. The onboarding process will equip them with a thorough understanding of the project objectives, methodologies, and expected outcomes, preparing them to effectively contribute to the collaborative efforts required for the exploration's success. Together, these stages aim to establish a strong, collaborative team capable of driving the project forward through its critical early phases. How can we achieve it? To achieve the objectives, the PST will work closely with the pilot leader to identify and engage potential stakeholders who could contribute to the thematic co-exploration. This will involve the strategic mapping of stakeholders, identifying their roles, interests, and potential contributions to the project by filling the Stakeholder Mapping Canvas . A communication campaign will be created to invite citizens to take part in the thematic co-exploration. Following this to facilitate discussions and community engagement, Discourse will be integrated, providing a modern and user-friendly forum for exchange and collaboration. In parallel, WordPress will serve as our content management system, offering a versatile platform for information dissemination and interaction with the broader community. Through these platforms, the way in which the interested stakeholders can join and contribute to the thematic co-exploration will be communicated (following all the GDPR and ethical guidelines). Finally, a first workshop will be organized to onboard the Core Team, specifically designed for the Core Team, aimed at providing a comprehensive overview of the pilot's objectives, the governance model, and presenting a preliminary draft of the forthcoming steps. This session serves as a crucial platform for aligning the team's understanding and expectations, fostering an environment of open communication and collaboration from the outset. It is an opportunity not only to introduce team members to the broader vision and operational framework of the pilot but also to engage in a dialogue that encourages feedback, suggestions, and collective brainstorming. Resources Stakeholder Mapping Canvas \u2013 document to help you understand who are the stakeholders of your thematic co-exploration WordPress \u2013 HOWTO for tool essential to communicate the existence of a thematic co-exploration and allow dissemination of its results to the broader community Discourse \u2013 HOWTO for tool necessary to foster discussions and dialogue among GObs and allow them to request help and support each other Collaborative Environment \u2013 HOWTO for essential tool to govern, guide and trace the co-production process executed during the co-design and co-delivery of the thematic co-exploration Communication map (UnaLab and Scivil) Inclusion check list (SOCIO-BEE) 1.4. Core team training Responsible Pilot leader with the support of the PST. Objective The objective of this task is to provide the Core Team with a comprehensive and deep understanding of the thematic co-exploration area. This involves a detailed briefing and training session that encompasses several key components essential for its successful execution. Firstly, the Core Team will be extensively familiarized with the specific thematic area of exploration, including its significance, objectives, and the specific challenges or questions it aims to address. This foundational knowledge is crucial for aligning the team's efforts and ensuring a cohesive approach throughout the exploration process. In addition to understanding the thematic area, the Core Team will receive training on the ethical framework of GREENGAGE. This includes discussions on data privacy, ethical considerations in citizen science, and how to ensure that all activities conducted within the thematic co-exploration adhere to these ethical guidelines. Finally, the training will cover the technological tools available within the GREENGAGE ecosystem. The Core Team will learn how to utilize these technologies effectively, from data collection and analysis tools to platforms for collaboration and dissemination of findings. This technical training is essential for maximizing the impact of the thematic co-exploration, enabling the team to gather, analyse, and share data efficiently. How can we achieve it? Training materials for the following topics have been made ready by the project and should be extended and customized for each thematic co-exploration: Data I Data II Co-design I Co-design II Co-design III [Air quality & Mobility Monitoring] Technology I [Technology II] [Technology III] [Policy-making & Governance] [Analysis & Visualisation] Create a training plan specific for the pilot. Create the needed training material. Perform at least two training sessions \u2013 1 hour each \u2013 to the Core Team. Make them as practical and applied as possible. Resources Training plan template (to be done) Materials for thematic co-exploration and citizen onboarding \u2013 TO BE PREPARED Folder where trainings are currently uploaded which will be published as part of the Knowledge Base aka GREENGAGE Academy of the main project\u2019s website Phase 2: Designing 2.1. Experiment specification Responsible Pilot Leader with the support of the Core Team and the PST Objective To successfully navigate through the latter stages of the thematic co-exploration, specifically phases 3 (Experimenting) and 4 (Sharing), a meticulously crafted list of activities is essential. This list will outline the strategic actions required to not only implement the findings and insights gleaned from the exploration but also to disseminate these outcomes effectively to a broader audience. This comprehensive list of activities will serve as a roadmap, guiding the seamless transition from theory to practice and ensuring that the valuable insights generated through the exploration are shared widely and effectively, fostering greater impact, and facilitating informed decision-making. How can we achieve it? Organise a second workshop conducted with the Core Team. This workshop aims to collaboratively design the specific actions, including the identification of concrete indicators or Key Performance Indicators (KPIs) for this thematic co-exploration, that will be evaluated in phases 3 (Experimenting) and 4 (Sharing) of the thematic co-exploration. By engaging in a co-design process, the Core Team will collectively determine the most effective strategies and activities to ensure the successful delivery and dissemination of the project's outcomes. Notice that the core team with the support of the Pilot Owner, must discuss about indicators and metrics suitable to assess the thematic co-exploration, e.g. if the purpose is to explore air pollution the following indexes could be considered Air Quality Index or Liveability Index. In these discussions, GREENGAGE\u2019s global KPIs should be considered (KPIs-monitoring-table.xlsx) giving place to a thematic co-exploration\u2019s concrete KPI, where partial contribution to global KPIs will be planned and how to contribute to the specific ones also defined. Furthermore, a new co-production process will be set up in GREENGAGE Collaborative Environment. Hence, the accomplishment of this step requires usage of all the tools belonging to the \u201cCommunity and co-production process management\u201d layer of GREEN Engine, in the following order: A entry created in preparing within the parent pilot / Citizen Observatory page, for the newly specified thematic co-exploration should be refined now. The instructions to use Wordpress should be followed to achieve this. Notice that GREENGAGE provides a default template that can be used to instantiate thematic co-explorations. A new set of categories for topics of discussion following the instructions of usage of the Discourse tool , should be created by the GObs leading and managing the thematic co-exploration. This will serve to foster the collaboration and dialogue among participants in the thematic co-exploration. Notice that GREENGAGE provides a default template with categories and candidate topics that is recommended to each thematic co-exploration. A new co-production process based on the \u201cGREENGAGE way\u201d co-production process schema or blueprint already loaded into Collaborative Environment should be instantiated. Besides, there is the need to involve the core team in the analysis of the available datasets and the reflection about dataset gaps to be filled in to complete the thematic co-exploration. Hence, a revisit of the dataset inventory initially created in the \u201cPreparing\u201d phase by pilot owners will be performed. Finally, it is time to think about how the different activities and events that will be organized with the community of observers. For that, the GREENGAGE community building plan knowledge asset may be used. Resources Work plan template Data management plan Data protocol specification Exemplary document reflecting about metrics and KPIs Reference KPI table for GREENGAGE project Collaborative Environment \u2013 HOWTO for essential tool to govern, guide and trace the co-production process executed during the co-design and co-delivery of the thematic co-exploration CS-CO-schema-process-specification \u2013 to specify the process of the thematic co-exploration ala \u201cGREENGAGE way\u201d Problem Statement Questionnaire document \u2013 Dataset Inventory template \u2013 knowledge asset with fields to identify every dataset\u2019s metadata that will be needed for the thematic co-explorations in each Citizen Observatory. It is recommended that for each thematic co-exploration, a copy of this template is completed. Template for planning of activities and events 2.2. Tools and resources selection Responsible Pilot Leader with the support of the Core Team and the PST. Objective To achieve the set objectives of the thematic co-exploration efficiently, a meticulous selection process will be undertaken to finalize the list of tools and resources essential for the project. This encompasses a wide array of assets, including, but not limited to, existing datasets that are pivotal for research and analysis, comprehensive guidelines that outline procedural directives, and thorough documentation that provides detailed instructions and insights into the project's methodologies. Additionally, other instrumental resources such as analytical software, data management platforms, and collaborative tools will be evaluated for their relevance, efficiency, and compatibility with the thematic co-exploration\u2019s goals. This careful curation process ensures that every selected tool and resource aligns with the specific needs of the exploration, facilitating a streamlined, effective approach to achieving the research objectives. How can we achieve it? To achieve this stage, we will conduct a third workshop with the Core Team. This workshop will focus on the selection of the tools and resources that will be used in the thematic co-exploration. The Core Team will collaboratively evaluate and select the most appropriate tools and resources, ensuring that they align with the project's objectives and can meet the specific needs of the experiment specified in T2.1. This analysis may rise the identification of gaps in the available tools and resources, which will be addressed in the T2.3. Finally, the Data Protocol to be used in the thematic co-exploration needs to be defined, reflecting upon it guided by Data Protocol Specification template provided by GREENGAGE project. Resources List of resources List of GREENGAGE tools \u2013 Overall documentation of the tools that are available in the GREEN Engine. Template for tools and resources selection \u2013 document that guides you in the process of selecting the knowledge assets, tools and resources necessary to run the thematic co-exploration Data protocol specification 2.3. Tools and resources customization Responsible Pilot Owner with the support of PST and WP4. Objective To ensure the effectiveness and efficiency of the thematic co-exploration, it is necessary to undertake customizations on the tools utilized, including calibration, configuration, and possible extensions. This process is complemented by providing technical consultancy with the support of GREENGAGE consortium to address specific needs, thereby enhancing the functionality and usability of these tools for the observers. Concurrently, resources such as templates, documentation, and translations are optimized, along with the development of additional (thematic co-exploration specific) enablers or knowledge assets, to streamline observers' activities and facilitate their engagement. Data quality is also a priority, with efforts directed towards cleaning available datasets to ensure reliability and accuracy in analysis. Additionally, ad-hoc development may be undertaken to meet unique project requirements regarding datasets. Preparing evaluation materials forms a critical component of this task too, encompassing the creation of questionnaires, refinement of Key Performance Indicators (KPIs), and the selection or development of measurement instruments, all aimed at rigorously assessing the impact and outcomes of the co-exploration activities. In this task the tools and resources selection document, the data protocol and data management plan may be updated. New evaluation questionnaires and scripts to gather metrics values will be devised. Past CS projects\u2019 evaluation questionnaires such as those of SOCIO-BEE can be used to inspire the new questionnaires to be developed. How can we achieve it? The Pilot Owner and PST will agree on a plan with WP4. DISCLAIMER All resources to be used from external locations (APIs, websites,...) must be listed and communicated in advance to the technical administration of the project. Resources Reference KPI table for GREENGAGE project Template for tools and resources selection \u2013 document that guides you in the process of selecting the knowledge assets, tools and resources necessary to run the thematic co-exploration Data protocol specification PRE SOCIO-BEE Citizen Science Activists Evaluation Questionnaire POST SOCIO-BEE Citizen Science Activists Evaluation Questionnaire 2.4. Testing tools and resources Responsible Pilot Leader with the support of the Core Team and the PST. Objective The objective of this task is to perform a last test on the selected and adapted tools and resources. This task is essential to validate the functionality, compatibility, and performance of all tools and resources that have been selected to conduct the designed experiment. It involves rigorous testing scenarios that mimic real-world applications to identify any remaining issues or inefficiencies that could impact the project's success. This process not only encompasses technical tools such as data analysis software and data collection applications but also includes testing the usability and clarity of guidelines, documentation, and any other support materials. The objective is to guarantee that every component functions seamlessly together, providing a robust framework for observers and participants to engage effectively in the project activities. How can we achieve it? The Core Team will have a workshop in which the selected tools and resources will be tested in real life scenarios to ensure the functionality, compatibility, and performance of them. Resources Template for tools and resources selection \u2013 document that guides you in the process of selecting the knowledge assets, tools and resources necessary to run the thematic co-exploration List of tools \u2013 Overall documentation of the tools that are available in the GREEN Engine. PRE SOCIO-BEE Citizen Science Activists Evaluation Questionnaire POST SOCIO-BEE Citizen Science Activists Evaluation Questionnaire Template for selected verified list of knowledge assets User acceptance test \u2013 this document guides about the different aspects that should be checked in the infrastructure in order validate its correct operation. Phase 3: Experimenting 3.1. Observers onboarding Responsible Pilot Owner with the support of the Core Team and the PST. Objective This task focuses primarily on integrating these key participants into the project's operational phase without delving into training and support, which are addressed in subsequent steps. This phase is about ensuring that GOs are well-informed about their roles, the project's objectives, and the overarching framework within which they will operate. How can we achieve it? Similar to what we conducted in T1.3. , we will continue adapting and enhancing the communication campaign in the pilot. Together with this, we should update the Discourse and WordPress platforms to ensure that the information is up-to-date and accessible to the GOs. In essence, three main activities will be considered in this task: Promote and publicize thematic co-exploration Onboard further Citizen Observers Ensure ethics protocol are followed Resources Stakeholder Mapping Canvas \u2013 document to help you understand who are the stakeholders of your thematic co-exploration Discourse \u2013 HOWTO for tool necessary to foster discussions and dialogue among GObs and allow them to request help and support each other Communication map (UnaLab and Scivil) - available at Communication map Materials for thematic co-exploration and citizen onboarding \u2013 TO BE PREPARED Data ethics in participatory science GREENGAGE Data Ethics starter kit 3.2. Observers Training & support Responsible Pilot Leader with the support of the Core Team and the PST. Objective The core objective of this task is to train the community of observers in the tools and resources needed to execute the thematic co-exploration. The training will be complemented by ongoing support, providing the observers with the necessary guidance and assistance to navigate through the project's operational phase. This support will be delivered through various channels, including Discourse, direct communication, and documentation, ensuring that the observers have access to the resources and assistance they need to engage effectively in the project. Part of this step on training and support will be to organize the data collection process with observers and agree on the data collection protocol to perform analysis and reflection upon the collected data. The Data Protocol Template is a resource that should help organizing the data collection process. How can we achieve it? GOs will attend a welcome event to train on the objectives of the experiment, the need to gather new data, the selected technologies by the core team, ethical aspects and other important topics identified in T2.1. This welcome event or workshop will be complemented with asynchronous training materials that will be available in the Knowledge Base aka GREENGAGE Academy of the main project\u2019s website. Training materials for the following topics have been made ready by the project and should be extended and customized for each thematic co-exploration: Data I Data II Co-design I Co-design II Co-design III [Air quality & Mobility Monitoring] Technology I [Technology II] [Technology III] [Policy-making & Governance] [Analysis & Visualisation] Resources Training plan template (to be adapted) Data crowdsourcing and capture layer \u2013 Overall documentation of the tools that are available in the GREEN Engine. Folder where trainings are currently uploaded which will be published as part of the Knowledge Base aka GREENGAGE Academy of the main project\u2019s website [DataProtocolSpecification] (https://aitonline.sharepoint.com/:w:/r/sites/HEUGREENGAGE337/Shared%20Documents/WP4%20CO%20enabling%20infrastructure%20and%20interoperable/D4.1%20GREEN%20Engine%20and%20manuals/Academy/resources/DataProtocolSpecification.docx?d=w086eacc673a846eaac82aa959ef89d5c&csf=1&web=1&e=dfqsBH) 3.3. Data collection Responsible Pilot Leader of the GOs Objective This step aims to retrieve the data identified in the previous phase. This data may come from three different sources: Data that are already available in Open Data portals or provided by Copernicus data. This process will involve the creation of the pipelines (ingestion sources) that will introduce each dataset in DataHub, which will allow us to retrieve the data from the source and store it in the centralised repository. The technical team should create the ingestion sources with the pilot owners' support. Data that are not available and need to be collected through human collaboration. This process will involve the creation of data gathering campaigns that include missions (tasks) to be completed by the public. The missions will be designed by the thematic co-exploration\u2019s moderators, with the support of the technical team, to retrieve specific data. The missions, thus, the data capture process, will involve using specific tools designed for this task, such as MODE, GREENGAGE app or MindEarth. Notice that mission is a GREENGAGE concept denoting then need to collect certain data in a given area and time period to address a certain data gap identified within a thematic co-exploration. Apart from quantitative data, during this step, qualitative data about perceptions of observers regarding different aspects associated to the thematic co-exploration will also be gathered. This will involve making observers and even external citizens to the thematic co-exploration to be involved in completing surveys, realizing in depth interviews with stakeholders of the thematic co-exploration or organizing focus groups, local walks and other alternative mechanisms where discussion and reflection around the thematic co-exploration can be generated. How can we achieve it? This must be done in three different ways, depending on the data availability: Data already available For the data that is already available, the technical team will create the ingestion sources that will allow the data to be retrieved from the source and stored in the centralised repository. The ingestion sources can be created using Apache NiFi, allowing us to retrieve data from APIs, FTP servers, and other sources. Alternatively, other ingestion sources, such as Python scripts, may require programmatic solutions. Finally, the ingestion sources can also be configured using Apache Druid, allowing us to load data directly into the project's database. The technical team will be responsible for creating the ingestion sources, while the pilot owners will be responsible for providing the necessary information and testing that the ingested data complies with the thematic co-exploration\u2019s data needs. Data not available To achieve this, the missions to collect data should be configured and launched. This process involves Pilot leaders and project\u2019s technical team\u2019s representatives that will help them configure the tools that will be used for the data collection or capture process. Once the tools are configured, observers should be trained to use them. During the operation of the data collection campaigns, the PST members should monitor their progress and performance, and the technical team should ensure that the data is being collected and stored correctly. Data capturing apps and sensors belonging to \u201cData crowdsourcing and capture layer\u201d of GREEN Engine, such as MODE, GREENGAGE app or MindEarth, will be used to collect the data. Qualitative data Questionnaires, surveys, interviews, focus groups or local walks, among other alternative activities, will be organized. The section and customization of such evaluation materials and instruments was performed in Task 2.3, based on examples of such mechanisms already published as knowledge assets within the GREENGAGE Academy. Resources Resources and budget template \u2013 this document developed in T2.2. can be used to keep track of the resources and budget planned for the data collection process. HOWTOs to gather data from Copernicus and from Open Data portals considered in the project (TBD) Apache NiFi official documentation \u2013 Official documentation for tool to create ingestion sources Apache NiFi GREENGAGE documentation \u2013 HOWTO for tool in GREENGAGE data stack configured for creating ingestion sources Apache Druid official documentation \u2013 HOWTO for tool to load data directly into the centralised datastore offered by GREENGAGE MODEs GREENGAGE documentation \u2013 HOWTO for tool to collect commuting data MindViews GREENGAGE documentation \u2013 HOWTO for tool to collect geo-tagged street-level imagery GRENGAGE apps GREENGAGE documentation \u2013 HOWTO for tool to collect data via campaigns AtmoTube PRO sensor webpage \u2013 webpage of sensor selected to measure air quality data in pilots [Sensors integration in GREENGAGE] (https://greengage-project.github.io/Documentation/tools/sensorsIntegration/) \u2013 HOWTO to off-the-shelf sensors (ATMOtube and sensing apps, e.g. Noisetube ) to be used in the project Data management plan Data protocol specification Questionnaires of evaluation from SOCIO-BEE 3.4. Data combination, analysis & visualization Responsible Pilot Leader with the support of PST and WP4. Objective This multifaceted task begins with preprocessing steps to cleanse datasets of any inconsistencies, errors, or irrelevant information using automated data ingestion workflows through tools like Apache NiFi, channelling data efficiently into Apache Druid for optimal analysis readiness. The aim is to maintain data accuracy and integrity for a deeper, more precise understanding critical for informed decision-making. Concurrently, the development of interactive dashboards aims to transform this complex, cleansed data into accessible visual formats, enabling stakeholders\u2014including policymakers, researchers, and citizens\u2014to engage with data themes and patterns in a collaborative exploration environment. These dashboards, providing real-time monitoring and analysis, are crucial for facilitating quick, informed decisions and policy development, while promoting transparency and inclusivity. Together, these objectives strive to simplify data analysis and visualization processes through interlinked datasets and shared schemas, ultimately fostering a shared understanding and enhancing the thematic co-exploration\u2019s impact. How can we achieve it? To successfully create workflows that can process the data into the desired format we have several ways to do it: creating a NiFi Flow, creating a python script or manually. NiFi flow: Apache NiFi is a powerful tool that allows us to create data pipelines that can be scheduled to run periodically. This tool is very useful for data ingestion, as it allows us to fetch data from APIs, FTP servers, and other sources. Additionally, NiFi can be used to clean and transform data, ensuring that it is in the desired format and structure. Finally, NiFi can be used to load data into Apache Druid, ensuring that the data is correctly ingested into the database. Notably, in the data curation and cleaning process the Data quality VRVIS tool can also be useful. Python script: Develop Python scripts for more complex data transformations that might not be directly feasible within NiFi. This can include data cleaning, normalization, or feature engineering tasks. It can also be integrated with Apache NiFi to run as part of the flow. Furthermore, by using the Apache Druid API we can directly load data into the database. Manually: For simple data transformations, it might be more efficient to perform them manually. This can include tasks such as renaming columns, changing data types, or removing columns. However, this approach is not scalable and should only be used for simple tasks. Once the data is pre-processed and cleaned the analysis may be conducted by combining and exploring the data using Apache Superset and Apache Druid. Apache Superset will allow us to create and share rich data visualizations and dashboards, providing an intuitive interface for users to interact with the data. Apache Druid will be utilized for its real-time data analytics capabilities, ensuring that our dashboards can display the most current data efficiently. Additionally, the integration of Apache NiFi will streamline the data flow into these systems, ensuring a consistent and reliable data pipeline. The combination of these tools, supported by thorough documentation and examples of integration, will enable us to create dynamic and insightful thematic co-exploration dashboards that serve the diverse needs of our project stakeholders. Resources Apache NiFi documentation \u2013 HOWTO for tool to create preprocessing flows Apache Druid documentation \u2013 HOWTO for tool to load data into the project's database Apache Superset documentation \u2013 HOWTO for tool to create dashboards and visualizations ThematicCoExplorationSpec \u2013 continuously updated document where to revisit the objectives of the thematic co-exploration and generate dashboards that address them. Once visualizations are generated, it is convenient to edit the corresponding section in the template, so tracking the progress of the thematic co-exploration. Data quality VRVIS \u2013 HOWTO for tool to handle data quality assurance on the gathered datasets Apache Superset documentation \u2013 official documentation for tool to create dashboards and visualizations HOWTO GISAT tool to do these visualizations \u2013 documentation for tool designed to facilitate the seamless integration, exploration, visualization, and analysis of diverse datasets HOWTO thematic co-exploration \u2013 step by step example on how to co-produce a simple thematic co-exploration, using real datasets. It includes a video illustrating the usage of the different data management tools that intervene. 3.5. Experiment evaluation Responsible Pilot owners, GREENGAGE Observers and PST Objective This objective involves an evaluation of the experiment\u2019s objectives, outputs, and overarching goals have been achieved, alongside a detailed review of how the planned activities were implemented in practice. By meticulously examining the alignment between planned and actual activities, the assessment will not only highlight the thematic co-exploration's accomplishments and areas of strength but also uncover any discrepancies or challenges encountered along the way. Furthermore, this process is instrumental in identifying valuable opportunities for learning and improvement, facilitating a reflective analysis that can inform future initiatives. How can we achieve it? Implement the evaluation strategy planned sketched in \u201cPhase 1 Preparing\u201d and refined in \u201cPhase 2 Designing\u201d (defined KPIs and mechanisms for their assessment). Facilitate spaces for collective reflection about barriers and enablers encountered during the Preparing, Designing, and Experimenting phases. Include all participants involved (PST members, pilot owners, Core Group, and Observers). Think on the possible following iteration objective, goals, and outputs. Identify possibilities to mitigate or overcome barriers and enhance enablers based on what have you have learned so far. Adapt or change your thematic co-exploration specification, work plan or other planning strategies in place for future GO activities. Resources TBD Phase 4: Sharing 4.1. Storytelling (from findings) Responsible Pilot owners, GREENGAGE Observers (GOs) and PST Objective The objective in the \"Generation of storylines for wide dissemination\" stage is to craft engaging and accessible narratives that convey the thematic co-exploration's key findings and insights to a broad audience. These storylines are designed to translate complex data and research outcomes into formats that are both engaging and easily understandable by the public. The aim is to underscore the thematic co-exploration's significance and its potential impact on policymaking, community initiatives, and scientific research. By doing so, the goal is to foster increased public awareness and interest in the project, thereby enhancing its overall reach and influence. To achieve wide visibility and impact, these storylines will be disseminated through various media channels, tailored to resonate with diverse audiences, including: GREENGAGE partners and pilots. Local communities addressed by or emerging through the Observatories. Audiences beyond GREENGAGE that are interested in tools, topics, or areas addressed by the GO How can we achieve it? To effectively generate storylines for wide dissemination, we will utilize a range of tools specified in our project documentation. The Communications and Outreach Team will collaborate closely with the Data Analysis Team, utilizing Apache Superset for creating compelling data visualizations that bring complex data to life in an easily digestible format. These visual elements will be integral in crafting storylines that are both informative and engaging to the public. Additionally, we will leverage WordPress and Discourse platforms to draft, refine, and share these narratives, ensuring they are tailored to resonate with diverse audience segments. The integration of these tools will enable us to effectively communicate the thematic co-exploration's impact and findings through various media channels, from social media to press releases, thereby maximizing reach and fostering greater public engagement with the project's outcomes. Resources Course on Storytelling for Citizen Science \u2013 this course gives some tips about how to perform storytelling for Citizen Science projects Presentation of Storytelling and Citizen Science \u2013 this presentation describes about the value of storytelling in Citizen Science. It takes us through several viewpoints and tools you can use to tell your story, considering the needs of the audience, the human instinct for storytelling, how to use it to make your Citizen Science project exciting and meaningful to other people, and encourages you to think about ways to use these tools to reach more audiences Communication map (UnaLab and Scivil) - available at Communication map 4.2. Impact on policies Responsible Pilot Owners, GREENGAGE Observers (GOs), PST. Objective The generation of policy briefs for policymakers, a key objective of this project step, involves synthesizing research findings and data into clear, concise, and actionable recommendations. These briefs are crafted to effectively communicate pivotal insights and findings to policymakers, providing them with evidence-based guidance for policy interventions. By directly aligning with current policy debates and challenges, these briefs aim to bridge the gap between research and practical policymaking, offering solutions that are grounded in thorough data analysis. This approach not only enhances the relevancy of the research to ongoing policy discussions but also aims to significantly influence policy decisions and strategies, thus amplifying the overall impact of a given thematic co-exploration. How can we achieve it? To achieve this, we will first aggregate and analyse the data collected and insights generated throughout the project, using tools like Apache Druid for real-time analytics and Apache Superset for data visualization. These tools will help in identifying key trends and findings that are most relevant to policymaking. The policy analysis team will then work closely with research teams to distil these insights into policy briefs, ensuring that they are both data-driven and aligned with current policy needs. Communication specialists will refine these briefs to be clear and compelling, making them accessible to non-specialist policymakers. Throughout this process, a given thematic co-exploration\u2019s coordinators will ensure that the briefs align with the strategic objectives of the thematic co-exploration and effectively convey its potential impact on policy and governance. Resources Collaborative Environment \u2013 HOWTO about environment to collaborate in the policy briefs' generation in a centralised repository. The CE will suggest templates to guide the elaboration of policy briefs. Policy brief guideline \u2013 public access web document explaining how to format a policy brief Example of Policy brief \u2013 example of policy brief in the domain of Citizen Science that could inspire those embarking in the preparation of a policy brief 4.3. Experiment sustainability Responsible Pilot Owners, GREENGAGE Observers (GOs), PST. Objective The objective of identifying mechanisms for sustaining thematic co-explorations entails developing a strategic framework that ensures the long-term and continued relevance of these initiatives. This involves a thorough analysis of the resources, partnerships, and methodologies that have contributed to the success of the exploration, with an eye towards replicating and adapting these elements for future cycles. Critical to this process is the engagement with stakeholders to understand their needs, motivations, and feedback, which can inform the refinement of objectives and approaches for subsequent iterations. As part of this objective, the preparation for a possible new iteration phase or launch of a new thematic co-exploration as side effect of this thematic co-exploration begins with a clear articulation of lessons learned, achievements, and challenges from the first cycle, setting the stage for enhanced planning, execution, and impact in the next phase. This proactive approach not only ensures that thematic co-explorations remain dynamically aligned with evolving research landscapes and societal needs but also fosters a culture of continuous improvement and innovation within the community of practice. How can we achieve it? To achieve the objective of sustaining thematic co-explorations for future iterations, a multifaceted approach is essential. Initially, crafting an attractive and compelling report that encapsulates the achievements, insights, and value of the exploration becomes crucial. This report should be designed to resonate with both current stakeholders and potential future participants, highlighting the exploration's impact and the opportunities it presents. Following this, a thorough review and discussion of the experiment's evaluation, as outlined in T3.5. will be instrumental. This step involves critically assessing the outcomes and feedback to identify areas for improvement, such as optimizing engagement activities and incorporating successful stories into the communication strategy, thereby enriching the narrative and appeal of the project. Furthermore, validating the pertinence and relevance of the thematic co-exploration as an enabler of policymaking is paramount for the second iteration. This includes a detailed review of the local situation and the exploration's alignment with policy objectives, ensuring that any necessary adjustments are made to enhance its effectiveness and impact. Through this comprehensive approach, the project aims to not only solidify its foundation for sustainability but also ensure its continued evolution and relevance in contributing to meaningful policy development and stakeholder engagement. Resources Open Data, Software and Code Guidelines \u2013 These guidelines relate to the Open Research Europe policy on data availability","title":"HOWTO Thematic co-exploration"},{"location":"HOWTO%20Thematic%20co-explorations/#howto-thematic-co-exploration","text":"This document describes the collaborative process, i.e. set of steps that should be carried out in a logical sequence to co-deliver a thematic co-exploration. Notice that co-production, alternatively known as co-creation, consists of two phases CO-DESIGN and CO-DELIVERY . Importantly, these steps are indicative and there could be different journeys through them, i.e. some may be skipped, and some new steps added if the co-producers of a thematic co-exploration so consider. Besides, a thematic co-exploration is a collaborative and iterative process, i.e. several iterations over some of the steps of the collaborative process may be needed. There are four main phases that the thematic co-exploration might cover. Probably you already performed some of these phases, or at least some steps of those phases, of this workplan, you can take advantage on that and go for the next one. Phase 1: Preparing 1.1 Theme Selection 1.2 Internal (Pilot Owner) training & preparing 1.3 Core Team Onboarding 1.4 Training of the core team Phase 2: Designing 2.1 Experiment specification 2.2 Tools and resources selection 2.3 Tools and resources customization 2.4 Testing tools and resources Phase 3: Experimenting 3.1 Observers onboarding 3.2 Observers Training & support 3.3 Data collection 3.4 Data combination, analysis & visualization 3.5 Experiment evaluation Phase 4: Sharing 4.1 Storytelling (from findings) 4.2 Translating into policies & assess impact 4.3 Experiment sustainability","title":"HOWTO Thematic co-exploration"},{"location":"HOWTO%20Thematic%20co-explorations/#phase-1-preparing","text":"","title":"Phase 1: Preparing"},{"location":"HOWTO%20Thematic%20co-explorations/#11-theme-selection","text":"","title":"1.1. Theme Selection"},{"location":"HOWTO%20Thematic%20co-explorations/#responsible","text":"Pilot owner and PST members.","title":"Responsible"},{"location":"HOWTO%20Thematic%20co-explorations/#objective","text":"The primary objective of this step is to formulate precise research questions and hypotheses that are anchored in the needs and priorities of policymaking for a given area where thematic co-exploration could be advisable. Guided by pilot owners who are part of the PST team and know well what the priorities of a given Citizen Observatory are, an area of interest to research through a thematic co-exploration is chosen. It might be necessary to get additional stakeholders on board who e.g. are relevant as they provide specific local knowledge, or they should have a meaningful role in the innovation of governance processes. The devised research questions and hypotheses should be actionable, measurable, and capable of being empirically tested within the thematic co-exploration\u2019s scope. They should be designed to fill knowledge gaps, address current challenges, and generate evidence to translate into effective policy interventions. This step will involve a thorough analysis of the current policy landscape, consultation with stakeholders of the GREENGAGE Observatory, and a review of the existing literature to ensure that the research is targeted and relevant. This foundational step sets the stage for the subsequent research and data collection efforts, ensuring that the thematic co-exploration\u2019s outcomes align with the strategic goals of influencing and enabling policymaking.","title":"Objective"},{"location":"HOWTO%20Thematic%20co-explorations/#how-can-we-achieve-it","text":"After having chosen an area of high priority to tackle a thematic co-exploration, guided by the Thematic Co-Exploration for Citizen Observatory (COb) Specification document, a group of GREENGAGE Observers (GObs), citizens, civil servants and other stakeholders, who will govern / organise the pilot and moderate the thematic co-exploration, answer, in collaboration with owners of the Citizen Observatory, the following core questions: WHY \u2013 Reason why this Citizen Observatory\u2019 thematic co-exploration is needed (arguments for promoting the execution of this Citizen Observatory\u2019s campaign) WHO \u2013 different and affected stakeholders\u2019 groups in Citizen Observatory\u2019s thematic co-exploration (describe the target groups and their possible motivation) WHAT \u2013 Actual endeavours of the Citizen Observatory\u2019s thematic co-exploration (describe what will exactly be done in this Citizen Observatory) WHEN \u2013 Planning of activities and period when Citizen Observatory\u2019s thematic co-exploration will be executed (indicate for how long, what activities, where and for whom will be realized) WHERE \u2013 Geographical locations where Citizen Observatory\u2019s thematic co-exploration will take place (actual geographical areas where data collection and analysis will be carried out) WHICH \u2013 Materials and resources (actual materials and resources needed to execute the Citizen Observatory\u2019s thematic co-exploration) HOW \u2013 Data analysis process to be able to capture, analyse and generate indicators and visualizations sought in Citizen Observatory\u2019s thematic co-exploration The Thematic Co-Exploration for Citizen Observatory (COb) Specification document does not need to be completed thoroughly in this step. It is paramount that the first questions of this document are filled in. It is the WHAT question the one that addresses the following aspects essential to start with the co-production of a thematic co-exploration: The problem (describe the problem statement and the challenge that you are addressing) Objectives (describe the intended result) Added value (describe the potential benefits for the Citizen Observatory\u2019s thematic co-exploration\u2019s stakeholders) Current and desired situation (describe the current approach including existing practices, and the desired situation) CS hypothesis and research questions (to be validated by the outcomes of the execution of the Citizen Observatory\u2019s thematic co-exploration) Metrics definition (of indicators of success for the Citizen Observatory\u2019s thematic co-exploration) Notice that some additional optional resources are listed below which could be used before filling in the thematic co-exploration\u2019s specification, to help narrowing the problem and reflecting about the suitability of the problem to be addressed through a thematic co-exploration.","title":"How can we achieve it?"},{"location":"HOWTO%20Thematic%20co-explorations/#resources","text":"Citizen Science problem statement analysis \u2013 this resource allows to perform a minimal viability analysis to determine whether a problem that wants to be tackled is approachable through Citizen Science or not. Questionnaire to analyse suitability of the problem for co-production of CS experiment - this second resource can help you figure out whether a collaborative process can be organized or not, to address a given problem, through a Citizen Science experiment. Thematic Co-Exploration for GREENGAGE Observatory (GO) Specification Template \u2013 this resource is the most important one in this stage since it allows to answer the WHY, WHO, WHAT, WHEN, WHERE, with WHICH and HOW questions associated to the organization and planning of a thematic co-exploration. It is designed to compile all the information needed to get started with a thematic co-exploration. It gets information from the CONCEPTUALISING and PREPARING phases of the GREENGAGE methodology for Citizen Observatories.","title":"Resources"},{"location":"HOWTO%20Thematic%20co-explorations/#12-pilot-owners-training-preparing","text":"","title":"1.2. Pilot Owner\u2019s training &amp; preparing"},{"location":"HOWTO%20Thematic%20co-explorations/#responsible_1","text":"Pilot owners with the support of the PST.","title":"Responsible"},{"location":"HOWTO%20Thematic%20co-explorations/#objective_1","text":"The objective of this task is to empower pilot owners with a deep understanding of the capabilities and resources provided by the GREENGAGE platform (GREEN engine toolbox + knowledge assets in AcadeMe), facilitating the strategic planning and execution of thematic co-explorations. Besides, in this task, a comprehensive and organized inventory of datasets relevant to the thematic area of exploration is compiled. This inventory will not only catalogue existing datasets but also identify critical data gaps that cannot be filled by existing resources. Addressing these gaps is crucial for the thorough examination of the thematic area and for generating actionable insights. To fill these gaps, the strategy includes mobilizing citizen participation in data gathering campaigns. By leveraging the collective effort and unique perspectives of citizens, the initiative aims to enrich the dataset inventory with valuable, real-world data.","title":"Objective"},{"location":"HOWTO%20Thematic%20co-explorations/#how-can-we-achieve-it_1","text":"To optimize the integration of technology in thematic co-explorations, pilot owners will first identify relevant technologies offered by the GREEN Engine that align with their exploration objectives. Subsequently, tailored practical training sessions will be conducted for each selected technology, ensuring pilot owners are equipped with the necessary operational skills and knowledge. These sessions are designed to be hands-on, focusing on the functionalities, data management, and application of technologies pertinent to their specific needs. This approach ensures efficient learning and preparation, enabling pilot owners to effectively leverage these technologies to enhance data collection, analysis, and the overall success of their thematic explorations, thereby contributing to impactful insights and policymaking. Furthermore, PST members should comprehensively identify all existing and potential datasets within the scope of the Citizen Observatory and, particularly, those needed for the thematic co-exploration considered. This step aims to create an inventory of datasets, categorising them based on the current thematic co-exploration\u2019s ID, relevance, data type, source, and intended use. Furthermore, it is essential to specify how each dataset may be retrieved and accessed. This inventory serves as the foundation for the dataset metadata registration process in DataHub. Population of Dataset with the newly created inventory is the final action within this step.","title":"How can we achieve it?"},{"location":"HOWTO%20Thematic%20co-explorations/#resources_1","text":"Training materials folder Dataset Inventory template \u2013 knowledge asset with fields to identify every dataset\u2019s metadata that will be needed for the thematic co-explorations in each Citizen Observatory. It is recommended that for each thematic co-exploration a a copy of this template is performed and filled in. DataHub \u2013 HOWTO for tool used to register, search and view metadata about what datasets of a given thematic area are already available in the catalogue of such Citizen Observatory. The dataset inventory template completed for a given thematic co-exploration should be used as input to complete the datasets metadata in DataHub. GREENGAGE documentation \u2013 Overall documentation of the tools that are available in the GREEN Engine.","title":"Resources"},{"location":"HOWTO%20Thematic%20co-explorations/#13-core-team-onboarding","text":"","title":"1.3. Core Team Onboarding"},{"location":"HOWTO%20Thematic%20co-explorations/#responsible_2","text":"Pilot leader with the support of the PST.","title":"Responsible"},{"location":"HOWTO%20Thematic%20co-explorations/#objective_2","text":"To build a solid foundation for thematic co-explorations, the first stage involves creating a comprehensive map of potential stakeholders who could contribute significantly to the Core Team or join the ranks of the GREENGAGE Observers. This mapping will identify key individuals and organizations whose expertise, interests, and resources align with the goals of the exploration, ensuring a diverse and skilled team that can address the project's multifaceted challenges. Following this strategic identification, the second stage focuses on the onboarding of approximately 15 individuals who will play a crucial role in Phase 2 of the exploration. This group will be meticulously selected based on their potential to contribute to the project's success, ensuring they possess the necessary skills, knowledge, and enthusiasm. The onboarding process will equip them with a thorough understanding of the project objectives, methodologies, and expected outcomes, preparing them to effectively contribute to the collaborative efforts required for the exploration's success. Together, these stages aim to establish a strong, collaborative team capable of driving the project forward through its critical early phases.","title":"Objective"},{"location":"HOWTO%20Thematic%20co-explorations/#how-can-we-achieve-it_2","text":"To achieve the objectives, the PST will work closely with the pilot leader to identify and engage potential stakeholders who could contribute to the thematic co-exploration. This will involve the strategic mapping of stakeholders, identifying their roles, interests, and potential contributions to the project by filling the Stakeholder Mapping Canvas . A communication campaign will be created to invite citizens to take part in the thematic co-exploration. Following this to facilitate discussions and community engagement, Discourse will be integrated, providing a modern and user-friendly forum for exchange and collaboration. In parallel, WordPress will serve as our content management system, offering a versatile platform for information dissemination and interaction with the broader community. Through these platforms, the way in which the interested stakeholders can join and contribute to the thematic co-exploration will be communicated (following all the GDPR and ethical guidelines). Finally, a first workshop will be organized to onboard the Core Team, specifically designed for the Core Team, aimed at providing a comprehensive overview of the pilot's objectives, the governance model, and presenting a preliminary draft of the forthcoming steps. This session serves as a crucial platform for aligning the team's understanding and expectations, fostering an environment of open communication and collaboration from the outset. It is an opportunity not only to introduce team members to the broader vision and operational framework of the pilot but also to engage in a dialogue that encourages feedback, suggestions, and collective brainstorming.","title":"How can we achieve it?"},{"location":"HOWTO%20Thematic%20co-explorations/#resources_2","text":"Stakeholder Mapping Canvas \u2013 document to help you understand who are the stakeholders of your thematic co-exploration WordPress \u2013 HOWTO for tool essential to communicate the existence of a thematic co-exploration and allow dissemination of its results to the broader community Discourse \u2013 HOWTO for tool necessary to foster discussions and dialogue among GObs and allow them to request help and support each other Collaborative Environment \u2013 HOWTO for essential tool to govern, guide and trace the co-production process executed during the co-design and co-delivery of the thematic co-exploration Communication map (UnaLab and Scivil) Inclusion check list (SOCIO-BEE)","title":"Resources"},{"location":"HOWTO%20Thematic%20co-explorations/#14-core-team-training","text":"","title":"1.4. Core team training"},{"location":"HOWTO%20Thematic%20co-explorations/#responsible_3","text":"Pilot leader with the support of the PST.","title":"Responsible"},{"location":"HOWTO%20Thematic%20co-explorations/#objective_3","text":"The objective of this task is to provide the Core Team with a comprehensive and deep understanding of the thematic co-exploration area. This involves a detailed briefing and training session that encompasses several key components essential for its successful execution. Firstly, the Core Team will be extensively familiarized with the specific thematic area of exploration, including its significance, objectives, and the specific challenges or questions it aims to address. This foundational knowledge is crucial for aligning the team's efforts and ensuring a cohesive approach throughout the exploration process. In addition to understanding the thematic area, the Core Team will receive training on the ethical framework of GREENGAGE. This includes discussions on data privacy, ethical considerations in citizen science, and how to ensure that all activities conducted within the thematic co-exploration adhere to these ethical guidelines. Finally, the training will cover the technological tools available within the GREENGAGE ecosystem. The Core Team will learn how to utilize these technologies effectively, from data collection and analysis tools to platforms for collaboration and dissemination of findings. This technical training is essential for maximizing the impact of the thematic co-exploration, enabling the team to gather, analyse, and share data efficiently.","title":"Objective"},{"location":"HOWTO%20Thematic%20co-explorations/#how-can-we-achieve-it_3","text":"Training materials for the following topics have been made ready by the project and should be extended and customized for each thematic co-exploration: Data I Data II Co-design I Co-design II Co-design III [Air quality & Mobility Monitoring] Technology I [Technology II] [Technology III] [Policy-making & Governance] [Analysis & Visualisation] Create a training plan specific for the pilot. Create the needed training material. Perform at least two training sessions \u2013 1 hour each \u2013 to the Core Team. Make them as practical and applied as possible.","title":"How can we achieve it?"},{"location":"HOWTO%20Thematic%20co-explorations/#resources_3","text":"Training plan template (to be done) Materials for thematic co-exploration and citizen onboarding \u2013 TO BE PREPARED Folder where trainings are currently uploaded which will be published as part of the Knowledge Base aka GREENGAGE Academy of the main project\u2019s website","title":"Resources"},{"location":"HOWTO%20Thematic%20co-explorations/#phase-2-designing","text":"","title":"Phase 2: Designing"},{"location":"HOWTO%20Thematic%20co-explorations/#21-experiment-specification","text":"","title":"2.1. Experiment specification"},{"location":"HOWTO%20Thematic%20co-explorations/#responsible_4","text":"Pilot Leader with the support of the Core Team and the PST","title":"Responsible"},{"location":"HOWTO%20Thematic%20co-explorations/#objective_4","text":"To successfully navigate through the latter stages of the thematic co-exploration, specifically phases 3 (Experimenting) and 4 (Sharing), a meticulously crafted list of activities is essential. This list will outline the strategic actions required to not only implement the findings and insights gleaned from the exploration but also to disseminate these outcomes effectively to a broader audience. This comprehensive list of activities will serve as a roadmap, guiding the seamless transition from theory to practice and ensuring that the valuable insights generated through the exploration are shared widely and effectively, fostering greater impact, and facilitating informed decision-making.","title":"Objective"},{"location":"HOWTO%20Thematic%20co-explorations/#how-can-we-achieve-it_4","text":"Organise a second workshop conducted with the Core Team. This workshop aims to collaboratively design the specific actions, including the identification of concrete indicators or Key Performance Indicators (KPIs) for this thematic co-exploration, that will be evaluated in phases 3 (Experimenting) and 4 (Sharing) of the thematic co-exploration. By engaging in a co-design process, the Core Team will collectively determine the most effective strategies and activities to ensure the successful delivery and dissemination of the project's outcomes. Notice that the core team with the support of the Pilot Owner, must discuss about indicators and metrics suitable to assess the thematic co-exploration, e.g. if the purpose is to explore air pollution the following indexes could be considered Air Quality Index or Liveability Index. In these discussions, GREENGAGE\u2019s global KPIs should be considered (KPIs-monitoring-table.xlsx) giving place to a thematic co-exploration\u2019s concrete KPI, where partial contribution to global KPIs will be planned and how to contribute to the specific ones also defined. Furthermore, a new co-production process will be set up in GREENGAGE Collaborative Environment. Hence, the accomplishment of this step requires usage of all the tools belonging to the \u201cCommunity and co-production process management\u201d layer of GREEN Engine, in the following order: A entry created in preparing within the parent pilot / Citizen Observatory page, for the newly specified thematic co-exploration should be refined now. The instructions to use Wordpress should be followed to achieve this. Notice that GREENGAGE provides a default template that can be used to instantiate thematic co-explorations. A new set of categories for topics of discussion following the instructions of usage of the Discourse tool , should be created by the GObs leading and managing the thematic co-exploration. This will serve to foster the collaboration and dialogue among participants in the thematic co-exploration. Notice that GREENGAGE provides a default template with categories and candidate topics that is recommended to each thematic co-exploration. A new co-production process based on the \u201cGREENGAGE way\u201d co-production process schema or blueprint already loaded into Collaborative Environment should be instantiated. Besides, there is the need to involve the core team in the analysis of the available datasets and the reflection about dataset gaps to be filled in to complete the thematic co-exploration. Hence, a revisit of the dataset inventory initially created in the \u201cPreparing\u201d phase by pilot owners will be performed. Finally, it is time to think about how the different activities and events that will be organized with the community of observers. For that, the GREENGAGE community building plan knowledge asset may be used.","title":"How can we achieve it?"},{"location":"HOWTO%20Thematic%20co-explorations/#resources_4","text":"Work plan template Data management plan Data protocol specification Exemplary document reflecting about metrics and KPIs Reference KPI table for GREENGAGE project Collaborative Environment \u2013 HOWTO for essential tool to govern, guide and trace the co-production process executed during the co-design and co-delivery of the thematic co-exploration CS-CO-schema-process-specification \u2013 to specify the process of the thematic co-exploration ala \u201cGREENGAGE way\u201d Problem Statement Questionnaire document \u2013 Dataset Inventory template \u2013 knowledge asset with fields to identify every dataset\u2019s metadata that will be needed for the thematic co-explorations in each Citizen Observatory. It is recommended that for each thematic co-exploration, a copy of this template is completed. Template for planning of activities and events","title":"Resources"},{"location":"HOWTO%20Thematic%20co-explorations/#22-tools-and-resources-selection","text":"","title":"2.2. Tools and resources selection"},{"location":"HOWTO%20Thematic%20co-explorations/#responsible_5","text":"Pilot Leader with the support of the Core Team and the PST.","title":"Responsible"},{"location":"HOWTO%20Thematic%20co-explorations/#objective_5","text":"To achieve the set objectives of the thematic co-exploration efficiently, a meticulous selection process will be undertaken to finalize the list of tools and resources essential for the project. This encompasses a wide array of assets, including, but not limited to, existing datasets that are pivotal for research and analysis, comprehensive guidelines that outline procedural directives, and thorough documentation that provides detailed instructions and insights into the project's methodologies. Additionally, other instrumental resources such as analytical software, data management platforms, and collaborative tools will be evaluated for their relevance, efficiency, and compatibility with the thematic co-exploration\u2019s goals. This careful curation process ensures that every selected tool and resource aligns with the specific needs of the exploration, facilitating a streamlined, effective approach to achieving the research objectives.","title":"Objective"},{"location":"HOWTO%20Thematic%20co-explorations/#how-can-we-achieve-it_5","text":"To achieve this stage, we will conduct a third workshop with the Core Team. This workshop will focus on the selection of the tools and resources that will be used in the thematic co-exploration. The Core Team will collaboratively evaluate and select the most appropriate tools and resources, ensuring that they align with the project's objectives and can meet the specific needs of the experiment specified in T2.1. This analysis may rise the identification of gaps in the available tools and resources, which will be addressed in the T2.3. Finally, the Data Protocol to be used in the thematic co-exploration needs to be defined, reflecting upon it guided by Data Protocol Specification template provided by GREENGAGE project.","title":"How can we achieve it?"},{"location":"HOWTO%20Thematic%20co-explorations/#resources_5","text":"List of resources List of GREENGAGE tools \u2013 Overall documentation of the tools that are available in the GREEN Engine. Template for tools and resources selection \u2013 document that guides you in the process of selecting the knowledge assets, tools and resources necessary to run the thematic co-exploration Data protocol specification","title":"Resources"},{"location":"HOWTO%20Thematic%20co-explorations/#23-tools-and-resources-customization","text":"","title":"2.3. Tools and resources customization"},{"location":"HOWTO%20Thematic%20co-explorations/#responsible_6","text":"Pilot Owner with the support of PST and WP4.","title":"Responsible"},{"location":"HOWTO%20Thematic%20co-explorations/#objective_6","text":"To ensure the effectiveness and efficiency of the thematic co-exploration, it is necessary to undertake customizations on the tools utilized, including calibration, configuration, and possible extensions. This process is complemented by providing technical consultancy with the support of GREENGAGE consortium to address specific needs, thereby enhancing the functionality and usability of these tools for the observers. Concurrently, resources such as templates, documentation, and translations are optimized, along with the development of additional (thematic co-exploration specific) enablers or knowledge assets, to streamline observers' activities and facilitate their engagement. Data quality is also a priority, with efforts directed towards cleaning available datasets to ensure reliability and accuracy in analysis. Additionally, ad-hoc development may be undertaken to meet unique project requirements regarding datasets. Preparing evaluation materials forms a critical component of this task too, encompassing the creation of questionnaires, refinement of Key Performance Indicators (KPIs), and the selection or development of measurement instruments, all aimed at rigorously assessing the impact and outcomes of the co-exploration activities. In this task the tools and resources selection document, the data protocol and data management plan may be updated. New evaluation questionnaires and scripts to gather metrics values will be devised. Past CS projects\u2019 evaluation questionnaires such as those of SOCIO-BEE can be used to inspire the new questionnaires to be developed.","title":"Objective"},{"location":"HOWTO%20Thematic%20co-explorations/#how-can-we-achieve-it_6","text":"The Pilot Owner and PST will agree on a plan with WP4.","title":"How can we achieve it?"},{"location":"HOWTO%20Thematic%20co-explorations/#disclaimer","text":"All resources to be used from external locations (APIs, websites,...) must be listed and communicated in advance to the technical administration of the project.","title":"DISCLAIMER"},{"location":"HOWTO%20Thematic%20co-explorations/#resources_6","text":"Reference KPI table for GREENGAGE project Template for tools and resources selection \u2013 document that guides you in the process of selecting the knowledge assets, tools and resources necessary to run the thematic co-exploration Data protocol specification PRE SOCIO-BEE Citizen Science Activists Evaluation Questionnaire POST SOCIO-BEE Citizen Science Activists Evaluation Questionnaire","title":"Resources"},{"location":"HOWTO%20Thematic%20co-explorations/#24-testing-tools-and-resources","text":"","title":"2.4. Testing tools and resources"},{"location":"HOWTO%20Thematic%20co-explorations/#responsible_7","text":"Pilot Leader with the support of the Core Team and the PST.","title":"Responsible"},{"location":"HOWTO%20Thematic%20co-explorations/#objective_7","text":"The objective of this task is to perform a last test on the selected and adapted tools and resources. This task is essential to validate the functionality, compatibility, and performance of all tools and resources that have been selected to conduct the designed experiment. It involves rigorous testing scenarios that mimic real-world applications to identify any remaining issues or inefficiencies that could impact the project's success. This process not only encompasses technical tools such as data analysis software and data collection applications but also includes testing the usability and clarity of guidelines, documentation, and any other support materials. The objective is to guarantee that every component functions seamlessly together, providing a robust framework for observers and participants to engage effectively in the project activities.","title":"Objective"},{"location":"HOWTO%20Thematic%20co-explorations/#how-can-we-achieve-it_7","text":"The Core Team will have a workshop in which the selected tools and resources will be tested in real life scenarios to ensure the functionality, compatibility, and performance of them.","title":"How can we achieve it?"},{"location":"HOWTO%20Thematic%20co-explorations/#resources_7","text":"Template for tools and resources selection \u2013 document that guides you in the process of selecting the knowledge assets, tools and resources necessary to run the thematic co-exploration List of tools \u2013 Overall documentation of the tools that are available in the GREEN Engine. PRE SOCIO-BEE Citizen Science Activists Evaluation Questionnaire POST SOCIO-BEE Citizen Science Activists Evaluation Questionnaire Template for selected verified list of knowledge assets User acceptance test \u2013 this document guides about the different aspects that should be checked in the infrastructure in order validate its correct operation.","title":"Resources"},{"location":"HOWTO%20Thematic%20co-explorations/#phase-3-experimenting","text":"","title":"Phase 3: Experimenting"},{"location":"HOWTO%20Thematic%20co-explorations/#31-observers-onboarding","text":"","title":"3.1. Observers onboarding"},{"location":"HOWTO%20Thematic%20co-explorations/#responsible_8","text":"Pilot Owner with the support of the Core Team and the PST.","title":"Responsible"},{"location":"HOWTO%20Thematic%20co-explorations/#objective_8","text":"This task focuses primarily on integrating these key participants into the project's operational phase without delving into training and support, which are addressed in subsequent steps. This phase is about ensuring that GOs are well-informed about their roles, the project's objectives, and the overarching framework within which they will operate.","title":"Objective"},{"location":"HOWTO%20Thematic%20co-explorations/#how-can-we-achieve-it_8","text":"Similar to what we conducted in T1.3. , we will continue adapting and enhancing the communication campaign in the pilot. Together with this, we should update the Discourse and WordPress platforms to ensure that the information is up-to-date and accessible to the GOs. In essence, three main activities will be considered in this task: Promote and publicize thematic co-exploration Onboard further Citizen Observers Ensure ethics protocol are followed","title":"How can we achieve it?"},{"location":"HOWTO%20Thematic%20co-explorations/#resources_8","text":"Stakeholder Mapping Canvas \u2013 document to help you understand who are the stakeholders of your thematic co-exploration Discourse \u2013 HOWTO for tool necessary to foster discussions and dialogue among GObs and allow them to request help and support each other Communication map (UnaLab and Scivil) - available at Communication map Materials for thematic co-exploration and citizen onboarding \u2013 TO BE PREPARED Data ethics in participatory science GREENGAGE Data Ethics starter kit","title":"Resources"},{"location":"HOWTO%20Thematic%20co-explorations/#32-observers-training-support","text":"","title":"3.2. Observers Training &amp; support"},{"location":"HOWTO%20Thematic%20co-explorations/#responsible_9","text":"Pilot Leader with the support of the Core Team and the PST.","title":"Responsible"},{"location":"HOWTO%20Thematic%20co-explorations/#objective_9","text":"The core objective of this task is to train the community of observers in the tools and resources needed to execute the thematic co-exploration. The training will be complemented by ongoing support, providing the observers with the necessary guidance and assistance to navigate through the project's operational phase. This support will be delivered through various channels, including Discourse, direct communication, and documentation, ensuring that the observers have access to the resources and assistance they need to engage effectively in the project. Part of this step on training and support will be to organize the data collection process with observers and agree on the data collection protocol to perform analysis and reflection upon the collected data. The Data Protocol Template is a resource that should help organizing the data collection process.","title":"Objective"},{"location":"HOWTO%20Thematic%20co-explorations/#how-can-we-achieve-it_9","text":"GOs will attend a welcome event to train on the objectives of the experiment, the need to gather new data, the selected technologies by the core team, ethical aspects and other important topics identified in T2.1. This welcome event or workshop will be complemented with asynchronous training materials that will be available in the Knowledge Base aka GREENGAGE Academy of the main project\u2019s website. Training materials for the following topics have been made ready by the project and should be extended and customized for each thematic co-exploration: Data I Data II Co-design I Co-design II Co-design III [Air quality & Mobility Monitoring] Technology I [Technology II] [Technology III] [Policy-making & Governance] [Analysis & Visualisation]","title":"How can we achieve it?"},{"location":"HOWTO%20Thematic%20co-explorations/#resources_9","text":"Training plan template (to be adapted) Data crowdsourcing and capture layer \u2013 Overall documentation of the tools that are available in the GREEN Engine. Folder where trainings are currently uploaded which will be published as part of the Knowledge Base aka GREENGAGE Academy of the main project\u2019s website [DataProtocolSpecification] (https://aitonline.sharepoint.com/:w:/r/sites/HEUGREENGAGE337/Shared%20Documents/WP4%20CO%20enabling%20infrastructure%20and%20interoperable/D4.1%20GREEN%20Engine%20and%20manuals/Academy/resources/DataProtocolSpecification.docx?d=w086eacc673a846eaac82aa959ef89d5c&csf=1&web=1&e=dfqsBH)","title":"Resources"},{"location":"HOWTO%20Thematic%20co-explorations/#33-data-collection","text":"","title":"3.3. Data collection"},{"location":"HOWTO%20Thematic%20co-explorations/#responsible_10","text":"Pilot Leader of the GOs","title":"Responsible"},{"location":"HOWTO%20Thematic%20co-explorations/#objective_10","text":"This step aims to retrieve the data identified in the previous phase. This data may come from three different sources: Data that are already available in Open Data portals or provided by Copernicus data. This process will involve the creation of the pipelines (ingestion sources) that will introduce each dataset in DataHub, which will allow us to retrieve the data from the source and store it in the centralised repository. The technical team should create the ingestion sources with the pilot owners' support. Data that are not available and need to be collected through human collaboration. This process will involve the creation of data gathering campaigns that include missions (tasks) to be completed by the public. The missions will be designed by the thematic co-exploration\u2019s moderators, with the support of the technical team, to retrieve specific data. The missions, thus, the data capture process, will involve using specific tools designed for this task, such as MODE, GREENGAGE app or MindEarth. Notice that mission is a GREENGAGE concept denoting then need to collect certain data in a given area and time period to address a certain data gap identified within a thematic co-exploration. Apart from quantitative data, during this step, qualitative data about perceptions of observers regarding different aspects associated to the thematic co-exploration will also be gathered. This will involve making observers and even external citizens to the thematic co-exploration to be involved in completing surveys, realizing in depth interviews with stakeholders of the thematic co-exploration or organizing focus groups, local walks and other alternative mechanisms where discussion and reflection around the thematic co-exploration can be generated.","title":"Objective"},{"location":"HOWTO%20Thematic%20co-explorations/#how-can-we-achieve-it_10","text":"This must be done in three different ways, depending on the data availability:","title":"How can we achieve it?"},{"location":"HOWTO%20Thematic%20co-explorations/#data-already-available","text":"For the data that is already available, the technical team will create the ingestion sources that will allow the data to be retrieved from the source and stored in the centralised repository. The ingestion sources can be created using Apache NiFi, allowing us to retrieve data from APIs, FTP servers, and other sources. Alternatively, other ingestion sources, such as Python scripts, may require programmatic solutions. Finally, the ingestion sources can also be configured using Apache Druid, allowing us to load data directly into the project's database. The technical team will be responsible for creating the ingestion sources, while the pilot owners will be responsible for providing the necessary information and testing that the ingested data complies with the thematic co-exploration\u2019s data needs.","title":"Data already available"},{"location":"HOWTO%20Thematic%20co-explorations/#data-not-available","text":"To achieve this, the missions to collect data should be configured and launched. This process involves Pilot leaders and project\u2019s technical team\u2019s representatives that will help them configure the tools that will be used for the data collection or capture process. Once the tools are configured, observers should be trained to use them. During the operation of the data collection campaigns, the PST members should monitor their progress and performance, and the technical team should ensure that the data is being collected and stored correctly. Data capturing apps and sensors belonging to \u201cData crowdsourcing and capture layer\u201d of GREEN Engine, such as MODE, GREENGAGE app or MindEarth, will be used to collect the data.","title":"Data not available"},{"location":"HOWTO%20Thematic%20co-explorations/#qualitative-data","text":"Questionnaires, surveys, interviews, focus groups or local walks, among other alternative activities, will be organized. The section and customization of such evaluation materials and instruments was performed in Task 2.3, based on examples of such mechanisms already published as knowledge assets within the GREENGAGE Academy.","title":"Qualitative data"},{"location":"HOWTO%20Thematic%20co-explorations/#resources_10","text":"Resources and budget template \u2013 this document developed in T2.2. can be used to keep track of the resources and budget planned for the data collection process. HOWTOs to gather data from Copernicus and from Open Data portals considered in the project (TBD) Apache NiFi official documentation \u2013 Official documentation for tool to create ingestion sources Apache NiFi GREENGAGE documentation \u2013 HOWTO for tool in GREENGAGE data stack configured for creating ingestion sources Apache Druid official documentation \u2013 HOWTO for tool to load data directly into the centralised datastore offered by GREENGAGE MODEs GREENGAGE documentation \u2013 HOWTO for tool to collect commuting data MindViews GREENGAGE documentation \u2013 HOWTO for tool to collect geo-tagged street-level imagery GRENGAGE apps GREENGAGE documentation \u2013 HOWTO for tool to collect data via campaigns AtmoTube PRO sensor webpage \u2013 webpage of sensor selected to measure air quality data in pilots [Sensors integration in GREENGAGE] (https://greengage-project.github.io/Documentation/tools/sensorsIntegration/) \u2013 HOWTO to off-the-shelf sensors (ATMOtube and sensing apps, e.g. Noisetube ) to be used in the project Data management plan Data protocol specification Questionnaires of evaluation from SOCIO-BEE","title":"Resources"},{"location":"HOWTO%20Thematic%20co-explorations/#34-data-combination-analysis-visualization","text":"","title":"3.4. Data combination, analysis &amp; visualization"},{"location":"HOWTO%20Thematic%20co-explorations/#responsible_11","text":"Pilot Leader with the support of PST and WP4.","title":"Responsible"},{"location":"HOWTO%20Thematic%20co-explorations/#objective_11","text":"This multifaceted task begins with preprocessing steps to cleanse datasets of any inconsistencies, errors, or irrelevant information using automated data ingestion workflows through tools like Apache NiFi, channelling data efficiently into Apache Druid for optimal analysis readiness. The aim is to maintain data accuracy and integrity for a deeper, more precise understanding critical for informed decision-making. Concurrently, the development of interactive dashboards aims to transform this complex, cleansed data into accessible visual formats, enabling stakeholders\u2014including policymakers, researchers, and citizens\u2014to engage with data themes and patterns in a collaborative exploration environment. These dashboards, providing real-time monitoring and analysis, are crucial for facilitating quick, informed decisions and policy development, while promoting transparency and inclusivity. Together, these objectives strive to simplify data analysis and visualization processes through interlinked datasets and shared schemas, ultimately fostering a shared understanding and enhancing the thematic co-exploration\u2019s impact.","title":"Objective"},{"location":"HOWTO%20Thematic%20co-explorations/#how-can-we-achieve-it_11","text":"To successfully create workflows that can process the data into the desired format we have several ways to do it: creating a NiFi Flow, creating a python script or manually. NiFi flow: Apache NiFi is a powerful tool that allows us to create data pipelines that can be scheduled to run periodically. This tool is very useful for data ingestion, as it allows us to fetch data from APIs, FTP servers, and other sources. Additionally, NiFi can be used to clean and transform data, ensuring that it is in the desired format and structure. Finally, NiFi can be used to load data into Apache Druid, ensuring that the data is correctly ingested into the database. Notably, in the data curation and cleaning process the Data quality VRVIS tool can also be useful. Python script: Develop Python scripts for more complex data transformations that might not be directly feasible within NiFi. This can include data cleaning, normalization, or feature engineering tasks. It can also be integrated with Apache NiFi to run as part of the flow. Furthermore, by using the Apache Druid API we can directly load data into the database. Manually: For simple data transformations, it might be more efficient to perform them manually. This can include tasks such as renaming columns, changing data types, or removing columns. However, this approach is not scalable and should only be used for simple tasks. Once the data is pre-processed and cleaned the analysis may be conducted by combining and exploring the data using Apache Superset and Apache Druid. Apache Superset will allow us to create and share rich data visualizations and dashboards, providing an intuitive interface for users to interact with the data. Apache Druid will be utilized for its real-time data analytics capabilities, ensuring that our dashboards can display the most current data efficiently. Additionally, the integration of Apache NiFi will streamline the data flow into these systems, ensuring a consistent and reliable data pipeline. The combination of these tools, supported by thorough documentation and examples of integration, will enable us to create dynamic and insightful thematic co-exploration dashboards that serve the diverse needs of our project stakeholders.","title":"How can we achieve it?"},{"location":"HOWTO%20Thematic%20co-explorations/#resources_11","text":"Apache NiFi documentation \u2013 HOWTO for tool to create preprocessing flows Apache Druid documentation \u2013 HOWTO for tool to load data into the project's database Apache Superset documentation \u2013 HOWTO for tool to create dashboards and visualizations ThematicCoExplorationSpec \u2013 continuously updated document where to revisit the objectives of the thematic co-exploration and generate dashboards that address them. Once visualizations are generated, it is convenient to edit the corresponding section in the template, so tracking the progress of the thematic co-exploration. Data quality VRVIS \u2013 HOWTO for tool to handle data quality assurance on the gathered datasets Apache Superset documentation \u2013 official documentation for tool to create dashboards and visualizations HOWTO GISAT tool to do these visualizations \u2013 documentation for tool designed to facilitate the seamless integration, exploration, visualization, and analysis of diverse datasets HOWTO thematic co-exploration \u2013 step by step example on how to co-produce a simple thematic co-exploration, using real datasets. It includes a video illustrating the usage of the different data management tools that intervene.","title":"Resources"},{"location":"HOWTO%20Thematic%20co-explorations/#35-experiment-evaluation","text":"","title":"3.5. Experiment evaluation"},{"location":"HOWTO%20Thematic%20co-explorations/#responsible_12","text":"Pilot owners, GREENGAGE Observers and PST","title":"Responsible"},{"location":"HOWTO%20Thematic%20co-explorations/#objective_12","text":"This objective involves an evaluation of the experiment\u2019s objectives, outputs, and overarching goals have been achieved, alongside a detailed review of how the planned activities were implemented in practice. By meticulously examining the alignment between planned and actual activities, the assessment will not only highlight the thematic co-exploration's accomplishments and areas of strength but also uncover any discrepancies or challenges encountered along the way. Furthermore, this process is instrumental in identifying valuable opportunities for learning and improvement, facilitating a reflective analysis that can inform future initiatives.","title":"Objective"},{"location":"HOWTO%20Thematic%20co-explorations/#how-can-we-achieve-it_12","text":"Implement the evaluation strategy planned sketched in \u201cPhase 1 Preparing\u201d and refined in \u201cPhase 2 Designing\u201d (defined KPIs and mechanisms for their assessment). Facilitate spaces for collective reflection about barriers and enablers encountered during the Preparing, Designing, and Experimenting phases. Include all participants involved (PST members, pilot owners, Core Group, and Observers). Think on the possible following iteration objective, goals, and outputs. Identify possibilities to mitigate or overcome barriers and enhance enablers based on what have you have learned so far. Adapt or change your thematic co-exploration specification, work plan or other planning strategies in place for future GO activities.","title":"How can we achieve it?"},{"location":"HOWTO%20Thematic%20co-explorations/#resources_12","text":"TBD","title":"Resources"},{"location":"HOWTO%20Thematic%20co-explorations/#phase-4-sharing","text":"","title":"Phase 4: Sharing"},{"location":"HOWTO%20Thematic%20co-explorations/#41-storytelling-from-findings","text":"","title":"4.1. Storytelling (from findings)"},{"location":"HOWTO%20Thematic%20co-explorations/#responsible_13","text":"Pilot owners, GREENGAGE Observers (GOs) and PST","title":"Responsible"},{"location":"HOWTO%20Thematic%20co-explorations/#objective_13","text":"The objective in the \"Generation of storylines for wide dissemination\" stage is to craft engaging and accessible narratives that convey the thematic co-exploration's key findings and insights to a broad audience. These storylines are designed to translate complex data and research outcomes into formats that are both engaging and easily understandable by the public. The aim is to underscore the thematic co-exploration's significance and its potential impact on policymaking, community initiatives, and scientific research. By doing so, the goal is to foster increased public awareness and interest in the project, thereby enhancing its overall reach and influence. To achieve wide visibility and impact, these storylines will be disseminated through various media channels, tailored to resonate with diverse audiences, including: GREENGAGE partners and pilots. Local communities addressed by or emerging through the Observatories. Audiences beyond GREENGAGE that are interested in tools, topics, or areas addressed by the GO","title":"Objective"},{"location":"HOWTO%20Thematic%20co-explorations/#how-can-we-achieve-it_13","text":"To effectively generate storylines for wide dissemination, we will utilize a range of tools specified in our project documentation. The Communications and Outreach Team will collaborate closely with the Data Analysis Team, utilizing Apache Superset for creating compelling data visualizations that bring complex data to life in an easily digestible format. These visual elements will be integral in crafting storylines that are both informative and engaging to the public. Additionally, we will leverage WordPress and Discourse platforms to draft, refine, and share these narratives, ensuring they are tailored to resonate with diverse audience segments. The integration of these tools will enable us to effectively communicate the thematic co-exploration's impact and findings through various media channels, from social media to press releases, thereby maximizing reach and fostering greater public engagement with the project's outcomes.","title":"How can we achieve it?"},{"location":"HOWTO%20Thematic%20co-explorations/#resources_13","text":"Course on Storytelling for Citizen Science \u2013 this course gives some tips about how to perform storytelling for Citizen Science projects Presentation of Storytelling and Citizen Science \u2013 this presentation describes about the value of storytelling in Citizen Science. It takes us through several viewpoints and tools you can use to tell your story, considering the needs of the audience, the human instinct for storytelling, how to use it to make your Citizen Science project exciting and meaningful to other people, and encourages you to think about ways to use these tools to reach more audiences Communication map (UnaLab and Scivil) - available at Communication map","title":"Resources"},{"location":"HOWTO%20Thematic%20co-explorations/#42-impact-on-policies","text":"","title":"4.2. Impact on policies"},{"location":"HOWTO%20Thematic%20co-explorations/#responsible_14","text":"Pilot Owners, GREENGAGE Observers (GOs), PST.","title":"Responsible"},{"location":"HOWTO%20Thematic%20co-explorations/#objective_14","text":"The generation of policy briefs for policymakers, a key objective of this project step, involves synthesizing research findings and data into clear, concise, and actionable recommendations. These briefs are crafted to effectively communicate pivotal insights and findings to policymakers, providing them with evidence-based guidance for policy interventions. By directly aligning with current policy debates and challenges, these briefs aim to bridge the gap between research and practical policymaking, offering solutions that are grounded in thorough data analysis. This approach not only enhances the relevancy of the research to ongoing policy discussions but also aims to significantly influence policy decisions and strategies, thus amplifying the overall impact of a given thematic co-exploration.","title":"Objective"},{"location":"HOWTO%20Thematic%20co-explorations/#how-can-we-achieve-it_14","text":"To achieve this, we will first aggregate and analyse the data collected and insights generated throughout the project, using tools like Apache Druid for real-time analytics and Apache Superset for data visualization. These tools will help in identifying key trends and findings that are most relevant to policymaking. The policy analysis team will then work closely with research teams to distil these insights into policy briefs, ensuring that they are both data-driven and aligned with current policy needs. Communication specialists will refine these briefs to be clear and compelling, making them accessible to non-specialist policymakers. Throughout this process, a given thematic co-exploration\u2019s coordinators will ensure that the briefs align with the strategic objectives of the thematic co-exploration and effectively convey its potential impact on policy and governance.","title":"How can we achieve it?"},{"location":"HOWTO%20Thematic%20co-explorations/#resources_14","text":"Collaborative Environment \u2013 HOWTO about environment to collaborate in the policy briefs' generation in a centralised repository. The CE will suggest templates to guide the elaboration of policy briefs. Policy brief guideline \u2013 public access web document explaining how to format a policy brief Example of Policy brief \u2013 example of policy brief in the domain of Citizen Science that could inspire those embarking in the preparation of a policy brief","title":"Resources"},{"location":"HOWTO%20Thematic%20co-explorations/#43-experiment-sustainability","text":"","title":"4.3. Experiment sustainability"},{"location":"HOWTO%20Thematic%20co-explorations/#responsible_15","text":"Pilot Owners, GREENGAGE Observers (GOs), PST.","title":"Responsible"},{"location":"HOWTO%20Thematic%20co-explorations/#objective_15","text":"The objective of identifying mechanisms for sustaining thematic co-explorations entails developing a strategic framework that ensures the long-term and continued relevance of these initiatives. This involves a thorough analysis of the resources, partnerships, and methodologies that have contributed to the success of the exploration, with an eye towards replicating and adapting these elements for future cycles. Critical to this process is the engagement with stakeholders to understand their needs, motivations, and feedback, which can inform the refinement of objectives and approaches for subsequent iterations. As part of this objective, the preparation for a possible new iteration phase or launch of a new thematic co-exploration as side effect of this thematic co-exploration begins with a clear articulation of lessons learned, achievements, and challenges from the first cycle, setting the stage for enhanced planning, execution, and impact in the next phase. This proactive approach not only ensures that thematic co-explorations remain dynamically aligned with evolving research landscapes and societal needs but also fosters a culture of continuous improvement and innovation within the community of practice.","title":"Objective"},{"location":"HOWTO%20Thematic%20co-explorations/#how-can-we-achieve-it_15","text":"To achieve the objective of sustaining thematic co-explorations for future iterations, a multifaceted approach is essential. Initially, crafting an attractive and compelling report that encapsulates the achievements, insights, and value of the exploration becomes crucial. This report should be designed to resonate with both current stakeholders and potential future participants, highlighting the exploration's impact and the opportunities it presents. Following this, a thorough review and discussion of the experiment's evaluation, as outlined in T3.5. will be instrumental. This step involves critically assessing the outcomes and feedback to identify areas for improvement, such as optimizing engagement activities and incorporating successful stories into the communication strategy, thereby enriching the narrative and appeal of the project. Furthermore, validating the pertinence and relevance of the thematic co-exploration as an enabler of policymaking is paramount for the second iteration. This includes a detailed review of the local situation and the exploration's alignment with policy objectives, ensuring that any necessary adjustments are made to enhance its effectiveness and impact. Through this comprehensive approach, the project aims to not only solidify its foundation for sustainability but also ensure its continued evolution and relevance in contributing to meaningful policy development and stakeholder engagement.","title":"How can we achieve it?"},{"location":"HOWTO%20Thematic%20co-explorations/#resources_15","text":"Open Data, Software and Code Guidelines \u2013 These guidelines relate to the Open Research Europe policy on data availability","title":"Resources"},{"location":"guidelines/","text":"Comprehensive Guidelines for Documenting and Integrating Tools 1. Duplicating the Documentation Template Action : Begin by duplicating the folder docs/tools/TOOLNAME_template . Purpose : This template serves as a foundational structure for documenting new tools, ensuring consistency across your documentation. 2. Describing the Tool in index.md Content Requirements : In index.md , offer a comprehensive overview of the tool. This should include its purpose, key features, and any unique aspects that make it beneficial for your project. Incorporating External Documentation : If there are external resources or official documentation for the tool, link them. This helps users understand the broader context and capabilities of the tool. External resources should be listed in the References section at the end of index.md, check the template provided. Notice that resources like images or PDFs should be stored in subfolder assets . Example Structure : ```markdown # Tool Name ## Introduction to Tool Name [Detailed introduction, including the tool's purpose and general functionality.] ## Features [Detailed list of features, explaining how each aspect adds value to the project.] ## Use Case Illustration [Detailed description of a use case, demonstrating how the tool integrates into workflows or solves specific problems.] ## References [Detailed list of references, include as one reference PDF version of catalogue entry for your tool.] ``` 3. Usage and Integration Guide in integration.md Content : Provide some brief description on how the tools is supposed to be used. If your tool is going to be offered as a SaaS extend such usage highlights, always referring to further documentation. In case your app is designed to be integrated with third party tools, then provide a clear, step-by-step guide for integrating the tool with third-party services. Remember provide a channel to ask for support about the tool. Structure : Usage instructions : Brief usage instructions of the tool. Add references to external resources where further details about usage are given if available. Otherwise, try to make these descriptions self-contained, including snaphshots, URLs and other usage tips. Requesting Credentials : Outline the process for obtaining necessary access credentials. Configuration Steps : Detail the steps for integrating the tool within their systems, including any specific settings or parameters. Testing and Validation : Emphasize the importance of testing the integration and provide guidance on troubleshooting common issues. 4. Proof of Concept Code in examples Purpose : Supply ready-to-use code examples in various programming languages to demonstrate practical integration or programmatic consumption/usage of the tool documented. Details : Each example should include comprehensive comments explaining each step and its relevance to the integration/usage process. 5. Documenting Examples in examples/index.md Structure : For each example, include objective of example, prerequisites, a detailed walkthrough of the project structure, and step-by-step instructions for setup and execution. Clarity : Ensure that the instructions are clear, concise, and suitable for users with varying levels of expertise. 6. Summarizing Tool in docs/index.md Overview : Provide a brief yet informative summary of the tool, highlighting its significance and utility within the project. Example Entry : ```markdown ### Tool Name Directory : /tools/toolname Documentation Link : Tool Documentation Description : A concise summary highlighting the key features and benefits of the tool. Insight into how it integrates with and enhances the project. Integration Example: Node.js Integration Example: Python ``` These guidelines aim to provide a thorough and user-friendly approach to documenting and integrating tools, thereby enhancing the accessibility and usability of your project's documentation.","title":"Guidelines"},{"location":"guidelines/#comprehensive-guidelines-for-documenting-and-integrating-tools","text":"","title":"Comprehensive Guidelines for Documenting and Integrating Tools"},{"location":"guidelines/#1-duplicating-the-documentation-template","text":"Action : Begin by duplicating the folder docs/tools/TOOLNAME_template . Purpose : This template serves as a foundational structure for documenting new tools, ensuring consistency across your documentation.","title":"1. Duplicating the Documentation Template"},{"location":"guidelines/#2-describing-the-tool-in-indexmd","text":"Content Requirements : In index.md , offer a comprehensive overview of the tool. This should include its purpose, key features, and any unique aspects that make it beneficial for your project. Incorporating External Documentation : If there are external resources or official documentation for the tool, link them. This helps users understand the broader context and capabilities of the tool. External resources should be listed in the References section at the end of index.md, check the template provided. Notice that resources like images or PDFs should be stored in subfolder assets . Example Structure : ```markdown # Tool Name ## Introduction to Tool Name [Detailed introduction, including the tool's purpose and general functionality.] ## Features [Detailed list of features, explaining how each aspect adds value to the project.] ## Use Case Illustration [Detailed description of a use case, demonstrating how the tool integrates into workflows or solves specific problems.] ## References [Detailed list of references, include as one reference PDF version of catalogue entry for your tool.] ```","title":"2. Describing the Tool in index.md"},{"location":"guidelines/#3-usage-and-integration-guide-in-integrationmd","text":"Content : Provide some brief description on how the tools is supposed to be used. If your tool is going to be offered as a SaaS extend such usage highlights, always referring to further documentation. In case your app is designed to be integrated with third party tools, then provide a clear, step-by-step guide for integrating the tool with third-party services. Remember provide a channel to ask for support about the tool. Structure : Usage instructions : Brief usage instructions of the tool. Add references to external resources where further details about usage are given if available. Otherwise, try to make these descriptions self-contained, including snaphshots, URLs and other usage tips. Requesting Credentials : Outline the process for obtaining necessary access credentials. Configuration Steps : Detail the steps for integrating the tool within their systems, including any specific settings or parameters. Testing and Validation : Emphasize the importance of testing the integration and provide guidance on troubleshooting common issues.","title":"3. Usage and Integration Guide in integration.md"},{"location":"guidelines/#4-proof-of-concept-code-in-examples","text":"Purpose : Supply ready-to-use code examples in various programming languages to demonstrate practical integration or programmatic consumption/usage of the tool documented. Details : Each example should include comprehensive comments explaining each step and its relevance to the integration/usage process.","title":"4. Proof of Concept Code in examples"},{"location":"guidelines/#5-documenting-examples-in-examplesindexmd","text":"Structure : For each example, include objective of example, prerequisites, a detailed walkthrough of the project structure, and step-by-step instructions for setup and execution. Clarity : Ensure that the instructions are clear, concise, and suitable for users with varying levels of expertise.","title":"5. Documenting Examples in examples/index.md"},{"location":"guidelines/#6-summarizing-tool-in-docsindexmd","text":"Overview : Provide a brief yet informative summary of the tool, highlighting its significance and utility within the project. Example Entry : ```markdown ### Tool Name Directory : /tools/toolname Documentation Link : Tool Documentation Description : A concise summary highlighting the key features and benefits of the tool. Insight into how it integrates with and enhances the project. Integration Example: Node.js Integration Example: Python ``` These guidelines aim to provide a thorough and user-friendly approach to documenting and integrating tools, thereby enhancing the accessibility and usability of your project's documentation.","title":"6. Summarizing Tool in docs/index.md"},{"location":"thematic_coexploration_example/","text":"Thematic co-exploration example Thematic co-exploration contextualisation Thematic co-exploration in the context of Citizen Observatories refers to a collaborative approach where citizens actively participate alongside scientists and other stakeholders in exploring specific themes or topics related to environmental monitoring and observation. Citizen Observatories are organizations that leverage the collective efforts of individuals, often non-scientists, to gather, share, and analyse environmental data, typically facilitated by digital tools and technologies. In a thematic co-exploration, the focus is on a particular theme or subject matter, such as air quality, water pollution, biodiversity, or climate change phenomena. The key aspects of this approach include: Collaborative Research: Citizens collaborate with scientists, environmental experts, policy-makers, and other stakeholders. This collaboration is not just about data collection but also involves jointly defining research questions, methodologies, and analysis. Citizen Engagement: Citizens are not merely data collectors; they are integral to the research process. They contribute through observations, local knowledge, and experiences, thereby adding valuable context and richness to scientific data. Mutual Learning: There is a reciprocal transfer of knowledge. Scientists can learn from the lived experiences and local knowledge of citizens, while citizens gain a better understanding of scientific methods and environmental issues. Empowerment and Ownership: By engaging in the research process, citizens gain a sense of ownership and empowerment. This can lead to increased awareness and action on environmental issues at the local level. Technology Use: Digital tools such as mobile apps, collaborative online platforms, and sensor technologies, e.g. wearables, often facilitate thematic co-exploration. These tools enable easy data collection, sharing, and visualisation. Community Building: This approach fosters community engagement and networking, as it brings together people with shared interests in specific environmental issues. Policy Impact: The insights gained from thematic co-exploration can inform environmental policies and decision-making processes, making them more reflective of local needs and conditions. In summary, a thematic co-exploration in Citizen Observatories represents a participatory, inclusive approach to environmental research and monitoring, emphasising collaboration, mutual learning, and community engagement. Citizen Observer Journey In GREENGAGE, we propose a Citizen Observer Journey that represents the pathway that the citizen observers follow when conducting a thematic co-exploration. This journey is a structured pathway designed to empower citizens and stakeholders to actively engage in environmental observation and decision-making processes. It unfolds in three distinct yet interconnected phases, each integral to the overall success and impact of the GREENGAGE thematic co-explorations. These phases are: Community and Co-production Process Management: Throughout this phase, the emphasis is on building a strong, informed, and active community which collaborates through a co-production process. Data Crowdsourcing and Capture: Based on the groundwork of the previous phase, e.g. definition of hypothesis, research questions formulation or datasets selection, among others, this phase materialises into concrete data collection activities. It is characterized by active participation, leveraging technology to gather vital environmental data. Data Analysis and Insights Generation: In this phase the collected data is transformed into actionable insights. This phase is where the data, once transformed in actionable information, becomes a powerful tool for understanding and influencing environmental policy. The following figure shows the Citizen Observer Journey and the tools that are used in the different phases that it comprises: Example of a simple thematic co-exploration This example explains the steps to conduct a complete thematic co-exploration process using some of the tools provided by the GREENGAGE platform. To support the explanation, we will employ a video, however, in the video there are only covered the steps belonging to the third phase of the Citizen Observer Journey. Thus, we will complement the video with a textual explanation of the steps covered in the first and second phases. Objective The main objective of this thematic co-exploration is to analyse the air quality data from the city of Bristol. The analysis is conducted to identify the air pollution in the city and the different factors that may affect it. Thus, we gather the data from the different sensors deployed in the city and load them into the database. Then we analyse the data and create visualisations to facilitate the understanding of the data. Finally, we load the data and visualisations into the data catalogue to facilitate the access to them and foster the dissemination and adaption of the results to maximize impact. Dataset For this example we employed a single dataset from the Air Quality Data Continuous dataset. This dataset contains the air quality data from the city of Bristol. The data is collected from the different sensors deployed in the city. The data is collected every hour and it is stored in the dataset. The dataset contains the following fields: Date_Time Site_ID NOx NO2 NO PM10 O3 Temperature ObjectId ObjectId2 NVPM10 VPM10 NVPM2_5 PM2_5 VPM2_5 CO RH Pressure SO2 Note that not all the fields are present in all the rows. Video Some of the steps below are linked to the time in the video where they are covered. You can find the video in the following link or by clicking on the image below: Steps covered in this thematic co-exploration Thematic co-exploration specification : In this step, we should specify the thematic co-exploration that we want to conduct. By employing the Collaborative Environment tool, the thematic co-exploration owners may specify the phases, objectives and tasks that should be conducted in the thematic co-exploration. Furthermore, the tool allows to specify the different roles upon which they will participate in the thematic co-exploration and the users assigned to each role. Community building : Through Discourse and Wordpress tools, the thematic co-exploration owners may create a community around the thematic co-exploration. The community thus could discuss the different aspects of the thematic co-exploration and the different tasks that should be conducted. Furthermore, through it the community should share the results of the thematic co-exploration and discuss them. Data collection : In this step, we collect the data from the different sensors that deployed in the city. In this example, we use the Apache NiFi tool to collect the data from the open API because it cannot be accessed by Apache Druid directly. Thus, we created a NiFi flow that iteratively gathers the data from the API and stores in a local file. However, in another scenario, tools as MindEarth , MODE or GREENGAGEs IoT sensors could be used to collect the data. Data ingestion and storage : In this step, we ingest the data from the local file to the database. In this example, we use the Apache Druid datastore tool to ingest the data from the local file. Data visualisation and dashboard creation : In this step, we load the data from the database into Apache Superset to create visualisations that may support the decision-making process. Furthermore, we create a dashboard aggregating the different visualisations to facilitate the understanding of the data. This visualisations, alternatively, can also be created using the Data Quality Dashboard tool. Data catalogue : In this step, we add references to the data and the visualisations created in previous steps to the DataHub tool. DataHub allows to create a catalog of data that shows the schema, documentation, lineage, properties and common queries that can be done to the data. Furthermore, the visualisations can also be referenced from this catalog and the connections between them can be established. It is a good tool where a given thematic co-exploration results can be traced. Insights generation : In this step the participants of the thematic co-exploration analyse the data and the visualisations generated from it and extract insights from it. These insights may be shared with the community by using the tools covered in the Community and Co-production Process Management phase of the Citizen Observer Journey.","title":"Thematic co-exploration example"},{"location":"thematic_coexploration_example/#thematic-co-exploration-example","text":"","title":"Thematic co-exploration example"},{"location":"thematic_coexploration_example/#thematic-co-exploration-contextualisation","text":"Thematic co-exploration in the context of Citizen Observatories refers to a collaborative approach where citizens actively participate alongside scientists and other stakeholders in exploring specific themes or topics related to environmental monitoring and observation. Citizen Observatories are organizations that leverage the collective efforts of individuals, often non-scientists, to gather, share, and analyse environmental data, typically facilitated by digital tools and technologies. In a thematic co-exploration, the focus is on a particular theme or subject matter, such as air quality, water pollution, biodiversity, or climate change phenomena. The key aspects of this approach include: Collaborative Research: Citizens collaborate with scientists, environmental experts, policy-makers, and other stakeholders. This collaboration is not just about data collection but also involves jointly defining research questions, methodologies, and analysis. Citizen Engagement: Citizens are not merely data collectors; they are integral to the research process. They contribute through observations, local knowledge, and experiences, thereby adding valuable context and richness to scientific data. Mutual Learning: There is a reciprocal transfer of knowledge. Scientists can learn from the lived experiences and local knowledge of citizens, while citizens gain a better understanding of scientific methods and environmental issues. Empowerment and Ownership: By engaging in the research process, citizens gain a sense of ownership and empowerment. This can lead to increased awareness and action on environmental issues at the local level. Technology Use: Digital tools such as mobile apps, collaborative online platforms, and sensor technologies, e.g. wearables, often facilitate thematic co-exploration. These tools enable easy data collection, sharing, and visualisation. Community Building: This approach fosters community engagement and networking, as it brings together people with shared interests in specific environmental issues. Policy Impact: The insights gained from thematic co-exploration can inform environmental policies and decision-making processes, making them more reflective of local needs and conditions. In summary, a thematic co-exploration in Citizen Observatories represents a participatory, inclusive approach to environmental research and monitoring, emphasising collaboration, mutual learning, and community engagement.","title":"Thematic co-exploration contextualisation"},{"location":"thematic_coexploration_example/#citizen-observer-journey","text":"In GREENGAGE, we propose a Citizen Observer Journey that represents the pathway that the citizen observers follow when conducting a thematic co-exploration. This journey is a structured pathway designed to empower citizens and stakeholders to actively engage in environmental observation and decision-making processes. It unfolds in three distinct yet interconnected phases, each integral to the overall success and impact of the GREENGAGE thematic co-explorations. These phases are: Community and Co-production Process Management: Throughout this phase, the emphasis is on building a strong, informed, and active community which collaborates through a co-production process. Data Crowdsourcing and Capture: Based on the groundwork of the previous phase, e.g. definition of hypothesis, research questions formulation or datasets selection, among others, this phase materialises into concrete data collection activities. It is characterized by active participation, leveraging technology to gather vital environmental data. Data Analysis and Insights Generation: In this phase the collected data is transformed into actionable insights. This phase is where the data, once transformed in actionable information, becomes a powerful tool for understanding and influencing environmental policy. The following figure shows the Citizen Observer Journey and the tools that are used in the different phases that it comprises:","title":"Citizen Observer Journey"},{"location":"thematic_coexploration_example/#example-of-a-simple-thematic-co-exploration","text":"This example explains the steps to conduct a complete thematic co-exploration process using some of the tools provided by the GREENGAGE platform. To support the explanation, we will employ a video, however, in the video there are only covered the steps belonging to the third phase of the Citizen Observer Journey. Thus, we will complement the video with a textual explanation of the steps covered in the first and second phases.","title":"Example of a simple thematic co-exploration"},{"location":"thematic_coexploration_example/#objective","text":"The main objective of this thematic co-exploration is to analyse the air quality data from the city of Bristol. The analysis is conducted to identify the air pollution in the city and the different factors that may affect it. Thus, we gather the data from the different sensors deployed in the city and load them into the database. Then we analyse the data and create visualisations to facilitate the understanding of the data. Finally, we load the data and visualisations into the data catalogue to facilitate the access to them and foster the dissemination and adaption of the results to maximize impact.","title":"Objective"},{"location":"thematic_coexploration_example/#dataset","text":"For this example we employed a single dataset from the Air Quality Data Continuous dataset. This dataset contains the air quality data from the city of Bristol. The data is collected from the different sensors deployed in the city. The data is collected every hour and it is stored in the dataset. The dataset contains the following fields: Date_Time Site_ID NOx NO2 NO PM10 O3 Temperature ObjectId ObjectId2 NVPM10 VPM10 NVPM2_5 PM2_5 VPM2_5 CO RH Pressure SO2 Note that not all the fields are present in all the rows.","title":"Dataset"},{"location":"thematic_coexploration_example/#video","text":"Some of the steps below are linked to the time in the video where they are covered. You can find the video in the following link or by clicking on the image below:","title":"Video"},{"location":"thematic_coexploration_example/#steps-covered-in-this-thematic-co-exploration","text":"Thematic co-exploration specification : In this step, we should specify the thematic co-exploration that we want to conduct. By employing the Collaborative Environment tool, the thematic co-exploration owners may specify the phases, objectives and tasks that should be conducted in the thematic co-exploration. Furthermore, the tool allows to specify the different roles upon which they will participate in the thematic co-exploration and the users assigned to each role. Community building : Through Discourse and Wordpress tools, the thematic co-exploration owners may create a community around the thematic co-exploration. The community thus could discuss the different aspects of the thematic co-exploration and the different tasks that should be conducted. Furthermore, through it the community should share the results of the thematic co-exploration and discuss them. Data collection : In this step, we collect the data from the different sensors that deployed in the city. In this example, we use the Apache NiFi tool to collect the data from the open API because it cannot be accessed by Apache Druid directly. Thus, we created a NiFi flow that iteratively gathers the data from the API and stores in a local file. However, in another scenario, tools as MindEarth , MODE or GREENGAGEs IoT sensors could be used to collect the data. Data ingestion and storage : In this step, we ingest the data from the local file to the database. In this example, we use the Apache Druid datastore tool to ingest the data from the local file. Data visualisation and dashboard creation : In this step, we load the data from the database into Apache Superset to create visualisations that may support the decision-making process. Furthermore, we create a dashboard aggregating the different visualisations to facilitate the understanding of the data. This visualisations, alternatively, can also be created using the Data Quality Dashboard tool. Data catalogue : In this step, we add references to the data and the visualisations created in previous steps to the DataHub tool. DataHub allows to create a catalog of data that shows the schema, documentation, lineage, properties and common queries that can be done to the data. Furthermore, the visualisations can also be referenced from this catalog and the connections between them can be established. It is a good tool where a given thematic co-exploration results can be traced. Insights generation : In this step the participants of the thematic co-exploration analyse the data and the visualisations generated from it and extract insights from it. These insights may be shared with the community by using the tools covered in the Community and Co-production Process Management phase of the Citizen Observer Journey.","title":"Steps covered in this thematic co-exploration"},{"location":"tools/DataQualityDashboard/","text":"Data Quality Dashboard Introduction The data quality dashboard is a web-based tool that allows users to monitor the quality of collected data. It provides both visual and numeric representations of data quality through visual cues, quality metrics, and statistics, allowing users to quickly identify potential issues. The dashboard also offers a variety of features for data exploration and analysis, including the ability to filter data by time, location, and other parameters. Features of Data Quality Dashboard Interactive Dashboard : The dashboard reacts to user input, allowing them to interact with the data and the selection of graphs and charts. Data Quality Visualisation: The dashboard provides visual cues to highlight data quality issues, such as missing values, outliers, and inconsistencies. Data Preview: Users can preview current data in a tabular format, allowing them to quickly identify potential issues. Data Quality Metrics: It offers a variety of data quality metrics, including completeness, accuracy and consistency. Descriptive Statistics: The dashboard provides descriptive statistics window for better understanding of the data in question. Data Filtering: Users can filter data by time, location, and other parameters to perform in-depth analysis. Data Export: The dashboard allows users to export data in various formats, including CSV, JSON, and Excel. Use Case Scenario Context: In an urban citizen observatory, members of the community actively participate in monitoring and analysing environmental conditions. One of the critical concerns in many cities is air quality, which directly impacts public health and quality of life. Objective: To monitor, analyse, and visualise air quality data across different parts of the city to identify pollution hotspots, understand temporal trends, and engage the public in environmental awareness and decision-making processes. Implementation Using Data Quality Dashboard: Data Collection: Deploy HOPU sensors across the city to measure air quality indicators like PM2.5, PM10, NO2, CO2, and Ozone. Sensors transmit data to a centralised database, storing time-stamped readings and location data. Data Integration with Data Quality Dashboard: Connect the dashboard to the database where sensor data is stored. Exploratory Data Analysis: Use the dashboard to explore data quality issues, such as missing values, outliers, and inconsistencies. Identify potential causes of data quality issues, such as sensor malfunction or data transmission errors. Use the insights to improve data collection processes and ensure data quality.","title":"Data Quality Dashboard"},{"location":"tools/DataQualityDashboard/#data-quality-dashboard","text":"","title":"Data Quality Dashboard"},{"location":"tools/DataQualityDashboard/#introduction","text":"The data quality dashboard is a web-based tool that allows users to monitor the quality of collected data. It provides both visual and numeric representations of data quality through visual cues, quality metrics, and statistics, allowing users to quickly identify potential issues. The dashboard also offers a variety of features for data exploration and analysis, including the ability to filter data by time, location, and other parameters.","title":"Introduction"},{"location":"tools/DataQualityDashboard/#features-of-data-quality-dashboard","text":"Interactive Dashboard : The dashboard reacts to user input, allowing them to interact with the data and the selection of graphs and charts. Data Quality Visualisation: The dashboard provides visual cues to highlight data quality issues, such as missing values, outliers, and inconsistencies. Data Preview: Users can preview current data in a tabular format, allowing them to quickly identify potential issues. Data Quality Metrics: It offers a variety of data quality metrics, including completeness, accuracy and consistency. Descriptive Statistics: The dashboard provides descriptive statistics window for better understanding of the data in question. Data Filtering: Users can filter data by time, location, and other parameters to perform in-depth analysis. Data Export: The dashboard allows users to export data in various formats, including CSV, JSON, and Excel.","title":"Features of Data Quality Dashboard"},{"location":"tools/DataQualityDashboard/#use-case-scenario","text":"","title":"Use Case Scenario"},{"location":"tools/DataQualityDashboard/#context","text":"In an urban citizen observatory, members of the community actively participate in monitoring and analysing environmental conditions. One of the critical concerns in many cities is air quality, which directly impacts public health and quality of life.","title":"Context:"},{"location":"tools/DataQualityDashboard/#objective","text":"To monitor, analyse, and visualise air quality data across different parts of the city to identify pollution hotspots, understand temporal trends, and engage the public in environmental awareness and decision-making processes.","title":"Objective:"},{"location":"tools/DataQualityDashboard/#implementation-using-data-quality-dashboard","text":"Data Collection: Deploy HOPU sensors across the city to measure air quality indicators like PM2.5, PM10, NO2, CO2, and Ozone. Sensors transmit data to a centralised database, storing time-stamped readings and location data. Data Integration with Data Quality Dashboard: Connect the dashboard to the database where sensor data is stored. Exploratory Data Analysis: Use the dashboard to explore data quality issues, such as missing values, outliers, and inconsistencies. Identify potential causes of data quality issues, such as sensor malfunction or data transmission errors. Use the insights to improve data collection processes and ensure data quality.","title":"Implementation Using Data Quality Dashboard:"},{"location":"tools/Discourse/","text":"Discourse Introduction Description : Discourse is a free, open-source forum software that plays a crucial role in modern tech environments by enabling thoughtful discussion and meaningful connections. It integrates seamlessly with various platforms, enhancing both application functionality and community engagement. Objective : The integration of Discourse can significantly benefit development projects by offering robust collaboration and communication tools. It enhances application security with features like spam blocking and two-factor authentication, ensuring a safe and productive environment for users. Features of Discourse List of Features : Conversations, Not Pages : Utilizes just-in-time loading for seamless conversation flow. Real-Time Chat : Enables informal chats alongside more structured discussions. AI Integration : Enhances user experience and moderation with AI capabilities. Customizable User Experience : Offers a personalized interface with adjustable preferences. Mobile Compatibility : Fully functional on high-resolution touch devices. Link Expansion : Automatically enhances shared links with additional context. Single Sign-On : Facilitates easy integration with existing login systems. Trust System : Rewards regular members with additional abilities. Community Moderation : Allows community-driven content moderation. Spam Blocking : Integrates Akismet spam protection. Social Login : Supports common social media logins. Topic Summarization : Condenses long discussions to highlights. Badges : Encourages positive behaviors with a badge system. Emoji Support : Offers a searchable list of emojis. Email Interaction : Allows replying to notifications via email. Two-Factor Authentication : Enhances account security. Admin Dashboard : Provides essential community health metrics. Official Plugins : Supports various additional features. Comprehensive API : Facilitates broad functionality access. Open Source : Ensures transparency and adaptability. One-Click Upgrades : Simplifies version updates. Additional Features : Includes support for multiple languages, SEO optimization, various content formatting options, automatic backups, blog integration, and more","title":"Discourse"},{"location":"tools/Discourse/#discourse","text":"","title":"Discourse"},{"location":"tools/Discourse/#introduction","text":"Description : Discourse is a free, open-source forum software that plays a crucial role in modern tech environments by enabling thoughtful discussion and meaningful connections. It integrates seamlessly with various platforms, enhancing both application functionality and community engagement. Objective : The integration of Discourse can significantly benefit development projects by offering robust collaboration and communication tools. It enhances application security with features like spam blocking and two-factor authentication, ensuring a safe and productive environment for users.","title":"Introduction"},{"location":"tools/Discourse/#features-of-discourse","text":"List of Features : Conversations, Not Pages : Utilizes just-in-time loading for seamless conversation flow. Real-Time Chat : Enables informal chats alongside more structured discussions. AI Integration : Enhances user experience and moderation with AI capabilities. Customizable User Experience : Offers a personalized interface with adjustable preferences. Mobile Compatibility : Fully functional on high-resolution touch devices. Link Expansion : Automatically enhances shared links with additional context. Single Sign-On : Facilitates easy integration with existing login systems. Trust System : Rewards regular members with additional abilities. Community Moderation : Allows community-driven content moderation. Spam Blocking : Integrates Akismet spam protection. Social Login : Supports common social media logins. Topic Summarization : Condenses long discussions to highlights. Badges : Encourages positive behaviors with a badge system. Emoji Support : Offers a searchable list of emojis. Email Interaction : Allows replying to notifications via email. Two-Factor Authentication : Enhances account security. Admin Dashboard : Provides essential community health metrics. Official Plugins : Supports various additional features. Comprehensive API : Facilitates broad functionality access. Open Source : Ensures transparency and adaptability. One-Click Upgrades : Simplifies version updates. Additional Features : Includes support for multiple languages, SEO optimization, various content formatting options, automatic backups, blog integration, and more","title":"Features of Discourse"},{"location":"tools/Discourse/integration/","text":"Discourse Plugin Integration Guide with Keycloak SSO on Bitnami Discourse is a popular open-source discussion platform designed for modern community engagement. It is highly customizable, supporting numerous plugins for extended functionality. For detailed usage instructions and deployment options, refer to the Discourse User Manual . Integrating Discourse into the GREENGAGE engine amplifies its capabilities by enabling seamless Single Sign-On (SSO) authentication through Keycloak. This guide provides steps to integrate Discourse with Keycloak SSO, ensuring a smooth authentication process for users across various GREENGAGE services. Requesting Access Credentials To integrate Discourse with Keycloak SSO, you'll need to request access credentials. ( Follow this guide ) Receiving Credentials Following your request, you'll receive the necessary credentials via email. These credentials will enable the setup of SSO with Keycloak in your Discourse installation. Configuring Your Service Installing OpenID Connect Plugin for Keycloak Integration : Navigate to the Discourse installation directory: cd /opt/bitnami/discourse Install the OpenID Connect plugin: RAILS_ENV=production bundle exec rake plugin:install repo=https://github.com/discourse/discourse-openid-connect Precompile the assets for the plugin to take effect: RAILS_ENV=production bundle exec rake assets:precompile Detailed guide for installing plugins on Discourse over Bitnami can be found here . Disabling Email Verification : To disable the email verification typically sent upon first-time registration in Discourse, use the following plugin: cd /opt/bitnami/discourse/plugins git clone https://github.com/codergautam/disable-email-verification-discourse.git cd .. RAILS_ENV=production bundle install RAILS_ENV=production bundle exec rake assets:precompile These steps will set up Discourse for Keycloak SSO and disable the default email verification, aligning the authentication process with the Keycloak-managed user accounts. Step 4: Testing the Integration Test the SSO integration by attempting to log in to Discourse using credentials managed by Keycloak. For testing API integrations, verify if you can call the Discourse API using the token obtained from Keycloak. Support and Contact For additional support, please reach out to greengage.admin@deusto.es. Important Notes: Always keep your credentials confidential and secure. In case of any security concerns or compromised credentials, immediately contact the support team. Regularly update your integration setup to comply with the latest security standards and feature enhancements.","title":"Discourse Plugin Integration Guide with Keycloak SSO on Bitnami"},{"location":"tools/Discourse/integration/#discourse-plugin-integration-guide-with-keycloak-sso-on-bitnami","text":"Discourse is a popular open-source discussion platform designed for modern community engagement. It is highly customizable, supporting numerous plugins for extended functionality. For detailed usage instructions and deployment options, refer to the Discourse User Manual . Integrating Discourse into the GREENGAGE engine amplifies its capabilities by enabling seamless Single Sign-On (SSO) authentication through Keycloak. This guide provides steps to integrate Discourse with Keycloak SSO, ensuring a smooth authentication process for users across various GREENGAGE services. Requesting Access Credentials To integrate Discourse with Keycloak SSO, you'll need to request access credentials. ( Follow this guide ) Receiving Credentials Following your request, you'll receive the necessary credentials via email. These credentials will enable the setup of SSO with Keycloak in your Discourse installation. Configuring Your Service Installing OpenID Connect Plugin for Keycloak Integration : Navigate to the Discourse installation directory: cd /opt/bitnami/discourse Install the OpenID Connect plugin: RAILS_ENV=production bundle exec rake plugin:install repo=https://github.com/discourse/discourse-openid-connect Precompile the assets for the plugin to take effect: RAILS_ENV=production bundle exec rake assets:precompile Detailed guide for installing plugins on Discourse over Bitnami can be found here . Disabling Email Verification : To disable the email verification typically sent upon first-time registration in Discourse, use the following plugin: cd /opt/bitnami/discourse/plugins git clone https://github.com/codergautam/disable-email-verification-discourse.git cd .. RAILS_ENV=production bundle install RAILS_ENV=production bundle exec rake assets:precompile These steps will set up Discourse for Keycloak SSO and disable the default email verification, aligning the authentication process with the Keycloak-managed user accounts. Step 4: Testing the Integration Test the SSO integration by attempting to log in to Discourse using credentials managed by Keycloak. For testing API integrations, verify if you can call the Discourse API using the token obtained from Keycloak. Support and Contact For additional support, please reach out to greengage.admin@deusto.es. Important Notes: Always keep your credentials confidential and secure. In case of any security concerns or compromised credentials, immediately contact the support team. Regularly update your integration setup to comply with the latest security standards and feature enhancements.","title":"Discourse Plugin Integration Guide with Keycloak SSO on Bitnami"},{"location":"tools/Discourse/examples/","text":"Discourse Deployment Guide Integrating Discourse in Docker Environment Introduction Description : Discourse is a versatile forum software ideal for creating and managing online communities. Its significance in development lies in its ability to foster engagement and provide a platform for discussions. Objective : Deploying Discourse using Docker enhances application scalability, security, and ease of maintenance. Prerequisites Docker and Docker Compose installed. Basic knowledge of Docker and YAML syntax. Project Structure Source : Discourse Docker Compose Example Directory Layout : postgresql : PostgreSQL container configuration. redis : Redis container configuration. discourse : Main Discourse application container setup. sidekiq : Sidekiq container for background jobs. Step-by-Step Setup and Configuration Download or Clone the Docker Compose YAML File : Obtain the Docker Compose file for Discourse deployment. Replace Placeholders with Actual Values : [HERE_YOUR_DOMAIN] : Enter the domain where your Discourse instance will be accessible. [HERE_ADMIN_EMAIL] : Provide the email address for the Discourse administrator account. [HERE_ADMIN_USERNAME] : Set the username for the Discourse administrator. [HERE_ADMIN_PASSWORD] : Choose a secure password for the Discourse administrator account. [HERE_SMTP_HOST] : Specify the SMTP server host for sending emails. [HERE_SMTP_PORT] : Enter the port number used by your SMTP server. [HERE_SMTP_USER] : Provide the SMTP username for email authentication. [HERE_SMTP_PASSWORD] : Set the SMTP password for the email account. [HERE_SMTP_PROTOCOL] : Indicate the email protocol used by your SMTP server (e.g., TLS). Each of these placeholders corresponds to essential configuration settings for your Discourse deployment, ensuring proper functionality and access to the application. Running the Services Run docker-compose up to start all services. Accessing the Application Access Discourse at the specified domain after successful deployment. Additional Configuration (Optional) Integrating Keycloak for Single Sign-On (SSO) : To enable SSO with Keycloak in Discourse: cd /opt/bitnami/discourse RAILS_ENV=production bundle exec rake plugin:install repo=https://github.com/discourse/discourse-openid-connect RAILS_ENV=production bundle exec rake assets:precompile This configures Discourse to use Keycloak as the authentication provider, allowing seamless login across connected services. Disabling Email Verification : Given that Keycloak is used for user management, you might want to disable the default email verification sent by Discourse: cd /opt/bitnami/discourse/plugins git clone https://github.com/codergautam/disable-email-verification-discourse.git cd .. RAILS_ENV=production bundle install RAILS_ENV=production bundle exec rake assets:precompile This step ensures that the email verification step is bypassed, streamlining the registration process in a Keycloak-managed environment. Troubleshooting Issues with User and Site Configuration : If you encounter problems related to configuring users or site settings in your Docker-deployed Discourse instance, refer to the Bitnami Discourse Container Configuration Guide . This resource provides detailed instructions and examples for configuring various aspects of the Discourse application within a Docker environment. General Docker Deployment Issues : For common Docker-related problems, such as container networking, volume management, or image retrieval issues, consult Docker's official documentation or community forums for guidance and solutions. Support and Additional Resources Support Channels : Community forums, email support. External Documentation : Docker Documentation , Discourse Official Documentation . Docker Integration for Discourse Introduction This section focuses on integrating the Discourse forum software within a Docker containerized environment, leveraging Docker's capabilities for efficient deployment and management. Prerequisites Knowledge of Docker and Docker Compose. Familiarity with YAML syntax. Project Structure The Docker Compose file defines multiple services: postgresql for the database, redis for caching, discourse for the main application, and sidekiq for background jobs. The volumes and networks sections manage data persistence and inter-container communication. Steps for Integration Clone or create a Docker Compose file similar to the provided example. Customize the environment variables and service settings as per your requirements. Running the Example Execute docker-compose up in the directory containing your Docker Compose file to start the Discourse environment. Accessing the Application Once the containers are running, access Discourse through the specified domain configured in the DISCOURSE_HOST environment variable of the discourse service. For specific details, always refer to the latest official documentation of the tools involved.","title":"Discourse Deployment Guide"},{"location":"tools/Discourse/examples/#discourse-deployment-guide","text":"","title":"Discourse Deployment Guide"},{"location":"tools/Discourse/examples/#integrating-discourse-in-docker-environment","text":"","title":"Integrating Discourse in Docker Environment"},{"location":"tools/Discourse/examples/#introduction","text":"Description : Discourse is a versatile forum software ideal for creating and managing online communities. Its significance in development lies in its ability to foster engagement and provide a platform for discussions. Objective : Deploying Discourse using Docker enhances application scalability, security, and ease of maintenance.","title":"Introduction"},{"location":"tools/Discourse/examples/#prerequisites","text":"Docker and Docker Compose installed. Basic knowledge of Docker and YAML syntax.","title":"Prerequisites"},{"location":"tools/Discourse/examples/#project-structure","text":"Source : Discourse Docker Compose Example Directory Layout : postgresql : PostgreSQL container configuration. redis : Redis container configuration. discourse : Main Discourse application container setup. sidekiq : Sidekiq container for background jobs.","title":"Project Structure"},{"location":"tools/Discourse/examples/#step-by-step-setup-and-configuration","text":"Download or Clone the Docker Compose YAML File : Obtain the Docker Compose file for Discourse deployment. Replace Placeholders with Actual Values : [HERE_YOUR_DOMAIN] : Enter the domain where your Discourse instance will be accessible. [HERE_ADMIN_EMAIL] : Provide the email address for the Discourse administrator account. [HERE_ADMIN_USERNAME] : Set the username for the Discourse administrator. [HERE_ADMIN_PASSWORD] : Choose a secure password for the Discourse administrator account. [HERE_SMTP_HOST] : Specify the SMTP server host for sending emails. [HERE_SMTP_PORT] : Enter the port number used by your SMTP server. [HERE_SMTP_USER] : Provide the SMTP username for email authentication. [HERE_SMTP_PASSWORD] : Set the SMTP password for the email account. [HERE_SMTP_PROTOCOL] : Indicate the email protocol used by your SMTP server (e.g., TLS). Each of these placeholders corresponds to essential configuration settings for your Discourse deployment, ensuring proper functionality and access to the application.","title":"Step-by-Step Setup and Configuration"},{"location":"tools/Discourse/examples/#running-the-services","text":"Run docker-compose up to start all services.","title":"Running the Services"},{"location":"tools/Discourse/examples/#accessing-the-application","text":"Access Discourse at the specified domain after successful deployment.","title":"Accessing the Application"},{"location":"tools/Discourse/examples/#additional-configuration-optional","text":"Integrating Keycloak for Single Sign-On (SSO) : To enable SSO with Keycloak in Discourse: cd /opt/bitnami/discourse RAILS_ENV=production bundle exec rake plugin:install repo=https://github.com/discourse/discourse-openid-connect RAILS_ENV=production bundle exec rake assets:precompile This configures Discourse to use Keycloak as the authentication provider, allowing seamless login across connected services. Disabling Email Verification : Given that Keycloak is used for user management, you might want to disable the default email verification sent by Discourse: cd /opt/bitnami/discourse/plugins git clone https://github.com/codergautam/disable-email-verification-discourse.git cd .. RAILS_ENV=production bundle install RAILS_ENV=production bundle exec rake assets:precompile This step ensures that the email verification step is bypassed, streamlining the registration process in a Keycloak-managed environment.","title":"Additional Configuration (Optional)"},{"location":"tools/Discourse/examples/#troubleshooting","text":"Issues with User and Site Configuration : If you encounter problems related to configuring users or site settings in your Docker-deployed Discourse instance, refer to the Bitnami Discourse Container Configuration Guide . This resource provides detailed instructions and examples for configuring various aspects of the Discourse application within a Docker environment. General Docker Deployment Issues : For common Docker-related problems, such as container networking, volume management, or image retrieval issues, consult Docker's official documentation or community forums for guidance and solutions.","title":"Troubleshooting"},{"location":"tools/Discourse/examples/#support-and-additional-resources","text":"Support Channels : Community forums, email support. External Documentation : Docker Documentation , Discourse Official Documentation .","title":"Support and Additional Resources"},{"location":"tools/Discourse/examples/#docker-integration-for-discourse","text":"","title":"Docker Integration for Discourse"},{"location":"tools/Discourse/examples/#introduction_1","text":"This section focuses on integrating the Discourse forum software within a Docker containerized environment, leveraging Docker's capabilities for efficient deployment and management.","title":"Introduction"},{"location":"tools/Discourse/examples/#prerequisites_1","text":"Knowledge of Docker and Docker Compose. Familiarity with YAML syntax.","title":"Prerequisites"},{"location":"tools/Discourse/examples/#project-structure_1","text":"The Docker Compose file defines multiple services: postgresql for the database, redis for caching, discourse for the main application, and sidekiq for background jobs. The volumes and networks sections manage data persistence and inter-container communication.","title":"Project Structure"},{"location":"tools/Discourse/examples/#steps-for-integration","text":"Clone or create a Docker Compose file similar to the provided example. Customize the environment variables and service settings as per your requirements.","title":"Steps for Integration"},{"location":"tools/Discourse/examples/#running-the-example","text":"Execute docker-compose up in the directory containing your Docker Compose file to start the Discourse environment.","title":"Running the Example"},{"location":"tools/Discourse/examples/#accessing-the-application_1","text":"Once the containers are running, access Discourse through the specified domain configured in the DISCOURSE_HOST environment variable of the discourse service. For specific details, always refer to the latest official documentation of the tools involved.","title":"Accessing the Application"},{"location":"tools/TOOLNAME_template/","text":"Tool Name Introduction Description: Provide an overview of the tool's purpose and its importance in modern tech environments. Objective: Explain how integrating this tool can benefit development and enhance application security or functionality. Features of [Tool Name] List of Features: Outline key features and capabilities of the tool, emphasizing how each contributes to its effectiveness and utility in development projects. Use Case Scenario Description: Detail a typical use case scenario, demonstrating how the tool operates within a workflow or system. (if possible) Visual Aid: Include a diagram or flowchart to illustrate the use case, making it easier to understand the tool's application in a real-world scenario. References Description: list here a set of external references where User Manual of further information to tool is provide. It is suggested to include: a) User manual link and b) Link to PDF of description of tool in GREENGAGE catalogue User manual GREENGAGE catalogue entry","title":"Tool Name"},{"location":"tools/TOOLNAME_template/#tool-name","text":"","title":"Tool Name"},{"location":"tools/TOOLNAME_template/#introduction","text":"Description: Provide an overview of the tool's purpose and its importance in modern tech environments. Objective: Explain how integrating this tool can benefit development and enhance application security or functionality.","title":"Introduction"},{"location":"tools/TOOLNAME_template/#features-of-tool-name","text":"List of Features: Outline key features and capabilities of the tool, emphasizing how each contributes to its effectiveness and utility in development projects.","title":"Features of [Tool Name]"},{"location":"tools/TOOLNAME_template/#use-case-scenario","text":"Description: Detail a typical use case scenario, demonstrating how the tool operates within a workflow or system. (if possible) Visual Aid: Include a diagram or flowchart to illustrate the use case, making it easier to understand the tool's application in a real-world scenario.","title":"Use Case Scenario"},{"location":"tools/TOOLNAME_template/#references","text":"Description: list here a set of external references where User Manual of further information to tool is provide. It is suggested to include: a) User manual link and b) Link to PDF of description of tool in GREENGAGE catalogue User manual GREENGAGE catalogue entry","title":"References"},{"location":"tools/TOOLNAME_template/integration/","text":"[Tool Name] Usage & Integration Guide for Third-Party Services Generic usage instructions for [Tool Name] - Instructions on how to use the tool. - In case there is external user manual make a small summary and refer to it. - Otherwise, provide enough explanations and snapshots to let a user get started with the tool usage. Include links to URLs where the tool might be deployed. Introduction explaining the importance of integrating [Tool Name] to make your tool part of the GREENGAGE engine. This implies that your tool has to use the single sign on (SSO) service provided by GREENGAGE. However, you should also describe not only how your tool is integrated with GREENGAGE's SSO but also how other GREENGAGE tools may integrate with your tools. Feel free to add and rename the integrate steps listed below: Step 1: Requesting Access Credentials Instructions on how to request access credentials. Contact details (e.g., email address) for credential requests. Information to be provided in the request (e.g., tool name, description, contact details). Step 2: Receiving Credentials Process description after the request is made. Information on how credentials will be received. Step 3: Configuring Your Service Steps to incorporate the received credentials into the service. Guidelines on setting up the service for proper communication with the [Tool Name]. Step 4: Testing the Integration Instructions for testing the integration to ensure everything is set up correctly. Contact information for support in case of issues during testing. Support and Contact Additional support contact details. Encouragement to reach out with questions or for further assistance. Important Notes: Security reminders and best practices (e.g., keeping credentials confidential). Instructions on what to do if credentials are compromised.","title":"[Tool Name] Usage &amp; Integration Guide for Third-Party Services"},{"location":"tools/TOOLNAME_template/integration/#tool-name-usage-integration-guide-for-third-party-services","text":"Generic usage instructions for [Tool Name] - Instructions on how to use the tool. - In case there is external user manual make a small summary and refer to it. - Otherwise, provide enough explanations and snapshots to let a user get started with the tool usage. Include links to URLs where the tool might be deployed. Introduction explaining the importance of integrating [Tool Name] to make your tool part of the GREENGAGE engine. This implies that your tool has to use the single sign on (SSO) service provided by GREENGAGE. However, you should also describe not only how your tool is integrated with GREENGAGE's SSO but also how other GREENGAGE tools may integrate with your tools. Feel free to add and rename the integrate steps listed below: Step 1: Requesting Access Credentials Instructions on how to request access credentials. Contact details (e.g., email address) for credential requests. Information to be provided in the request (e.g., tool name, description, contact details). Step 2: Receiving Credentials Process description after the request is made. Information on how credentials will be received. Step 3: Configuring Your Service Steps to incorporate the received credentials into the service. Guidelines on setting up the service for proper communication with the [Tool Name]. Step 4: Testing the Integration Instructions for testing the integration to ensure everything is set up correctly. Contact information for support in case of issues during testing. Support and Contact Additional support contact details. Encouragement to reach out with questions or for further assistance. Important Notes: Security reminders and best practices (e.g., keeping credentials confidential). Instructions on what to do if credentials are compromised.","title":"[Tool Name] Usage &amp; Integration Guide for Third-Party Services"},{"location":"tools/TOOLNAME_template/examples/","text":"[Tool Name] Integrating [Tool Name] in [Environment] Introduction Description: Provide an overview of the tool's purpose and significance in development. Objective: Explain how this tool enhances the application's functionality or security. Prerequisites List the prerequisites needed to use the tool effectively. Project Structure Source: Link to the example project. Directory Layout: Describe the structure of the example project. Step-by-Step Setup and Configuration Outline the steps to set up the tool in the chosen environment. Detailed Configuration Steps Provide detailed instructions for each configuration file and setting. Running the Services Instructions for starting the services on different operating systems. Accessing the Application Guide on how to access the application once it is running. Additional Configuration (Optional) Discuss any optional configuration steps for advanced users. Troubleshooting Provide common troubleshooting steps or issues and their solutions. Support and Additional Resources Support Channels: List available support channels. External Documentation: Link to the official documentation or other helpful resources. [Language/Framework Integration] (e.g., Python, Node.js) Introduction Brief introduction to integrating the tool with the specific language or framework. Prerequisites List any prerequisites specific to this language/framework. Project Structure Describe the structure of the example integration project. Steps for Integration Step-by-step guide for integrating the tool into projects using this language/framework. Running the Example Instructions on how to run the example project. Accessing the Application How to access and test the application once it's running. Replace sections marked with [brackets] with the relevant information for your specific tool and environment.","title":"[Tool Name]"},{"location":"tools/TOOLNAME_template/examples/#tool-name","text":"","title":"[Tool Name]"},{"location":"tools/TOOLNAME_template/examples/#integrating-tool-name-in-environment","text":"","title":"Integrating [Tool Name] in [Environment]"},{"location":"tools/TOOLNAME_template/examples/#introduction","text":"Description: Provide an overview of the tool's purpose and significance in development. Objective: Explain how this tool enhances the application's functionality or security.","title":"Introduction"},{"location":"tools/TOOLNAME_template/examples/#prerequisites","text":"List the prerequisites needed to use the tool effectively.","title":"Prerequisites"},{"location":"tools/TOOLNAME_template/examples/#project-structure","text":"Source: Link to the example project. Directory Layout: Describe the structure of the example project.","title":"Project Structure"},{"location":"tools/TOOLNAME_template/examples/#step-by-step-setup-and-configuration","text":"Outline the steps to set up the tool in the chosen environment.","title":"Step-by-Step Setup and Configuration"},{"location":"tools/TOOLNAME_template/examples/#detailed-configuration-steps","text":"Provide detailed instructions for each configuration file and setting.","title":"Detailed Configuration Steps"},{"location":"tools/TOOLNAME_template/examples/#running-the-services","text":"Instructions for starting the services on different operating systems.","title":"Running the Services"},{"location":"tools/TOOLNAME_template/examples/#accessing-the-application","text":"Guide on how to access the application once it is running.","title":"Accessing the Application"},{"location":"tools/TOOLNAME_template/examples/#additional-configuration-optional","text":"Discuss any optional configuration steps for advanced users.","title":"Additional Configuration (Optional)"},{"location":"tools/TOOLNAME_template/examples/#troubleshooting","text":"Provide common troubleshooting steps or issues and their solutions.","title":"Troubleshooting"},{"location":"tools/TOOLNAME_template/examples/#support-and-additional-resources","text":"Support Channels: List available support channels. External Documentation: Link to the official documentation or other helpful resources.","title":"Support and Additional Resources"},{"location":"tools/TOOLNAME_template/examples/#languageframework-integration-eg-python-nodejs","text":"","title":"[Language/Framework Integration] (e.g., Python, Node.js)"},{"location":"tools/TOOLNAME_template/examples/#introduction_1","text":"Brief introduction to integrating the tool with the specific language or framework.","title":"Introduction"},{"location":"tools/TOOLNAME_template/examples/#prerequisites_1","text":"List any prerequisites specific to this language/framework.","title":"Prerequisites"},{"location":"tools/TOOLNAME_template/examples/#project-structure_1","text":"Describe the structure of the example integration project.","title":"Project Structure"},{"location":"tools/TOOLNAME_template/examples/#steps-for-integration","text":"Step-by-step guide for integrating the tool into projects using this language/framework.","title":"Steps for Integration"},{"location":"tools/TOOLNAME_template/examples/#running-the-example","text":"Instructions on how to run the example project.","title":"Running the Example"},{"location":"tools/TOOLNAME_template/examples/#accessing-the-application_1","text":"How to access and test the application once it's running. Replace sections marked with [brackets] with the relevant information for your specific tool and environment.","title":"Accessing the Application"},{"location":"tools/collaborativeEnvironment/","text":"Collaborative Environment Introduction Description The Collaborative Environment, developed as part of the INTERLINK project, serves as a foundational tool for Citizen Observatories. It leverages cutting-edge frontend technologies to enable effective collaboration and intuitive user interaction. Its modular design, based on functional React components, ensures easy expansion and maintenance, adapting to the evolving needs of Citizen Observatories. Objective Integrating the Collaborative Environment into development projects can significantly enhance team communication, document collaboration, and stakeholder engagement. Its capabilities in process planning and promoting loyalty through incentives and rewards make it an invaluable asset for co-production in diverse fields. Features of Collaborative Environment List of Features: Internationalization Support : Utilizes \"react-i18next\" for advanced translation features, catering to a global user base. Flexible Development : Offers well-documented examples for easy addition of new elements and routes, supporting agile development practices. State Management : Employs Redux for effective global state management and React's Context API for smooth data flow across components. Inclusivity and Diversity : Focus on internationalization promotes access and usability across diverse user groups. Modular Structure : Facilitates easy expansion and maintenance, adapting to changing requirements. Efficient Data Handling : Uses a microservices architecture with RESTful interfaces, ensuring scalability and performance. Security Features : Includes authentication through OpenID Connect (OIDC) and secure data transfer protocols. Gamification Elements : Incorporates a gamification engine to incentivize user participation and engagement. Use Case Scenario Description A typical use case for the Collaborative Environment involves setting up and managing a co-production process within a Citizen Observatory. Users can create organizations, form teams, and develop processes, each with clearly defined phases, objectives, and tasks. The platform allows for resource management, tracking progress, and engaging communities effectively. It also supports integration with external tools, enhancing its utility in diverse scenarios. References User manual of Collaborative Environment Documentation in GREENGAGE catalogue","title":"Collaborative Environment"},{"location":"tools/collaborativeEnvironment/#collaborative-environment","text":"","title":"Collaborative Environment"},{"location":"tools/collaborativeEnvironment/#introduction","text":"","title":"Introduction"},{"location":"tools/collaborativeEnvironment/#description","text":"The Collaborative Environment, developed as part of the INTERLINK project, serves as a foundational tool for Citizen Observatories. It leverages cutting-edge frontend technologies to enable effective collaboration and intuitive user interaction. Its modular design, based on functional React components, ensures easy expansion and maintenance, adapting to the evolving needs of Citizen Observatories.","title":"Description"},{"location":"tools/collaborativeEnvironment/#objective","text":"Integrating the Collaborative Environment into development projects can significantly enhance team communication, document collaboration, and stakeholder engagement. Its capabilities in process planning and promoting loyalty through incentives and rewards make it an invaluable asset for co-production in diverse fields.","title":"Objective"},{"location":"tools/collaborativeEnvironment/#features-of-collaborative-environment","text":"","title":"Features of Collaborative Environment"},{"location":"tools/collaborativeEnvironment/#list-of-features","text":"Internationalization Support : Utilizes \"react-i18next\" for advanced translation features, catering to a global user base. Flexible Development : Offers well-documented examples for easy addition of new elements and routes, supporting agile development practices. State Management : Employs Redux for effective global state management and React's Context API for smooth data flow across components. Inclusivity and Diversity : Focus on internationalization promotes access and usability across diverse user groups. Modular Structure : Facilitates easy expansion and maintenance, adapting to changing requirements. Efficient Data Handling : Uses a microservices architecture with RESTful interfaces, ensuring scalability and performance. Security Features : Includes authentication through OpenID Connect (OIDC) and secure data transfer protocols. Gamification Elements : Incorporates a gamification engine to incentivize user participation and engagement.","title":"List of Features:"},{"location":"tools/collaborativeEnvironment/#use-case-scenario","text":"","title":"Use Case Scenario"},{"location":"tools/collaborativeEnvironment/#description_1","text":"A typical use case for the Collaborative Environment involves setting up and managing a co-production process within a Citizen Observatory. Users can create organizations, form teams, and develop processes, each with clearly defined phases, objectives, and tasks. The platform allows for resource management, tracking progress, and engaging communities effectively. It also supports integration with external tools, enhancing its utility in diverse scenarios.","title":"Description"},{"location":"tools/collaborativeEnvironment/#references","text":"User manual of Collaborative Environment Documentation in GREENGAGE catalogue","title":"References"},{"location":"tools/collaborativeEnvironment/usage/","text":"Collaborative Environment Usage Guide for Third-Party Services This guide explains the process for integrating third-party services into the Collaborative Environment, a platform centralizing organizational management and enhancing collaborative workflows. Initial Login and registration Create an account either by registering with an email or using a Google account at Collaborative Environment > Login. Organization and Team Setup After logging in, navigate to Organizations tab > Create new organization. Fill in the necessary details to create your organization. To create a team, ensure you have an existing organization. In the Organizations tab, select your organization and click on Create new team. Define your team, stakeholders, and invite members. Process Creation and Configuration From the dashboard, click on Go to process list then on Create new process and fill in the required fields. Configure your process by selecting it from the process list and accessing the Overview section. This includes setting up various sections like Front Page, Overview, Resources, Guide, Leaderboard, Workplan, Team, and Settings. Define specific elements like Type, Schema, Organizations, Resources, Permissions, Rewards, and Finish in the roadmap for the process. Development and Completion of the Co-Production Process Begin the co-production process by selecting tasks from the Guide section. Manage phases, objectives, and tasks, including descriptions, statuses, planning times, and permissions. Once the process is complete, change the \"State of the project\" to \"Finished\" in Settings. Support and Contact For additional support or assistance with the integration process, please refer to the Collaborative Environment\u2019s online resources, or contact via https://github.com/Greengage-project/interlink-project/issues . (Technical Issues) Important Notes: Adhere to security best practices throughout the integration process. Regularly update and maintain your service for compatibility with the Collaborative Environment. For further assistance or troubleshooting, refer to the Collaborative Environment\u2019s resources or community forums. By following these steps, third-party services can be effectively integrated into the Collaborative Environment, leveraging its capabilities to improve organizational efficiency and collaboration. References Further details about the usage of this tool are available in the following references: - Collaborative Environment's Usage Manual","title":"Collaborative Environment Usage Guide for Third-Party Services"},{"location":"tools/collaborativeEnvironment/usage/#collaborative-environment-usage-guide-for-third-party-services","text":"This guide explains the process for integrating third-party services into the Collaborative Environment, a platform centralizing organizational management and enhancing collaborative workflows. Initial Login and registration Create an account either by registering with an email or using a Google account at Collaborative Environment > Login. Organization and Team Setup After logging in, navigate to Organizations tab > Create new organization. Fill in the necessary details to create your organization. To create a team, ensure you have an existing organization. In the Organizations tab, select your organization and click on Create new team. Define your team, stakeholders, and invite members. Process Creation and Configuration From the dashboard, click on Go to process list then on Create new process and fill in the required fields. Configure your process by selecting it from the process list and accessing the Overview section. This includes setting up various sections like Front Page, Overview, Resources, Guide, Leaderboard, Workplan, Team, and Settings. Define specific elements like Type, Schema, Organizations, Resources, Permissions, Rewards, and Finish in the roadmap for the process. Development and Completion of the Co-Production Process Begin the co-production process by selecting tasks from the Guide section. Manage phases, objectives, and tasks, including descriptions, statuses, planning times, and permissions. Once the process is complete, change the \"State of the project\" to \"Finished\" in Settings. Support and Contact For additional support or assistance with the integration process, please refer to the Collaborative Environment\u2019s online resources, or contact via https://github.com/Greengage-project/interlink-project/issues . (Technical Issues) Important Notes: Adhere to security best practices throughout the integration process. Regularly update and maintain your service for compatibility with the Collaborative Environment. For further assistance or troubleshooting, refer to the Collaborative Environment\u2019s resources or community forums. By following these steps, third-party services can be effectively integrated into the Collaborative Environment, leveraging its capabilities to improve organizational efficiency and collaboration.","title":"Collaborative Environment Usage Guide for Third-Party Services"},{"location":"tools/collaborativeEnvironment/usage/#references","text":"Further details about the usage of this tool are available in the following references: - Collaborative Environment's Usage Manual","title":"References"},{"location":"tools/collaborativeEnvironment/examples/","text":"Collaborative Environment Installation Guide Deploying Collaborative Environment This guide provides detailed instructions for setting up the Collaborative Environment using Docker and Docker Compose. Prerequisites Docker and Docker Compose installed. Node.js and npm installed for Node.js integration. Python and pip installed for Python integration. Basic understanding of the Collaborative Environment, Docker, and the chosen programming language (Node.js or Python). Project Structure The project structure includes various components like Docker configurations, backend services, interlinkers, and frontend applications. The main components are located in different repositories, which can be cloned for development purposes. Collaborative Environment/ \u251c\u2500\u2500 docker/ # Docker configurations for various services \u251c\u2500\u2500 docs/ # Documentation files \u251c\u2500\u2500 envs/ # Environment configurations \u251c\u2500\u2500 Makefile # Makefile for automating setup and deployment ... Step 1: Cloning Repositories and Setting Up the Environment Use the Makefile to clone all necessary components: make setup This will clone repositories such as: Frontend application Backend services (auth, catalogue, coproduction, logging) Interlinkers (survey, googledrive, ceditor) Gamification service Step 2: Building and Starting Services Build the containers for all services: make build Start the services, set up the environment, and seed the database: make up Step 3: Accessing the Application Once all services are up and running, access the Collaborative Environment's frontend through the configured local or development URL. Additional Commands Stopping Services : To stop all containers and remove volumes, use make down . Pulling Latest Changes : To update all components, use make pull . Restarting Services : To restart containers, apply migrations and seed data, use make restart . Environment Variables Setup : To configure environment variables, use make envVariables . Troubleshooting Common issues can arise during the setup, such as Docker network conflicts or missing dependencies. Refer to the README.md files in each repository for specific troubleshooting steps. Support and Additional Resources For further assistance, refer to the detailed documentation in the docs/ directory or visit the official Collaborative Environment Documentation . Additional support can be obtained through community forums or by contacting the Collaborative Environment support team.","title":"Collaborative Environment Installation Guide"},{"location":"tools/collaborativeEnvironment/examples/#collaborative-environment-installation-guide","text":"","title":"Collaborative Environment Installation Guide"},{"location":"tools/collaborativeEnvironment/examples/#deploying-collaborative-environment","text":"This guide provides detailed instructions for setting up the Collaborative Environment using Docker and Docker Compose.","title":"Deploying Collaborative Environment"},{"location":"tools/collaborativeEnvironment/examples/#prerequisites","text":"Docker and Docker Compose installed. Node.js and npm installed for Node.js integration. Python and pip installed for Python integration. Basic understanding of the Collaborative Environment, Docker, and the chosen programming language (Node.js or Python).","title":"Prerequisites"},{"location":"tools/collaborativeEnvironment/examples/#project-structure","text":"The project structure includes various components like Docker configurations, backend services, interlinkers, and frontend applications. The main components are located in different repositories, which can be cloned for development purposes. Collaborative Environment/ \u251c\u2500\u2500 docker/ # Docker configurations for various services \u251c\u2500\u2500 docs/ # Documentation files \u251c\u2500\u2500 envs/ # Environment configurations \u251c\u2500\u2500 Makefile # Makefile for automating setup and deployment ...","title":"Project Structure"},{"location":"tools/collaborativeEnvironment/examples/#step-1-cloning-repositories-and-setting-up-the-environment","text":"Use the Makefile to clone all necessary components: make setup This will clone repositories such as: Frontend application Backend services (auth, catalogue, coproduction, logging) Interlinkers (survey, googledrive, ceditor) Gamification service","title":"Step 1: Cloning Repositories and Setting Up the Environment"},{"location":"tools/collaborativeEnvironment/examples/#step-2-building-and-starting-services","text":"Build the containers for all services: make build Start the services, set up the environment, and seed the database: make up","title":"Step 2: Building and Starting Services"},{"location":"tools/collaborativeEnvironment/examples/#step-3-accessing-the-application","text":"Once all services are up and running, access the Collaborative Environment's frontend through the configured local or development URL.","title":"Step 3: Accessing the Application"},{"location":"tools/collaborativeEnvironment/examples/#additional-commands","text":"Stopping Services : To stop all containers and remove volumes, use make down . Pulling Latest Changes : To update all components, use make pull . Restarting Services : To restart containers, apply migrations and seed data, use make restart . Environment Variables Setup : To configure environment variables, use make envVariables .","title":"Additional Commands"},{"location":"tools/collaborativeEnvironment/examples/#troubleshooting","text":"Common issues can arise during the setup, such as Docker network conflicts or missing dependencies. Refer to the README.md files in each repository for specific troubleshooting steps.","title":"Troubleshooting"},{"location":"tools/collaborativeEnvironment/examples/#support-and-additional-resources","text":"For further assistance, refer to the detailed documentation in the docs/ directory or visit the official Collaborative Environment Documentation . Additional support can be obtained through community forums or by contacting the Collaborative Environment support team.","title":"Support and Additional Resources"},{"location":"tools/datahub/","text":"DataHub DataHub Description Introduction Description DataHub is an open-source metadata platform designed to democratise data discovery. It acts as a centralised system for companies and teams to track, manage, and find data within their organisation. Its primary aim is to streamline the process of accessing and understanding data assets, making it easier for teams to derive insights and make informed decisions. Objective The primary objective of DataHub is to facilitate better data discovery and management. By offering a unified view of an organisation's data assets, it aims to enhance collaboration, data governance, and compliance, while reducing the time and effort spent in locating and understanding data resources. Features of Datahub Metadata Management: DataHub catalogs metadata from various data sources, offering insights into data quality, usage, and lineage. Search and Discovery: It provides powerful search capabilities, allowing users to quickly find datasets and understand their context. Data Lineage: Visual representation of data lineage helps in understanding how data is transformed and where it flows within the organisation. Data Observability: Monitors data health, alerting users to issues related to data quality and freshness. Access Control: Supports granular access control to manage who can view or edit different data assets. Extensibility: Offers APIs and an extensible architecture, allowing for customisation and integration with other tools and systems. Use Case Scenario DataHub is employed to bridge the gap between citisens, scientists, and open data portals. Pilots have a wealth of open data available \u2013 traffic patterns, pollution levels, public space usage, and more. With DataHub, the city creates a centralised, user-friendly platform where all this data is cataloged and made easily searchable. Citisens and local scientists can access this data to understand various aspects of the city's dynamics. For instance, a group of citisens concerned about air quality uses DataHub to find and analyse pollution data. Furthermore, DataHub can be used to track the lineage of data, allowing users to understand how data is transformed and where it flows within the organisation. This helps in ensuring data quality and compliance with regulations such as GDPR. References Documentation GREENGAGE catalogue entry","title":"DataHub"},{"location":"tools/datahub/#datahub","text":"","title":"DataHub"},{"location":"tools/datahub/#datahub-description","text":"","title":"DataHub Description"},{"location":"tools/datahub/#introduction","text":"","title":"Introduction"},{"location":"tools/datahub/#description","text":"DataHub is an open-source metadata platform designed to democratise data discovery. It acts as a centralised system for companies and teams to track, manage, and find data within their organisation. Its primary aim is to streamline the process of accessing and understanding data assets, making it easier for teams to derive insights and make informed decisions.","title":"Description"},{"location":"tools/datahub/#objective","text":"The primary objective of DataHub is to facilitate better data discovery and management. By offering a unified view of an organisation's data assets, it aims to enhance collaboration, data governance, and compliance, while reducing the time and effort spent in locating and understanding data resources.","title":"Objective"},{"location":"tools/datahub/#features-of-datahub","text":"Metadata Management: DataHub catalogs metadata from various data sources, offering insights into data quality, usage, and lineage. Search and Discovery: It provides powerful search capabilities, allowing users to quickly find datasets and understand their context. Data Lineage: Visual representation of data lineage helps in understanding how data is transformed and where it flows within the organisation. Data Observability: Monitors data health, alerting users to issues related to data quality and freshness. Access Control: Supports granular access control to manage who can view or edit different data assets. Extensibility: Offers APIs and an extensible architecture, allowing for customisation and integration with other tools and systems.","title":"Features of Datahub"},{"location":"tools/datahub/#use-case-scenario","text":"DataHub is employed to bridge the gap between citisens, scientists, and open data portals. Pilots have a wealth of open data available \u2013 traffic patterns, pollution levels, public space usage, and more. With DataHub, the city creates a centralised, user-friendly platform where all this data is cataloged and made easily searchable. Citisens and local scientists can access this data to understand various aspects of the city's dynamics. For instance, a group of citisens concerned about air quality uses DataHub to find and analyse pollution data. Furthermore, DataHub can be used to track the lineage of data, allowing users to understand how data is transformed and where it flows within the organisation. This helps in ensuring data quality and compliance with regulations such as GDPR.","title":"Use Case Scenario"},{"location":"tools/datahub/#references","text":"Documentation GREENGAGE catalogue entry","title":"References"},{"location":"tools/datahub/usage/","text":"DataHub Usage Guide To provide a centralised location for all data assets, we are deploying DataHub in a centralised manner. This means that all data assets will be catalogued in a single instance of DataHub. This usage guide provide instructions on how to use DataHub. Step 1: Requesting Credentials Access to DataHub using your Google account or the account created for Greengage project and registered in Keycloak. Send an email to ruben.sanchez@deusto.es with the subject line: \"[GREENGAGE] Request for rights in DataHub\" including the following information: Tool name. Contact name:(for technical propose) Contact email:(for technical propose) Once you have the credentials and the right access rights you can start using DataHub. Data Discovery and Search You can search for any asset located in DataHub using the search bar located on the main page or at the top of any other page within DataHub. The search bar, as shown in the figure below, enables users to quickly find relevant datasets, metrics, and dashboards. Metadata Management DataHub offers comprehensive metadata management capabilities. It aggregates and manages metadata from a variety of data sources, maintaining details about datasets, schemas, data lineage, and ownership. This feature is illustrated in the Metadata Management interface, providing a clear overview of all metadata assets. Data Lineage and Governance DataHub visualises data lineage, showing how data flows through various systems. This feature is crucial for understanding the transformation and lifecycle of data, aiding in governance and compliance. The Data Lineage interface provides a graphical representation of these data flows. Collaboration and Documentation The platform enhances collaboration among data teams by allowing for the documentation and sharing of insights about datasets. The Collaboration and Documentation interface facilitates this exchange, fostering a data-informed culture.","title":"DataHub Usage Guide"},{"location":"tools/datahub/usage/#datahub-usage-guide","text":"To provide a centralised location for all data assets, we are deploying DataHub in a centralised manner. This means that all data assets will be catalogued in a single instance of DataHub. This usage guide provide instructions on how to use DataHub. Step 1: Requesting Credentials Access to DataHub using your Google account or the account created for Greengage project and registered in Keycloak. Send an email to ruben.sanchez@deusto.es with the subject line: \"[GREENGAGE] Request for rights in DataHub\" including the following information: Tool name. Contact name:(for technical propose) Contact email:(for technical propose) Once you have the credentials and the right access rights you can start using DataHub. Data Discovery and Search You can search for any asset located in DataHub using the search bar located on the main page or at the top of any other page within DataHub. The search bar, as shown in the figure below, enables users to quickly find relevant datasets, metrics, and dashboards. Metadata Management DataHub offers comprehensive metadata management capabilities. It aggregates and manages metadata from a variety of data sources, maintaining details about datasets, schemas, data lineage, and ownership. This feature is illustrated in the Metadata Management interface, providing a clear overview of all metadata assets. Data Lineage and Governance DataHub visualises data lineage, showing how data flows through various systems. This feature is crucial for understanding the transformation and lifecycle of data, aiding in governance and compliance. The Data Lineage interface provides a graphical representation of these data flows. Collaboration and Documentation The platform enhances collaboration among data teams by allowing for the documentation and sharing of insights about datasets. The Collaboration and Documentation interface facilitates this exchange, fostering a data-informed culture.","title":"DataHub Usage Guide"},{"location":"tools/datahub/examples/","text":"DataHub example of Integration Creating an Ingestion Source Introduction In this example we will show how to create an ingestion source that pulls data from the desired platform to DataHub. Prerequisites Access to DataHub Access to the platform to be integrated with DataHub Check if the platform to be integrated with DataHub is already supported by Datahub. List of ingestion sources Creating the recipe 1) Access to DataHub 2) Go to the Ingestion view and click in create new source. 3) Select the platform to be integrated with DataHub. 4) Fill the form with the information of the platform to be integrated with DataHub. Some of the platforms have forms to be fulfilled with the information of the platform to be integrated with DataHub. In other cases, the information is provided in a yaml style as shown in the following figure: 5) Schedule the ingestion of data as shown in the following figure: 6) Click in Save & Run source and the ingestion will start. Adding metadata to the datasets 1) Access to DataHub 2) Access to the desired dataset 3) You can edit the metadata of the dataset in the column shown at the right side of the following figure and descriptions , tags and glossary terms can be added to the dataset fields: 4) In the documenation tab, the documentation of the dataset can be added directly in formatted text or via a link to the documentation of it. 5) The lineage tab allows to edit the lineage of the dataset. 6) The queries tab allows to add the most common queries conducted to the dataset. However, this queries cannot be executed from DataHub. Dataset Usage & Query History Altough is not possible to execute queries from DataHub, it is possible to add the usage and query history of the dataset. Some ingestion sources have the option to add the usage and query history. To see if our desired source has this option enabled we should check the documentation for it in the DataHub website . This figure shows the option that should be present to have the Dataset Usage enabled: If the option is enabled, the usage and query history of the dataset will be shown in the dataset view as shown in the following figure:","title":"DataHub example of Integration"},{"location":"tools/datahub/examples/#datahub-example-of-integration","text":"","title":"DataHub example of Integration"},{"location":"tools/datahub/examples/#creating-an-ingestion-source","text":"","title":"Creating an Ingestion Source"},{"location":"tools/datahub/examples/#introduction","text":"In this example we will show how to create an ingestion source that pulls data from the desired platform to DataHub.","title":"Introduction"},{"location":"tools/datahub/examples/#prerequisites","text":"Access to DataHub Access to the platform to be integrated with DataHub Check if the platform to be integrated with DataHub is already supported by Datahub. List of ingestion sources","title":"Prerequisites"},{"location":"tools/datahub/examples/#creating-the-recipe","text":"1) Access to DataHub 2) Go to the Ingestion view and click in create new source. 3) Select the platform to be integrated with DataHub. 4) Fill the form with the information of the platform to be integrated with DataHub. Some of the platforms have forms to be fulfilled with the information of the platform to be integrated with DataHub. In other cases, the information is provided in a yaml style as shown in the following figure: 5) Schedule the ingestion of data as shown in the following figure: 6) Click in Save & Run source and the ingestion will start.","title":"Creating the recipe"},{"location":"tools/datahub/examples/#adding-metadata-to-the-datasets","text":"1) Access to DataHub 2) Access to the desired dataset 3) You can edit the metadata of the dataset in the column shown at the right side of the following figure and descriptions , tags and glossary terms can be added to the dataset fields: 4) In the documenation tab, the documentation of the dataset can be added directly in formatted text or via a link to the documentation of it. 5) The lineage tab allows to edit the lineage of the dataset. 6) The queries tab allows to add the most common queries conducted to the dataset. However, this queries cannot be executed from DataHub.","title":"Adding metadata to the datasets"},{"location":"tools/datahub/examples/#dataset-usage-query-history","text":"Altough is not possible to execute queries from DataHub, it is possible to add the usage and query history of the dataset. Some ingestion sources have the option to add the usage and query history. To see if our desired source has this option enabled we should check the documentation for it in the DataHub website . This figure shows the option that should be present to have the Dataset Usage enabled: If the option is enabled, the usage and query history of the dataset will be shown in the dataset view as shown in the following figure:","title":"Dataset Usage &amp; Query History"},{"location":"tools/downscaling/","text":"Downscaling Introduction Downscaling is a numerical modeling tool that allows quantifying the level of pollution in cities. This tool is capable of integrating different data sources such as geospatial information, traffic data, COPERNICUS data, and regional atmospheric (WRF) and pollution (CHIMERE) models to calculate pollution levels in cities with high street-level resolution using a street canyon model MUNICH. This tool assists administrations in decision-making and plays a crucial role in the era of digital transformation by creating digital twins of cities. Features of Downscaling Prerequisites To compile the tool, some prerequisites relating to the computing resources are necessary. Linux system operating system CPU with 8-16 cores 32 GB RAM To download the MUNICH model, please, see the documentation here , where you can find all the dependencies and libraries necessary. Multisources data integration. The tool is capable of integrating traffic data from various cities, pollution data from COPERNICUS or satellite sources (SENTINEL 5 and 7), terrain elevation, and building geometry, among other information. Figure 1: Different kinds of sources that the tool are available to use These input data are typically provided in netCDF file format for the model to recognize and integrate. Advanced numerical model MUNICH. Based on all received data sources, Downscaling tool utilizes MUNICH (Model of Urban Network of Intersecting Canyons and Highways), an advanced numerical model that calculates the concentration of pollutants such as CO, NO, CO2, O3, NO2, particulate matter and black carbon. Figure 2: General diagram of downscaling superresolution architecture. Figure 3: Example of MUNICH pollutant calculation. Geospatial analysis. The tool enables spatial analysis of pollution, facilitating the identification of critical areas where high concentrations of pollutants are found to develop mitigation measures. Real-time data. Capable of generating and displaying real-time results, including simulations for the future and simulations over an extended period. Outputs and Interactive Visualization. The model provides results in netCDF files, which are subsequently processed to offer an interactive visualization showing the concentration of various pollutants in the city. Concentration values are displayed for each street. Figure 4: Workflow of the tool. Use case This is a case study in the city of Lindau (Germany) that illustrates how the tool operates, the results it produces, and the visualization it provides. In this visualizer, you can see the concentration level of each street, as well as apply different types of restrictions in the city to see the effect they have. Figure 5: Example of real use case in the city of Lindau. Online visualizer can be found in this link Another example in the city of Cartagena is available in this link References MUNICH documentation can be found here","title":"Downscaling"},{"location":"tools/downscaling/#downscaling","text":"","title":"Downscaling"},{"location":"tools/downscaling/#introduction","text":"Downscaling is a numerical modeling tool that allows quantifying the level of pollution in cities. This tool is capable of integrating different data sources such as geospatial information, traffic data, COPERNICUS data, and regional atmospheric (WRF) and pollution (CHIMERE) models to calculate pollution levels in cities with high street-level resolution using a street canyon model MUNICH. This tool assists administrations in decision-making and plays a crucial role in the era of digital transformation by creating digital twins of cities.","title":"Introduction"},{"location":"tools/downscaling/#features-of-downscaling","text":"","title":"Features of Downscaling"},{"location":"tools/downscaling/#prerequisites","text":"To compile the tool, some prerequisites relating to the computing resources are necessary. Linux system operating system CPU with 8-16 cores 32 GB RAM To download the MUNICH model, please, see the documentation here , where you can find all the dependencies and libraries necessary. Multisources data integration. The tool is capable of integrating traffic data from various cities, pollution data from COPERNICUS or satellite sources (SENTINEL 5 and 7), terrain elevation, and building geometry, among other information. Figure 1: Different kinds of sources that the tool are available to use These input data are typically provided in netCDF file format for the model to recognize and integrate. Advanced numerical model MUNICH. Based on all received data sources, Downscaling tool utilizes MUNICH (Model of Urban Network of Intersecting Canyons and Highways), an advanced numerical model that calculates the concentration of pollutants such as CO, NO, CO2, O3, NO2, particulate matter and black carbon. Figure 2: General diagram of downscaling superresolution architecture. Figure 3: Example of MUNICH pollutant calculation. Geospatial analysis. The tool enables spatial analysis of pollution, facilitating the identification of critical areas where high concentrations of pollutants are found to develop mitigation measures. Real-time data. Capable of generating and displaying real-time results, including simulations for the future and simulations over an extended period. Outputs and Interactive Visualization. The model provides results in netCDF files, which are subsequently processed to offer an interactive visualization showing the concentration of various pollutants in the city. Concentration values are displayed for each street. Figure 4: Workflow of the tool.","title":"Prerequisites"},{"location":"tools/downscaling/#use-case","text":"This is a case study in the city of Lindau (Germany) that illustrates how the tool operates, the results it produces, and the visualization it provides. In this visualizer, you can see the concentration level of each street, as well as apply different types of restrictions in the city to see the effect they have. Figure 5: Example of real use case in the city of Lindau. Online visualizer can be found in this link Another example in the city of Cartagena is available in this link","title":"Use case"},{"location":"tools/downscaling/#references","text":"MUNICH documentation can be found here","title":"References"},{"location":"tools/druid/","text":"Apache Druid Introduction Apache Druid is a high-performance, distributed data store designed for real-time analytics on large-scale datasets. It excels in scenarios where fast queries and ingest are crucial, such as in business intelligence applications, operational analytics, and real-time monitoring. Druid is known for its ability to handle high concurrency, its columnar storage format, and its ability to scale horizontally. It supports streaming and batch data ingestion, and its architecture allows for efficient data summarisation, fast aggregation queries, and low-latency data access. Druid is often used in conjunction with big data technologies like Hadoop and Kafka, making it a popular choice for organisations looking to implement real-time analytics solutions. Apache Druid is intended to be used by data engineers or data scientists who are familiar with data warehousing concepts and are looking for a high-performance, scalable solution for real-time analytics. Thus, this tool will be handled by the Greengage team and the data stored in it will be employed by non-tech users through Apache Superset or other tools. Features of Apache Druid Real-time Data Ingestion: Druid can ingest data in real-time, allowing for immediate data querying and analysis. This feature is crucial for applications that require up-to-the-minute data, like monitoring and event-driven systems. Horizontal Scalability: It can scale horizontally across commodity servers, enhancing performance and storage capacity linearly with the addition of more resources. This makes it well-suited for handling large and growing datasets. Columnar Storage Format: Druid stores data in a columnar format, which is optimised for fast querying. This format allows for efficient data compression and rapid aggregation, making it ideal for analytics workloads. Highly Concurrent: The system can handle a high number of concurrent queries, making it suitable for environments with multiple users or applications accessing the data simultaneously. Fast Aggregations and Filters: Druid is designed for quick data aggregations and filtering, enabling rapid, on-the-fly data analysis. This is particularly beneficial for business intelligence and reporting use cases. Approximate Queries: It supports approximate queries, such as top-N and count-distinct, which can be executed faster than exact computations at the cost of some accuracy. This is useful for large-scale data exploration. Data Summarization and Roll-up: Druid can automatically summarise and roll-up data at ingestion time, reducing the data volume and speeding up query times. This is particularly useful for time-series data. Fault Tolerance: The system is designed to be fault-tolerant, with data replication and recovery mechanisms in place. This ensures high availability and data integrity. Flexible Data Model: Druid supports a flexible schema-on-read model, allowing ingestion of semi-structured data and different data types. Use Case Scenario Context: In an urban citizen observatory, members of the community actively participate in monitoring and analysing environmental conditions. One of the critical concerns in many cities is air quality, which directly impacts public health and quality of life. Objective: To monitor, analyse, and visualise air quality data across different parts of the city to identify pollution hotspots, understand temporal trends, and engage the public in environmental awareness and decision-making processes. Implementation Using Apache Druid: 1) Data Collection: - Deploy IoT sensors across the city to measure air quality indicators like PM2.5, PM10, NO2, CO2, and Ozone. 2) Data Ingestion and Storage with Apache Druid: - Stream the sensor data into Apache Druid for real-time ingestion and storage. - Configure Druid to handle the high volume of time-series data from the sensors, optimising for fast access and query performance. 3) Real-time Data Analysis and Aggregation: - Perform real-time data analysis using Druid's fast aggregation and filtering capabilities. - Aggregate data at various time intervals to observe trends and detect anomalies in air quality. 4) Integration with Apache Superset for Visualisation: - Connect Apache Superset to Apache Druid to leverage its advanced visualisation capabilities. - Ensure real-time data is accessible in Superset for immediate analysis and dashboarding. 5) Dashboard Creation and Data-Driven Decision Making (Out of Scope of Apache Druid): - Develop an interactive dashboard in Superset, powered by the real-time data processed in Druid. - Use insights from the Superset dashboard, underpinned by Apache Druid's real-time analytics, to guide community actions and policy recommendations. References Apache Druid documentation","title":"Apache Druid"},{"location":"tools/druid/#apache-druid","text":"","title":"Apache Druid"},{"location":"tools/druid/#introduction","text":"Apache Druid is a high-performance, distributed data store designed for real-time analytics on large-scale datasets. It excels in scenarios where fast queries and ingest are crucial, such as in business intelligence applications, operational analytics, and real-time monitoring. Druid is known for its ability to handle high concurrency, its columnar storage format, and its ability to scale horizontally. It supports streaming and batch data ingestion, and its architecture allows for efficient data summarisation, fast aggregation queries, and low-latency data access. Druid is often used in conjunction with big data technologies like Hadoop and Kafka, making it a popular choice for organisations looking to implement real-time analytics solutions. Apache Druid is intended to be used by data engineers or data scientists who are familiar with data warehousing concepts and are looking for a high-performance, scalable solution for real-time analytics. Thus, this tool will be handled by the Greengage team and the data stored in it will be employed by non-tech users through Apache Superset or other tools.","title":"Introduction"},{"location":"tools/druid/#features-of-apache-druid","text":"Real-time Data Ingestion: Druid can ingest data in real-time, allowing for immediate data querying and analysis. This feature is crucial for applications that require up-to-the-minute data, like monitoring and event-driven systems. Horizontal Scalability: It can scale horizontally across commodity servers, enhancing performance and storage capacity linearly with the addition of more resources. This makes it well-suited for handling large and growing datasets. Columnar Storage Format: Druid stores data in a columnar format, which is optimised for fast querying. This format allows for efficient data compression and rapid aggregation, making it ideal for analytics workloads. Highly Concurrent: The system can handle a high number of concurrent queries, making it suitable for environments with multiple users or applications accessing the data simultaneously. Fast Aggregations and Filters: Druid is designed for quick data aggregations and filtering, enabling rapid, on-the-fly data analysis. This is particularly beneficial for business intelligence and reporting use cases. Approximate Queries: It supports approximate queries, such as top-N and count-distinct, which can be executed faster than exact computations at the cost of some accuracy. This is useful for large-scale data exploration. Data Summarization and Roll-up: Druid can automatically summarise and roll-up data at ingestion time, reducing the data volume and speeding up query times. This is particularly useful for time-series data. Fault Tolerance: The system is designed to be fault-tolerant, with data replication and recovery mechanisms in place. This ensures high availability and data integrity. Flexible Data Model: Druid supports a flexible schema-on-read model, allowing ingestion of semi-structured data and different data types.","title":"Features of Apache Druid"},{"location":"tools/druid/#use-case-scenario","text":"","title":"Use Case Scenario"},{"location":"tools/druid/#context","text":"In an urban citizen observatory, members of the community actively participate in monitoring and analysing environmental conditions. One of the critical concerns in many cities is air quality, which directly impacts public health and quality of life.","title":"Context:"},{"location":"tools/druid/#objective","text":"To monitor, analyse, and visualise air quality data across different parts of the city to identify pollution hotspots, understand temporal trends, and engage the public in environmental awareness and decision-making processes.","title":"Objective:"},{"location":"tools/druid/#implementation-using-apache-druid","text":"1) Data Collection: - Deploy IoT sensors across the city to measure air quality indicators like PM2.5, PM10, NO2, CO2, and Ozone. 2) Data Ingestion and Storage with Apache Druid: - Stream the sensor data into Apache Druid for real-time ingestion and storage. - Configure Druid to handle the high volume of time-series data from the sensors, optimising for fast access and query performance. 3) Real-time Data Analysis and Aggregation: - Perform real-time data analysis using Druid's fast aggregation and filtering capabilities. - Aggregate data at various time intervals to observe trends and detect anomalies in air quality. 4) Integration with Apache Superset for Visualisation: - Connect Apache Superset to Apache Druid to leverage its advanced visualisation capabilities. - Ensure real-time data is accessible in Superset for immediate analysis and dashboarding. 5) Dashboard Creation and Data-Driven Decision Making (Out of Scope of Apache Druid): - Develop an interactive dashboard in Superset, powered by the real-time data processed in Druid. - Use insights from the Superset dashboard, underpinned by Apache Druid's real-time analytics, to guide community actions and policy recommendations.","title":"Implementation Using Apache Druid:"},{"location":"tools/druid/#references","text":"Apache Druid documentation","title":"References"},{"location":"tools/druid/usage/","text":"Apache Druid Usage Guide Apache Druid offers a variety of ways to ingest data. The most common way is to use the native batch ingestion system. This system is designed to handle large amounts of data and is the recommended way to ingest data into Druid. This usage guide provide instructions on how to use Apache Druid. Step 1: Log In - Access to the Druid deploy of Greengage. - First of all, you should login using your Greengage login in the keycloak authentication webpage that will appear. - You will reach to the main view of the tool as shown in the figure below. Step 2: Loading data Click on the Load data button at the top menu. Here we have several options, however, we will use the Batch - SQL option. This will bring us to the view shown in the figure below. Apache Druid offers us several options to load data, from several online providers to local files. In this example we will employ a HTTPS URL to load the data from an Open API. However, you can use any option since the process is the same. Once we introduced the URL, we should click on the NEXT button at the bottom-right part of the view. This will bring us to the view shown in the figure below. In this step we should configure the data ingestion settings (usually Apache Druid detects them automatically). However, there is an important setting that we should configure: the timestamp . This setting is the one that will be used to order the data in the database. In this case, we will use the month field because the rows do not have a timestamp field. Once we have configured the settings, we should click on the NEXT button at the bottom-right part of the view. Subsequently, Apache Druid will identify the type of each column automatically. However, we can change the type of each column or add new ones if we want. Once we have configured the settings, we should click on the Start loading data button at the bottom-right part of the view. The loading of data is done in separated tasks. Once all the tasks are finished, the data will be available in the database. You can check the status of the task in the Tasks button at the top menu. Step 3: Check datasources Click on the Datasources button at the top menu. In this view, you can visualise the different datasources that you have in the database. When you click on a datasource, you will be able to see the different columns that it has. Furthermore, you can see the different values that each column has. In this view you can also conduct different actions to the datasource, such as, edit the several performance settings or delete it. Step 4: Querying datasources Click on the QUERY button at the top menu. This view will allow you to query the different datasources that you have in the database. At the left part of the view, all the available datasources are shown. In the middle part of the view, you can write the query that you want to execute. After that, you should click on the Run button at the middle-left part of the view. The results of the query will be shown at the bottom part of the view. Extra: Explore datasources Click on the THREE DOTS at the rightmost part of the top menu. Then, click on the Explore button. This view will allow you to explore the different datasources that you have in the database. At the left part you should select the datasource that you want to explore and in the right part you may choose the chart type and its configuration. After that, you should select the desired column to explore. Finally, you can add several filters at the top part of the view. Download data from Apache Druid Step 1: Go to query section Click on the QUERY button at the top menu. Step 2: Write the query Create a query to select the data that you want to download. In this case, we will download all the data from the test datasource. SELECT * FROM test Step 3: Run the query Click on the Run button at the middle-left part of the view. The results of the query will be shown at the bottom part of the view. VERY IMPORTANT! You should uncheck the Limit inline result option in the engine configuration. This option is shown in the figure below. Step 4: Download the data Finally, at the right-most part of the screen, click on the down pointing arrow and download the data in the desired format.","title":"Apache Druid Usage Guide"},{"location":"tools/druid/usage/#apache-druid-usage-guide","text":"Apache Druid offers a variety of ways to ingest data. The most common way is to use the native batch ingestion system. This system is designed to handle large amounts of data and is the recommended way to ingest data into Druid. This usage guide provide instructions on how to use Apache Druid. Step 1: Log In - Access to the Druid deploy of Greengage. - First of all, you should login using your Greengage login in the keycloak authentication webpage that will appear. - You will reach to the main view of the tool as shown in the figure below. Step 2: Loading data Click on the Load data button at the top menu. Here we have several options, however, we will use the Batch - SQL option. This will bring us to the view shown in the figure below. Apache Druid offers us several options to load data, from several online providers to local files. In this example we will employ a HTTPS URL to load the data from an Open API. However, you can use any option since the process is the same. Once we introduced the URL, we should click on the NEXT button at the bottom-right part of the view. This will bring us to the view shown in the figure below. In this step we should configure the data ingestion settings (usually Apache Druid detects them automatically). However, there is an important setting that we should configure: the timestamp . This setting is the one that will be used to order the data in the database. In this case, we will use the month field because the rows do not have a timestamp field. Once we have configured the settings, we should click on the NEXT button at the bottom-right part of the view. Subsequently, Apache Druid will identify the type of each column automatically. However, we can change the type of each column or add new ones if we want. Once we have configured the settings, we should click on the Start loading data button at the bottom-right part of the view. The loading of data is done in separated tasks. Once all the tasks are finished, the data will be available in the database. You can check the status of the task in the Tasks button at the top menu. Step 3: Check datasources Click on the Datasources button at the top menu. In this view, you can visualise the different datasources that you have in the database. When you click on a datasource, you will be able to see the different columns that it has. Furthermore, you can see the different values that each column has. In this view you can also conduct different actions to the datasource, such as, edit the several performance settings or delete it. Step 4: Querying datasources Click on the QUERY button at the top menu. This view will allow you to query the different datasources that you have in the database. At the left part of the view, all the available datasources are shown. In the middle part of the view, you can write the query that you want to execute. After that, you should click on the Run button at the middle-left part of the view. The results of the query will be shown at the bottom part of the view. Extra: Explore datasources Click on the THREE DOTS at the rightmost part of the top menu. Then, click on the Explore button. This view will allow you to explore the different datasources that you have in the database. At the left part you should select the datasource that you want to explore and in the right part you may choose the chart type and its configuration. After that, you should select the desired column to explore. Finally, you can add several filters at the top part of the view.","title":"Apache Druid Usage Guide"},{"location":"tools/druid/usage/#download-data-from-apache-druid","text":"Step 1: Go to query section Click on the QUERY button at the top menu. Step 2: Write the query Create a query to select the data that you want to download. In this case, we will download all the data from the test datasource. SELECT * FROM test Step 3: Run the query Click on the Run button at the middle-left part of the view. The results of the query will be shown at the bottom part of the view. VERY IMPORTANT! You should uncheck the Limit inline result option in the engine configuration. This option is shown in the figure below. Step 4: Download the data Finally, at the right-most part of the screen, click on the down pointing arrow and download the data in the desired format.","title":"Download data from Apache Druid"},{"location":"tools/druid/examples/","text":"Apache Druid The integration of Apache Druid are done from other tools like Apache Superset or DataHub. Thus, you can access to the intregration guides of Apache Druid from the guides of the other tools: - Apache Superset - DataHub","title":"Apache Druid"},{"location":"tools/druid/examples/#apache-druid","text":"The integration of Apache Druid are done from other tools like Apache Superset or DataHub. Thus, you can access to the intregration guides of Apache Druid from the guides of the other tools: - Apache Superset - DataHub","title":"Apache Druid"},{"location":"tools/greengage-app-api/","text":"Greengage APP API Introduction: The greengage app api is mesh of multiple services which are provided via a single gateway. This underlying docuemntation will show you in a simple way how you can interact with application and how you can adopt your application to fit in the ecosystem. Endpoints Staging: https://api-stage.greengage.dev/graphql Prouction (available January 2024): https://api.greengage.dev/graphql Datastudio The endpoints always a control-panel under /cp which allows you open the complete documenation of the active connected services eg. https://api-stage.greengage.dev/cp Technology Due to the fact that the (micro-)services are made accessable via the Apollo Server the location, storage of each service is masked by the gateway. GraphQL GraphQL is like a superhero of data query languages, swooping in to save the day for developers tired of over-fetching or under-fetching data. Created by Facebook, this query language provides a more efficient and flexible alternative to traditional REST APIs. With GraphQL, you get to specify exactly what data you need, and you'll receive it in a neat JSON package, eliminating the excess baggage of unnecessary information. It's like ordering a customized pizza instead of settling for a pre-made slice\u2014you get exactly what you asked for, and nothing more. GraphQL provides various type of interactions but the most important are - queries - mutations Queries To get data out of the API you make a \"query\". The datastudio can help you write meaningful queries and helps you find out what the services can do, without you knowing anything about the location or the queries. Example: query Cities { cities { id name } } Mutations Mutations on the other hand are the WRITE/UPDATE/DELETE operations. Or in simple terms every time you send data to the service which should taken care of. Example: ```graphql mutation RegisterAccount { register(data:{ email:\"mpi@sushi.ev\" }) { token refresh_token } } `````` For an overview of all the current data sets and mutations please vistit api-stage.greengage.dev where you can find the complete schema and the interactive documentation. Examples for useage or integratoin An example to provide a restful api via graphql endpoint can be seen here . The link includes a demo implementation and can be used for further useages. For a client side approach to use the graphql endpoint please follow this link . Integration If you want to be integrated into the greengage api or need further support please send an email to support@sushi.dev","title":"Greengage APP API"},{"location":"tools/greengage-app-api/#greengage-app-api","text":"","title":"Greengage APP API"},{"location":"tools/greengage-app-api/#introduction","text":"The greengage app api is mesh of multiple services which are provided via a single gateway. This underlying docuemntation will show you in a simple way how you can interact with application and how you can adopt your application to fit in the ecosystem.","title":"Introduction:"},{"location":"tools/greengage-app-api/#endpoints","text":"Staging: https://api-stage.greengage.dev/graphql Prouction (available January 2024): https://api.greengage.dev/graphql","title":"Endpoints"},{"location":"tools/greengage-app-api/#datastudio","text":"The endpoints always a control-panel under /cp which allows you open the complete documenation of the active connected services eg. https://api-stage.greengage.dev/cp","title":"Datastudio"},{"location":"tools/greengage-app-api/#technology","text":"Due to the fact that the (micro-)services are made accessable via the Apollo Server the location, storage of each service is masked by the gateway.","title":"Technology"},{"location":"tools/greengage-app-api/#graphql","text":"GraphQL is like a superhero of data query languages, swooping in to save the day for developers tired of over-fetching or under-fetching data. Created by Facebook, this query language provides a more efficient and flexible alternative to traditional REST APIs. With GraphQL, you get to specify exactly what data you need, and you'll receive it in a neat JSON package, eliminating the excess baggage of unnecessary information. It's like ordering a customized pizza instead of settling for a pre-made slice\u2014you get exactly what you asked for, and nothing more. GraphQL provides various type of interactions but the most important are - queries - mutations","title":"GraphQL"},{"location":"tools/greengage-app-api/#queries","text":"To get data out of the API you make a \"query\". The datastudio can help you write meaningful queries and helps you find out what the services can do, without you knowing anything about the location or the queries. Example: query Cities { cities { id name } }","title":"Queries"},{"location":"tools/greengage-app-api/#mutations","text":"Mutations on the other hand are the WRITE/UPDATE/DELETE operations. Or in simple terms every time you send data to the service which should taken care of. Example: ```graphql mutation RegisterAccount { register(data:{ email:\"mpi@sushi.ev\" }) { token refresh_token } } `````` For an overview of all the current data sets and mutations please vistit api-stage.greengage.dev where you can find the complete schema and the interactive documentation.","title":"Mutations"},{"location":"tools/greengage-app-api/#examples-for-useage-or-integratoin","text":"An example to provide a restful api via graphql endpoint can be seen here . The link includes a demo implementation and can be used for further useages. For a client side approach to use the graphql endpoint please follow this link .","title":"Examples for useage or integratoin"},{"location":"tools/greengage-app-api/#integration","text":"If you want to be integrated into the greengage api or need further support please send an email to support@sushi.dev","title":"Integration"},{"location":"tools/greengage-app-api/examples/client-js/","text":"How to to fetch data from an GRAPHQL endpoint Javascript Open index.html to see a live demo for a gql query in javascript. Please be a Curl curl --location --request POST 'https://api-stage.greengage.dev/graphql' \\ --header 'Content-Type: application/json' \\ --data-raw '{\"query\":\"query {\\n cities {\\n id\\n name\\n }\\n}\",\"variables\":{}}'","title":"Index"},{"location":"tools/greengage-app-api/examples/client-js/#how-to-to-fetch-data-from-an-graphql-endpoint","text":"","title":"How to to fetch data from an GRAPHQL endpoint"},{"location":"tools/greengage-app-api/examples/client-js/#javascript","text":"Open index.html to see a live demo for a gql query in javascript. Please be a","title":"Javascript"},{"location":"tools/greengage-app-api/examples/client-js/#curl","text":"curl --location --request POST 'https://api-stage.greengage.dev/graphql' \\ --header 'Content-Type: application/json' \\ --data-raw '{\"query\":\"query {\\n cities {\\n id\\n name\\n }\\n}\",\"variables\":{}}'","title":"Curl"},{"location":"tools/greengage-app-api/examples/restful-to-graphql/","text":"GraphQL Resolver a RESTful api Installation Step 1 Create an .env file with the following content. Add the base path to the API. PORT=3333 URL='https://example.com/' Please be aware that the URL parameter will not be taken if the request header \"drupal\" is set. Step 2 Run the following command: npm i && npm run production","title":"GraphQL Resolver a RESTful api"},{"location":"tools/greengage-app-api/examples/restful-to-graphql/#graphql-resolver-a-restful-api","text":"","title":"GraphQL Resolver a RESTful api"},{"location":"tools/greengage-app-api/examples/restful-to-graphql/#installation","text":"","title":"Installation"},{"location":"tools/greengage-app-api/examples/restful-to-graphql/#step-1","text":"Create an .env file with the following content. Add the base path to the API. PORT=3333 URL='https://example.com/' Please be aware that the URL parameter will not be taken if the request header \"drupal\" is set.","title":"Step 1"},{"location":"tools/greengage-app-api/examples/restful-to-graphql/#step-2","text":"Run the following command: npm i && npm run production","title":"Step 2"},{"location":"tools/keycloak/","text":"Keycloak Introduction: In today's rapidly evolving tech landscape, ensuring the security of your applications is paramount. As you venture into developing or enhancing your application, integrating a robust authentication and authorization mechanism is crucial. This tutorial aims to guide you through setting up a development environment by integrating Keycloak, an open-source Identity and Access Management solution. Whether you're working on a brand new project or improving an existing one, following this guide will provide a solid foundation for securing your application using Keycloak. Features Keycloak: Single Sign-On (SSO) & Sign-Out: Keycloak facilitates SSO and Single Sign-Out, allowing users to log in once and access various applications without needing to log in again. Standards-Based: Keycloak supports standard protocols for authentication and authorization such as OpenID Connect, OAuth 2.0, and SAML 2.0, ensuring compatibility with various systems. User Federation: It allows user federation with a variety of sources including LDAP and Active Directory, enabling you to manage users and their roles centrally. Customizable: Keycloak is highly customizable, offering a broad range of options to tailor authentication and authorization flows to meet your specific requirements. Scalable: It is designed to be scalable, catering to both small applications and large enterprise solutions with millions of users. Role-Based Access Control (RBAC): Keycloak provides fine-grained authorization and role management, enabling you to control who has access to what in your applications. Multi-factor Authentication (MFA): It supports multi-factor authentication, enhancing security by requiring users to provide multiple forms of identification before gaining access. Social Login: Keycloak supports social logins, allowing users to log in using their social media accounts which can enhance user experience and increase adoption rates. Easy to Integrate: With a variety of client adapters and libraries, integrating Keycloak with your application is straightforward regardless of the technology stack you're using. Support and Community: Being an open-source project, Keycloak has a vibrant community and a wealth of resources available to help you throughout your integration journey. Use case The following diagram show the sequences for an user auythentication procress using Keycloak as an OpenID Connect (OIDC) provider The User initiates the process by requesting a page from the Application. The Application then redirects the User to a browser for authentication. The browser initiates an Authentication Request to Keycloak. Keycloak presents a Login Page to the User. The User enters their credentials into the login page. The credentials are submitted back to Keycloak for validation. Upon successful validation, Keycloak redirects the User back to the Application. The Application makes a Token Request to Keycloak. Keycloak validates the request and, if valid, returns a Token to the Application. With the Token, the User is granted access to the requested page within the Application.","title":"Keycloak"},{"location":"tools/keycloak/#keycloak","text":"","title":"Keycloak"},{"location":"tools/keycloak/#introduction","text":"In today's rapidly evolving tech landscape, ensuring the security of your applications is paramount. As you venture into developing or enhancing your application, integrating a robust authentication and authorization mechanism is crucial. This tutorial aims to guide you through setting up a development environment by integrating Keycloak, an open-source Identity and Access Management solution. Whether you're working on a brand new project or improving an existing one, following this guide will provide a solid foundation for securing your application using Keycloak.","title":"Introduction:"},{"location":"tools/keycloak/#features-keycloak","text":"Single Sign-On (SSO) & Sign-Out: Keycloak facilitates SSO and Single Sign-Out, allowing users to log in once and access various applications without needing to log in again. Standards-Based: Keycloak supports standard protocols for authentication and authorization such as OpenID Connect, OAuth 2.0, and SAML 2.0, ensuring compatibility with various systems. User Federation: It allows user federation with a variety of sources including LDAP and Active Directory, enabling you to manage users and their roles centrally. Customizable: Keycloak is highly customizable, offering a broad range of options to tailor authentication and authorization flows to meet your specific requirements. Scalable: It is designed to be scalable, catering to both small applications and large enterprise solutions with millions of users. Role-Based Access Control (RBAC): Keycloak provides fine-grained authorization and role management, enabling you to control who has access to what in your applications. Multi-factor Authentication (MFA): It supports multi-factor authentication, enhancing security by requiring users to provide multiple forms of identification before gaining access. Social Login: Keycloak supports social logins, allowing users to log in using their social media accounts which can enhance user experience and increase adoption rates. Easy to Integrate: With a variety of client adapters and libraries, integrating Keycloak with your application is straightforward regardless of the technology stack you're using. Support and Community: Being an open-source project, Keycloak has a vibrant community and a wealth of resources available to help you throughout your integration journey.","title":"Features Keycloak:"},{"location":"tools/keycloak/#use-case","text":"The following diagram show the sequences for an user auythentication procress using Keycloak as an OpenID Connect (OIDC) provider The User initiates the process by requesting a page from the Application. The Application then redirects the User to a browser for authentication. The browser initiates an Authentication Request to Keycloak. Keycloak presents a Login Page to the User. The User enters their credentials into the login page. The credentials are submitted back to Keycloak for validation. Upon successful validation, Keycloak redirects the User back to the Application. The Application makes a Token Request to Keycloak. Keycloak validates the request and, if valid, returns a Token to the Application. With the Token, the User is granted access to the requested page within the Application.","title":"Use case"},{"location":"tools/keycloak/integration/","text":"Keycloak Integration Guide for Third-Party Services To facilitate a centralized user management system, we are utilizing Keycloak as our identity and access management solution. To integrate your service with our Keycloak system, please follow the instructions below to request the necessary credentials. Step 1: Requesting Client Credentials Send an email to greengage.admin@deusto.es with the subject line: \"[GREENGAGE] Request for Keycloak Client Credentials\". In the body of the email, please provide the following information: Tool name: Description: (no more than 200 characters): Contact name:(for technical propose) Contact email:(for technical propose) Step 2: Receiving Your Credentials Upon receipt of your request, we will review the information and create a unique Client ID and Secret for your service. We will then send you an email with your Client ID and Secret, which you will use to configure the integration on your side. Step 3: Configuring Your Service Once you have received your Client ID and Secret, incorporate these credentials into your service\u2019s authentication module. Ensure that your service is set to communicate with our Keycloak server using the OpenID Connect protocol. You can add the following link https://[KEYCLOAK]/ auth/realms/[REALM]/account in your system. To give your user access to the information managed by keycloak. Example https://auth1.demo.greengage-project.eu/auth/realms/greengage/account/ Step 4: Testing the Integration After configuring your service, conduct thorough testing to ensure that the authentication process works correctly. If you encounter any issues during testing, please reach out to us via email for support. If you have any questions about Keycloak integration, please contact us at greengage.admin@deusto.es with the subject \"[Greengage] Keycloak support request\" Important Notes: Keep your Client Secret confidential. Do not share it with unauthorized personnel or services. If you believe your Client Secret has been compromised, contact us immediately to issue a new one. Adhere to all security best practices when implementing the integration to safeguard user data.","title":"Keycloak Integration Guide for Third-Party Services"},{"location":"tools/keycloak/integration/#keycloak-integration-guide-for-third-party-services","text":"To facilitate a centralized user management system, we are utilizing Keycloak as our identity and access management solution. To integrate your service with our Keycloak system, please follow the instructions below to request the necessary credentials. Step 1: Requesting Client Credentials Send an email to greengage.admin@deusto.es with the subject line: \"[GREENGAGE] Request for Keycloak Client Credentials\". In the body of the email, please provide the following information: Tool name: Description: (no more than 200 characters): Contact name:(for technical propose) Contact email:(for technical propose) Step 2: Receiving Your Credentials Upon receipt of your request, we will review the information and create a unique Client ID and Secret for your service. We will then send you an email with your Client ID and Secret, which you will use to configure the integration on your side. Step 3: Configuring Your Service Once you have received your Client ID and Secret, incorporate these credentials into your service\u2019s authentication module. Ensure that your service is set to communicate with our Keycloak server using the OpenID Connect protocol. You can add the following link https://[KEYCLOAK]/ auth/realms/[REALM]/account in your system. To give your user access to the information managed by keycloak. Example https://auth1.demo.greengage-project.eu/auth/realms/greengage/account/ Step 4: Testing the Integration After configuring your service, conduct thorough testing to ensure that the authentication process works correctly. If you encounter any issues during testing, please reach out to us via email for support. If you have any questions about Keycloak integration, please contact us at greengage.admin@deusto.es with the subject \"[Greengage] Keycloak support request\" Important Notes: Keep your Client Secret confidential. Do not share it with unauthorized personnel or services. If you believe your Client Secret has been compromised, contact us immediately to issue a new one. Adhere to all security best practices when implementing the integration to safeguard user data.","title":"Keycloak Integration Guide for Third-Party Services"},{"location":"tools/keycloak/examples/","text":"Examples This section includes examples in different programming languages on how integration with the provided KeyCloak can be performed. Nodejs Prerequisites Docker and Docker Compose installed. Node.js and npm installed. Basic understanding of Keycloak, Docker, and Node.js. Project Structure Source: example example/ |-- docker-compose.yml |-- realm-export.json |-- keycloak.json |-- package.json |-- index.js |-- run.sh (for Linux/macOS) |-- run.bat (for Windows) Step 1: Setting Up Keycloak docker-compose.yml : This file contains the configuration to run a Keycloak container. Make sure the docker-compose.yml file is set up as provided in your project. realm-export.json : This file should be configured according to your Keycloak realm requirements. It contains realm, client, user, and role configurations. Step 2: Setting Up Node.js Application package.json : This file contains your project metadata and dependencies. Ensure express , express-session , and keycloak-connect dependencies are listed. index.js : This file contains your Express application setup. It sets up routes for login, logout, and the home page which displays the JWT. keycloak.json : This file contains the Keycloak client configuration. Update the realm , resource , and credentials fields with your Keycloak configuration. Installing Dependencies : Run the following command to install the necessary packages as listed in your package.json : npm i Step 3: Running the Services Linux/macOS : Ensure run.sh is executable: chmod +x run.sh . Execute run.sh to start the services: ./run.sh . Windows : Double-click run.bat or run it in the command prompt to start the services. Accessing the Application Navigate to localhost:3000/auth to log in using Keycloak. Once logged in, navigate to localhost:3000 to view the JWT and its decoded payload. To logout, navigate to localhost:3000/logout . Step 1: Install and Setup Keycloak Download and install Keycloak from the official website . Start Keycloak by navigating to the bin directory of your Keycloak installation and executing the standalone.sh (for Linux/macOS) or standalone.bat (for Windows) script. Access the Keycloak Admin Console at http://localhost:8080/auth and complete the initial setup. Create an admin user for managing Keycloak. Step 2: Create a Realm and a Client (OpenID Connect) Create a New Realm: Navigate to the Keycloak Admin Console. Click on \"Add realm\" to create a new realm. Enter the required details for your realm and save. Register a Client: Navigate to Clients and click Create . Provide a Client ID , and select the Client Protocol as openid-connect . Select the Client Access Type as confidential if your client is a web application that can secure the client secret. Otherwise, select public if your client is a native app or a JavaScript app running in the browser. Set Standard Flow Enabled to ON if you want to use the Authorization Code Flow which is recommended for most scenarios. Configure OpenID Connect Protocol: For each client, you can tailor what claims and assertions are stored in the OIDC token by creating and configuring protocol mappers. You may need to set up JSON mapping for certain claim keys in your application to handle roles or other claims passed by Keycloak. Client Adapters: Install a Keycloak Adapter in your application environment to communicate and be secured by Keycloak. Keycloak provides adapters for different platforms, and there are also third-party adapters available. Test Your Setup: At this point, it would be prudent to test your setup by attempting to authenticate using OpenID Connect. There are various grant types supported by Keycloak for authenticating users including authorization code, implicit, and client credentials. Additional Configuration (Optional): Depending on your application's requirements, you might need to configure additional settings in Keycloak or in your application. For instance, you might need to set up user roles, groups, and permissions, or configure multi-factor authentication. Documentations and Tutorials: There are various resources available that provide step-by-step guides or tutorials on integrating OpenID Connect with Keycloak, including the Keycloak official documentation . This extended step should provide a more thorough understanding of how to integrate OpenID Connect with Keycloak. However, the exact steps might vary based on your application's technology stack and your specific requirements. Step 3: Configure your System For a JavaScript application, you could use the Keycloak JavaScript adapter. npm install keycloak-js or a middleware as keycloak-connect npm install keycloak-connect Configure the library with the details of your Keycloak realm and client. // Example configuration for a JavaScript application const keycloak = Keycloak({ url: \"http://localhost:8080/auth\", realm: \"<your-realm>\", clientId: \"<your-client-id>\", }); Step 4: Integrate Authentication Use the library to add authentication to your system. For a web application, this would typically involve redirecting unauthenticated users to the Keycloak login page, and handling the tokens returned by Keycloak upon successful authentication. // Example integration for a JavaScript application keycloak .init({ onLoad: \"login-required\" }) .then((authenticated) => { console.log(authenticated ? \"Authenticated\" : \"Not authenticated\"); }) .catch((error) => { console.error(\"Failed to initialize authentication\", error); }); Step 5: Integrate Authorization Use the tokens obtained during authentication to make authorized requests to your system's backend, and to check the user's roles and permissions. // Example authorization check in a JavaScript application if (keycloak.hasRealmRole(\"admin\")) { console.log(\"User is an admin\"); } This tutorial provides a high-level overview of the steps involved in integrating Keycloak with your system. The exact steps and code may vary depending on the specifics of your system and the programming languages and frameworks you are using. Python Prerequisites Docker and Docker Compose installed. Python and pip installed. Basic understanding of Keycloak, Docker, Flask, and Python. Access the application over HTTPS and accept the self-signed certificate warning: \"The certificate is not trusted because it is self-signed.\" Project Structure Source: example example/ |-- docker-compose.yml |-- realm-export.json |-- client_secrets.json |-- requirements.txt |-- index.py |-- run.sh (for Linux/macOS) |-- run.bat (for Windows) Step 1: Setting Up Keycloak docker-compose.yml : This file contains the configuration to run a Keycloak container. Ensure it's set up as provided in your project. realm-export.json : Configure this file according to your Keycloak realm requirements. It contains realm, client, user, and role configurations. Step 2: Setting Up Python Flask Application requirements.txt : Lists the necessary Python packages, including Flask and Flask-OIDC. client_secrets.json : Contains Keycloak client configuration. Update the client_id , client_secret , and URLs according to your Keycloak setup. index.py : Contains your Flask application. Sets up routes for login, logout, and home page displaying user information. Step 3: Installing Dependencies Run the following command to install necessary Python packages: pip install -r requirements.txt Step 4: Running the Services Linux/macOS : Make run.sh executable: chmod +x run.sh . Execute run.sh to start the services: ./run.sh . Windows : Execute run.bat to start the services. Accessing the Application Navigate to https://localhost:3000/ (accept the self-signed certificate warning). Use the Keycloak authentication process to log in. Once logged in, user information will be displayed on the home page. To logout, navigate to https://localhost:3000/logout . Important Notes Ensure all URLs in client_secrets.json and realm-export.json match your Keycloak configuration. Remember to access the application via HTTPS and accept the browser warning about the self-signed certificate. Modify the index.py file to suit your application's specific Flask and OIDC needs. For more information you can visit the official documentation in https://www.keycloak.org/documentation","title":"Examples"},{"location":"tools/keycloak/examples/#examples","text":"This section includes examples in different programming languages on how integration with the provided KeyCloak can be performed.","title":"Examples"},{"location":"tools/keycloak/examples/#nodejs","text":"","title":"Nodejs"},{"location":"tools/keycloak/examples/#prerequisites","text":"Docker and Docker Compose installed. Node.js and npm installed. Basic understanding of Keycloak, Docker, and Node.js.","title":"Prerequisites"},{"location":"tools/keycloak/examples/#project-structure","text":"Source: example example/ |-- docker-compose.yml |-- realm-export.json |-- keycloak.json |-- package.json |-- index.js |-- run.sh (for Linux/macOS) |-- run.bat (for Windows)","title":"Project Structure"},{"location":"tools/keycloak/examples/#step-1-setting-up-keycloak","text":"docker-compose.yml : This file contains the configuration to run a Keycloak container. Make sure the docker-compose.yml file is set up as provided in your project. realm-export.json : This file should be configured according to your Keycloak realm requirements. It contains realm, client, user, and role configurations.","title":"Step 1: Setting Up Keycloak"},{"location":"tools/keycloak/examples/#step-2-setting-up-nodejs-application","text":"package.json : This file contains your project metadata and dependencies. Ensure express , express-session , and keycloak-connect dependencies are listed. index.js : This file contains your Express application setup. It sets up routes for login, logout, and the home page which displays the JWT. keycloak.json : This file contains the Keycloak client configuration. Update the realm , resource , and credentials fields with your Keycloak configuration. Installing Dependencies : Run the following command to install the necessary packages as listed in your package.json : npm i","title":"Step 2: Setting Up Node.js Application"},{"location":"tools/keycloak/examples/#step-3-running-the-services","text":"Linux/macOS : Ensure run.sh is executable: chmod +x run.sh . Execute run.sh to start the services: ./run.sh . Windows : Double-click run.bat or run it in the command prompt to start the services.","title":"Step 3: Running the Services"},{"location":"tools/keycloak/examples/#accessing-the-application","text":"Navigate to localhost:3000/auth to log in using Keycloak. Once logged in, navigate to localhost:3000 to view the JWT and its decoded payload. To logout, navigate to localhost:3000/logout .","title":"Accessing the Application"},{"location":"tools/keycloak/examples/#step-1-install-and-setup-keycloak","text":"Download and install Keycloak from the official website . Start Keycloak by navigating to the bin directory of your Keycloak installation and executing the standalone.sh (for Linux/macOS) or standalone.bat (for Windows) script. Access the Keycloak Admin Console at http://localhost:8080/auth and complete the initial setup. Create an admin user for managing Keycloak.","title":"Step 1: Install and Setup Keycloak"},{"location":"tools/keycloak/examples/#step-2-create-a-realm-and-a-client-openid-connect","text":"Create a New Realm: Navigate to the Keycloak Admin Console. Click on \"Add realm\" to create a new realm. Enter the required details for your realm and save. Register a Client: Navigate to Clients and click Create . Provide a Client ID , and select the Client Protocol as openid-connect . Select the Client Access Type as confidential if your client is a web application that can secure the client secret. Otherwise, select public if your client is a native app or a JavaScript app running in the browser. Set Standard Flow Enabled to ON if you want to use the Authorization Code Flow which is recommended for most scenarios. Configure OpenID Connect Protocol: For each client, you can tailor what claims and assertions are stored in the OIDC token by creating and configuring protocol mappers. You may need to set up JSON mapping for certain claim keys in your application to handle roles or other claims passed by Keycloak. Client Adapters: Install a Keycloak Adapter in your application environment to communicate and be secured by Keycloak. Keycloak provides adapters for different platforms, and there are also third-party adapters available. Test Your Setup: At this point, it would be prudent to test your setup by attempting to authenticate using OpenID Connect. There are various grant types supported by Keycloak for authenticating users including authorization code, implicit, and client credentials. Additional Configuration (Optional): Depending on your application's requirements, you might need to configure additional settings in Keycloak or in your application. For instance, you might need to set up user roles, groups, and permissions, or configure multi-factor authentication. Documentations and Tutorials: There are various resources available that provide step-by-step guides or tutorials on integrating OpenID Connect with Keycloak, including the Keycloak official documentation . This extended step should provide a more thorough understanding of how to integrate OpenID Connect with Keycloak. However, the exact steps might vary based on your application's technology stack and your specific requirements.","title":"Step 2: Create a Realm and a Client (OpenID Connect)"},{"location":"tools/keycloak/examples/#step-3-configure-your-system","text":"For a JavaScript application, you could use the Keycloak JavaScript adapter. npm install keycloak-js or a middleware as keycloak-connect npm install keycloak-connect Configure the library with the details of your Keycloak realm and client. // Example configuration for a JavaScript application const keycloak = Keycloak({ url: \"http://localhost:8080/auth\", realm: \"<your-realm>\", clientId: \"<your-client-id>\", });","title":"Step 3: Configure your System"},{"location":"tools/keycloak/examples/#step-4-integrate-authentication","text":"Use the library to add authentication to your system. For a web application, this would typically involve redirecting unauthenticated users to the Keycloak login page, and handling the tokens returned by Keycloak upon successful authentication. // Example integration for a JavaScript application keycloak .init({ onLoad: \"login-required\" }) .then((authenticated) => { console.log(authenticated ? \"Authenticated\" : \"Not authenticated\"); }) .catch((error) => { console.error(\"Failed to initialize authentication\", error); });","title":"Step 4: Integrate Authentication"},{"location":"tools/keycloak/examples/#step-5-integrate-authorization","text":"Use the tokens obtained during authentication to make authorized requests to your system's backend, and to check the user's roles and permissions. // Example authorization check in a JavaScript application if (keycloak.hasRealmRole(\"admin\")) { console.log(\"User is an admin\"); } This tutorial provides a high-level overview of the steps involved in integrating Keycloak with your system. The exact steps and code may vary depending on the specifics of your system and the programming languages and frameworks you are using.","title":"Step 5: Integrate Authorization"},{"location":"tools/keycloak/examples/#python","text":"","title":"Python"},{"location":"tools/keycloak/examples/#prerequisites_1","text":"Docker and Docker Compose installed. Python and pip installed. Basic understanding of Keycloak, Docker, Flask, and Python. Access the application over HTTPS and accept the self-signed certificate warning: \"The certificate is not trusted because it is self-signed.\"","title":"Prerequisites"},{"location":"tools/keycloak/examples/#project-structure_1","text":"Source: example example/ |-- docker-compose.yml |-- realm-export.json |-- client_secrets.json |-- requirements.txt |-- index.py |-- run.sh (for Linux/macOS) |-- run.bat (for Windows)","title":"Project Structure"},{"location":"tools/keycloak/examples/#step-1-setting-up-keycloak_1","text":"docker-compose.yml : This file contains the configuration to run a Keycloak container. Ensure it's set up as provided in your project. realm-export.json : Configure this file according to your Keycloak realm requirements. It contains realm, client, user, and role configurations.","title":"Step 1: Setting Up Keycloak"},{"location":"tools/keycloak/examples/#step-2-setting-up-python-flask-application","text":"requirements.txt : Lists the necessary Python packages, including Flask and Flask-OIDC. client_secrets.json : Contains Keycloak client configuration. Update the client_id , client_secret , and URLs according to your Keycloak setup. index.py : Contains your Flask application. Sets up routes for login, logout, and home page displaying user information.","title":"Step 2: Setting Up Python Flask Application"},{"location":"tools/keycloak/examples/#step-3-installing-dependencies","text":"Run the following command to install necessary Python packages: pip install -r requirements.txt","title":"Step 3: Installing Dependencies"},{"location":"tools/keycloak/examples/#step-4-running-the-services","text":"Linux/macOS : Make run.sh executable: chmod +x run.sh . Execute run.sh to start the services: ./run.sh . Windows : Execute run.bat to start the services.","title":"Step 4: Running the Services"},{"location":"tools/keycloak/examples/#accessing-the-application_1","text":"Navigate to https://localhost:3000/ (accept the self-signed certificate warning). Use the Keycloak authentication process to log in. Once logged in, user information will be displayed on the home page. To logout, navigate to https://localhost:3000/logout .","title":"Accessing the Application"},{"location":"tools/keycloak/examples/#important-notes","text":"Ensure all URLs in client_secrets.json and realm-export.json match your Keycloak configuration. Remember to access the application via HTTPS and accept the browser warning about the self-signed certificate. Modify the index.py file to suit your application's specific Flask and OIDC needs. For more information you can visit the official documentation in https://www.keycloak.org/documentation","title":"Important Notes"},{"location":"tools/keycloak/examples/flutter/","text":"keycloaklogin A new Flutter project. Getting Started This project is a starting point for a Flutter application. A few resources to get you started if this is your first Flutter project: Lab: Write your first Flutter app Cookbook: Useful Flutter samples For help getting started with Flutter development, view the online documentation , which offers tutorials, samples, guidance on mobile development, and a full API reference.","title":"keycloaklogin"},{"location":"tools/keycloak/examples/flutter/#keycloaklogin","text":"A new Flutter project.","title":"keycloaklogin"},{"location":"tools/keycloak/examples/flutter/#getting-started","text":"This project is a starting point for a Flutter application. A few resources to get you started if this is your first Flutter project: Lab: Write your first Flutter app Cookbook: Useful Flutter samples For help getting started with Flutter development, view the online documentation , which offers tutorials, samples, guidance on mobile development, and a full API reference.","title":"Getting Started"},{"location":"tools/keycloak/examples/flutter/ios/Runner/Assets.xcassets/LaunchImage.imageset/","text":"Launch Screen Assets You can customize the launch screen with your own desired assets by replacing the image files in this directory. You can also do it by opening your Flutter project's Xcode project with open ios/Runner.xcworkspace , selecting Runner/Assets.xcassets in the Project Navigator and dropping in the desired images.","title":"Launch Screen Assets"},{"location":"tools/keycloak/examples/flutter/ios/Runner/Assets.xcassets/LaunchImage.imageset/#launch-screen-assets","text":"You can customize the launch screen with your own desired assets by replacing the image files in this directory. You can also do it by opening your Flutter project's Xcode project with open ios/Runner.xcworkspace , selecting Runner/Assets.xcassets in the Project Navigator and dropping in the desired images.","title":"Launch Screen Assets"},{"location":"tools/mindview/","text":"MindView Introduction MindView is a crowd-driven platform for collecting and uploading geo-tagged street-level imagery, utilizing built-in smartphone cameras and compatible devices like 360\u00b0 cameras, car dashboards, and helmet cameras. This innovative system engages users to undertake diverse 'missions', such as traversing predefined routes, capturing specific objects, or remaining at designated locations, rewarding them based on the mission's complexity, duration, and quantity. The collected imagery undergoes processing through advanced deep-learning algorithms, extracting valuable, anonymized insights about urban environments and social dynamics. Key insights include housing and infrastructure data, green space analysis, socio-economic and cultural insights, and mobility and traffic patterns. This approach is particularly useful for maintaining up-to-date and comprehensive maps, a task that can be resource-intensive for official mapping agencies. This is particularly valuable in regions with limited or outdated official mapping sources. By harnessing the collective efforts of the community, especially in less documented or rapidly changing areas, MindView fills critical gaps in mapping data. MindView plays an integral role in the \"Data Crowdsourcing and Curation\" phase of the GREENGAGE project. It addresses a key citizen science challenge in geospatial data collection and mapping, enabling users to actively contribute to the creation and improvement of spatial maps, in a way that requires no pre-existing skills or knowledge. In this way, MindView serves a wide range of beneficiaries including urban planners, disaster risk managers, territorial managers, sociologists, and environmentalists, providing invaluable data for urban development, risk assessment, and sociocultural research. Features of MindView MindView is built upon several key components, each playing a vital role in its functionality: The MindEarth App (GREENGAGE Edition) This is an Android-based mobile application, currently available as a standalone APK file and soon to be launched on the Google Play Store. The MindEarth App provides users with access to a variety of missions, which are commissioned by third parties for purposes such as research, urban planning, or commercial objectives. These missions involve diverse tasks, including: - Moving from one place to another along a predetermined route, typically ranging from 500 meters to 2 kilometers. - Capturing photographs of specific objects at designated times of the day, such as shopfronts, monuments, or building facades. - Staying at a certain location for a fixed period, usually between 15 to 30 minutes. Some missions come with strict deadlines, while others can be completed at the user's convenience. The app allows users to view details of each mission, including the location, duration, required device, deadline, and compensation. Users can select missions based on their interests and availability. For instance, they might be tasked with photographing particular street scenes at specific times or days. The app is developed on a Flutter codebase with customized views and routers, facilitating each phase of the mission from reservation to image upload. Missions are designed modularly, with the possibility of integrating custom procedures to meet specific data collection needs. Mission Control: This dedicated back-end platform is crucial for defining, designing, and monitoring the missions available in the MindEarth App. It handles several key functions: - Generating Mapping Campaigns, each linked to a particular Area of Interest (AOI) that requires surveying. - Designing individual Missions for each Campaign, utilizing GeoJson format to outline the specific path mappers will follow. - Setting specific time constraints for each Campaign and Mission, including the start and end dates, as well as the times and dates for data collection. - Determining the effort (time and distance) and monetary reward associated with each mission. - Gathering statistics on mission completion, image collection, and survey quality for effective monitoring. DataView DataView is an interactive set of RESTAPIs that facilitate seamless interaction with the data generated by MindView. It allows the integration of additional data sources or services into the workflow of the APIs, enhancing their overall utility. DataView provides various data export options, enabling users to download the collected data in multiple formats such as JSON, CSV, or other geo-referenced file types. This feature is particularly useful for integrating with external data processing and visualization systems. DataView is secured by a user management system overseen by a KeyCloak instance. All API and backend connections are encrypted using SSL certificates, ensuring data security. Furthermore, all collected data is stored within the private section of an AWS VPC, emphasizing data privacy and protection. Use Case Scenario The MindEarth App is ideally suited for scenarios requiring immediate, real-time photographic surveys of urban environments. A prime example of its application is in analyzing pedestrian traffic patterns within bustling city centers or public spaces, which aids in refining urban planning and maximizing space utilization. To initiate a survey, the first step involves delineating the Area of Interest (AOI) to be surveyed. This includes setting specific parameters such as the time of day, day of the week, and the number of passovers necessary for enhancing the statistical reliability of the data. These parameters are established within the Mission Control platform, either directly by the MindEarth team according to client specifications or by an authorized external administrator. Once these missions are formulated and released, they are instantly accessible to users via the MindEarth App. Upon initial launch of the app, users are prompted to either create a new account or log in with existing credentials. Subsequently, they can explore available missions nearby through a list or map interface. These missions cover various activities, ranging from navigating specific streets or areas to following designated routes with clear start and end points. Each mission is detailed with a title, description, estimated duration, distance, and a reward for successful completion. Additionally, the app notifies users of new missions in their vicinity. Users have the flexibility to select and reserve one or more missions based on their interest. When they approach the mission's starting point, they can begin the survey by pressing the 'Start' button and then proceed as per the instructions displayed on their smartphone, including route guidance and location tracking. After completing the designated tasks and reaching the end point of the mission, users can finalize the survey. Post-mission, the app presents an upload page where users can review and upload the captured images to MindEarth\u2019s secure servers. The app's reward section displays the user's accumulated credits for completed tasks. In adherence to GDPR regulations, the app employs advanced machine learning algorithms to anonymize any personal data in the images, both on the local device and subsequently in a secure AWS environment managed by MindEarth. These anonymized images contribute to aggregated data layers focusing on specific aspects, such as pedestrian flow. The image processing is performed on MindEarth's cloud backend, remaining transparent to the user. Finally, the aggregated data and the data layers are made available to clients through DataView, a dedicated REST API that presents the database content according to a predefined schema. Users can access and interact with these APIs, which require no installation, through a public swagger interface for interactive development.","title":"MindView"},{"location":"tools/mindview/#mindview","text":"","title":"MindView"},{"location":"tools/mindview/#introduction","text":"MindView is a crowd-driven platform for collecting and uploading geo-tagged street-level imagery, utilizing built-in smartphone cameras and compatible devices like 360\u00b0 cameras, car dashboards, and helmet cameras. This innovative system engages users to undertake diverse 'missions', such as traversing predefined routes, capturing specific objects, or remaining at designated locations, rewarding them based on the mission's complexity, duration, and quantity. The collected imagery undergoes processing through advanced deep-learning algorithms, extracting valuable, anonymized insights about urban environments and social dynamics. Key insights include housing and infrastructure data, green space analysis, socio-economic and cultural insights, and mobility and traffic patterns. This approach is particularly useful for maintaining up-to-date and comprehensive maps, a task that can be resource-intensive for official mapping agencies. This is particularly valuable in regions with limited or outdated official mapping sources. By harnessing the collective efforts of the community, especially in less documented or rapidly changing areas, MindView fills critical gaps in mapping data. MindView plays an integral role in the \"Data Crowdsourcing and Curation\" phase of the GREENGAGE project. It addresses a key citizen science challenge in geospatial data collection and mapping, enabling users to actively contribute to the creation and improvement of spatial maps, in a way that requires no pre-existing skills or knowledge. In this way, MindView serves a wide range of beneficiaries including urban planners, disaster risk managers, territorial managers, sociologists, and environmentalists, providing invaluable data for urban development, risk assessment, and sociocultural research.","title":"Introduction"},{"location":"tools/mindview/#features-of-mindview","text":"MindView is built upon several key components, each playing a vital role in its functionality: The MindEarth App (GREENGAGE Edition) This is an Android-based mobile application, currently available as a standalone APK file and soon to be launched on the Google Play Store. The MindEarth App provides users with access to a variety of missions, which are commissioned by third parties for purposes such as research, urban planning, or commercial objectives. These missions involve diverse tasks, including: - Moving from one place to another along a predetermined route, typically ranging from 500 meters to 2 kilometers. - Capturing photographs of specific objects at designated times of the day, such as shopfronts, monuments, or building facades. - Staying at a certain location for a fixed period, usually between 15 to 30 minutes. Some missions come with strict deadlines, while others can be completed at the user's convenience. The app allows users to view details of each mission, including the location, duration, required device, deadline, and compensation. Users can select missions based on their interests and availability. For instance, they might be tasked with photographing particular street scenes at specific times or days. The app is developed on a Flutter codebase with customized views and routers, facilitating each phase of the mission from reservation to image upload. Missions are designed modularly, with the possibility of integrating custom procedures to meet specific data collection needs. Mission Control: This dedicated back-end platform is crucial for defining, designing, and monitoring the missions available in the MindEarth App. It handles several key functions: - Generating Mapping Campaigns, each linked to a particular Area of Interest (AOI) that requires surveying. - Designing individual Missions for each Campaign, utilizing GeoJson format to outline the specific path mappers will follow. - Setting specific time constraints for each Campaign and Mission, including the start and end dates, as well as the times and dates for data collection. - Determining the effort (time and distance) and monetary reward associated with each mission. - Gathering statistics on mission completion, image collection, and survey quality for effective monitoring. DataView DataView is an interactive set of RESTAPIs that facilitate seamless interaction with the data generated by MindView. It allows the integration of additional data sources or services into the workflow of the APIs, enhancing their overall utility. DataView provides various data export options, enabling users to download the collected data in multiple formats such as JSON, CSV, or other geo-referenced file types. This feature is particularly useful for integrating with external data processing and visualization systems. DataView is secured by a user management system overseen by a KeyCloak instance. All API and backend connections are encrypted using SSL certificates, ensuring data security. Furthermore, all collected data is stored within the private section of an AWS VPC, emphasizing data privacy and protection.","title":"Features of MindView"},{"location":"tools/mindview/#use-case-scenario","text":"The MindEarth App is ideally suited for scenarios requiring immediate, real-time photographic surveys of urban environments. A prime example of its application is in analyzing pedestrian traffic patterns within bustling city centers or public spaces, which aids in refining urban planning and maximizing space utilization. To initiate a survey, the first step involves delineating the Area of Interest (AOI) to be surveyed. This includes setting specific parameters such as the time of day, day of the week, and the number of passovers necessary for enhancing the statistical reliability of the data. These parameters are established within the Mission Control platform, either directly by the MindEarth team according to client specifications or by an authorized external administrator. Once these missions are formulated and released, they are instantly accessible to users via the MindEarth App. Upon initial launch of the app, users are prompted to either create a new account or log in with existing credentials. Subsequently, they can explore available missions nearby through a list or map interface. These missions cover various activities, ranging from navigating specific streets or areas to following designated routes with clear start and end points. Each mission is detailed with a title, description, estimated duration, distance, and a reward for successful completion. Additionally, the app notifies users of new missions in their vicinity. Users have the flexibility to select and reserve one or more missions based on their interest. When they approach the mission's starting point, they can begin the survey by pressing the 'Start' button and then proceed as per the instructions displayed on their smartphone, including route guidance and location tracking. After completing the designated tasks and reaching the end point of the mission, users can finalize the survey. Post-mission, the app presents an upload page where users can review and upload the captured images to MindEarth\u2019s secure servers. The app's reward section displays the user's accumulated credits for completed tasks. In adherence to GDPR regulations, the app employs advanced machine learning algorithms to anonymize any personal data in the images, both on the local device and subsequently in a secure AWS environment managed by MindEarth. These anonymized images contribute to aggregated data layers focusing on specific aspects, such as pedestrian flow. The image processing is performed on MindEarth's cloud backend, remaining transparent to the user. Finally, the aggregated data and the data layers are made available to clients through DataView, a dedicated REST API that presents the database content according to a predefined schema. Users can access and interact with these APIs, which require no installation, through a public swagger interface for interactive development.","title":"Use Case Scenario"},{"location":"tools/mindview/integration/","text":"MindView Usage & Integration Guide for Third-Party Services MindEarth App (GREENGAGE Edition) If you're interested in participating in the MindView project by using the MindEarth App, here's a detailed guide to help you get started: Step 1. Downloading and Installing the App Download the app : Download the MindEarth App on the Google Play Store, or if it\u2019s not yet available there, download the APK file from the link provided in the resources section. download app Install the App : Install the app on your Android smartphone. If you're installing via an APK file, make sure to allow installation from unknown sources in your phone's settings. Step 2. Setting Up Your Account Launch the App : Open the MindEarth App on your device. Please note that for the app to function correctly, you need to give it permission to track your precise location, access your camera, microphone, and storage. Register or Log In : Create a new account if you're a first-time user, providing necessary information including payment details to receive compensation for completed missions. If you're a returning user, simply log in with your credentials. Step 3. Exploring Available Missions Browse Missions : Once logged in, you can browse through the list of available missions. These missions are typically commissioned by third parties for research, urban planning, or commercial purposes. View Mission Details : For each mission, you can view detailed information such as the location, the task involved, duration, the device required, deadline, and the compensation offered. Step 4. Selecting and Carry out a Mission Choose a Mission : Select a mission that interests you and fits your availability. Carefully read the mission instructions, including any specific routes to follow or objects to photograph. Start the Mission : When you're ready and at the mission's start location, begin the mission by tapping the 'Start' button. This action automatically activates your camera, and it starts taking pictures as per the mission's requirements. Complete the Tasks : Follow the app's instructions closely, ensuring that all necessary photos are taken as per the mission guidelines. Step 5. Uploading Data and Getting Rewards Upload Your Data : After completing the mission, follow the prompts to upload the images collected to the MindEarth servers. Receive Compensation : Based on the mission's guidelines and your performance, you will be compensated. This could be in monetary form or other rewards as specified in the mission details. Compensation for the mission will be processed on monthly bases based on the payment information provided during registration. Step 6. Review and Repeat Track Your Performance and Progress : You can review your completed missions and rewards in the app and track your performance statistics in terms of km walked, missions completed, accuracy and more. Participate in More Missions : Feel free to engage in more missions as they become available. Additional Tips Stay Charged : Ensure your device is sufficiently charged, especially for longer missions. Follow Guidelines : Always adhere to the guidelines for each mission to ensure the quality of data and secure your reward. Privacy Compliance : Be mindful of privacy regulations, especially when photographing in public spaces. Do not deviate from the path associated with your mission. Mission Control Mission Control serves as the back-end component of MindView, designed for the generation and monitoring of mapping campaigns and missions. The platform is typically managed directly by authorized MindEarth team members, or by authorized third parties, each granted access through a secure Keycloak authentication method, functioning as the operational hub of all active campaigns. If you want to use Mission Control, here's a detailed guide to help you get started: Step 1: Accessing Mission Control Receive access credentials : You will need authorized credentials to log in, so ensure you have them beforehand. If you do not have access, please contact [support@mindearth.ch] (support@mindearth.ch) Log In : Access Mission Control through the platform link and using the login credentials provided. Step 2. Setting Up a New Mapping Campaign Create a New Campaign : Start by creating a new mapping campaign. This involves specifying the overarching goals and objectives of the campaign. Define Area of Interest (AOI) : Use the tools available in Mission Control to define the geographic boundaries of your campaign's Area of Interest. Step 3: Designing Missions Creating missions : Within a campaign, you can add individual missions. For each mission, provide specific details such as: A title of the mission A description of the tasks to be performed The path to be followed by the users, defined in a GeoJson format. Time constraints for the mission, including start and end dates and times. Step 4. Setting Rewards and Effort Estimations Effort Estimation : Time and distance effort associated with the mission is automatically calculated and displayed based on the provided path. Reward setting : A monetary reward can be manually assigned to the mission. Step 5. Publishing Missions Publish campaigns and missions : Once you have finalized campaigns and missions with all necessary details, publish them to make them available to MindEarth App users. Step 6. Monitoring and Managing Missions Track Progress : Use Mission Control\u2019s dashboard to monitor the progress of each mission in real-time. Manage Data Collection : Ensure that the data collected meets the quality standards and requirements of the campaign. Step 7. Confirm Mission Data Review and Analysis : Review data collected from completed missions and, if quality is satisfactory to the campaign goals, confirm it to send if for AI processing. Dataview REST API DataView is an integral part of the MindView system, offering a suite of interactive REST APIs that enable interaction with data collected via the MindEarth App and processed by MindEarth. Note: DataView is still under development and no final link to the resource nor a final outline of the data endpoints is available. Authentication KeyCloak Authentication : Secure access control for using DataView APIs. SSL Encryption : All API interactions are SSL encrypted for data security during transmission. Get List of Campaigns Description : Retrieve a list of available campaigns Endpoint : https://greengage-mindview.mindearth.ai/list/campaign Method : GET Header : Bearer Output Example : [ { \"id\": 1, \"campaignName\": \"Test 1\", \"features\": [ { \"id\": 6, \"type\": [1, 2] }, { \"id\": 2, \"type\": [2] } ] }, { \"id\": 2, \"campaignName\": \"Test 2\" } ] Accessing Geospatial Layers Description : Within the selected campaign, define the features of interest and other applicable parameters (e.g., results spatial aggregation methods). Endpoint : https://greengage-mindview.mindearth.ai/results/ / / Method : GET Header : Bearer Features ID (Example of typical socio-demographic features extractable through the GREENGAGE project, as discussed with Pilot cities): 9 \u2192 Footfall 10 \u2192 Gender 11 \u2192 Age \u2026 Geometry Type : Examples of different geometry types: 1 \u2192 Point 2 \u2192 Line 3 \u2192 Raster Grid 4 \u2192 Vector Polygon Data Export DataView provides options to export data in formats like JSON, CSV, etc., facilitating easy integration with external data processing tools. Single Sign On (SSO) Service To facilitate a centralized user management system, MindEarth App utilizes the Keycloak system provided by Deusto as our identity and access management solution. Please refer to the relevant documentation available at this link . Integrating MindView with the Greengage App To streamline user interactions and mission management between different crowd-based data collection tools, a dedicated integration protocol of the MindEarth App and the GREENGAGE App is implemented. The implementation of a dedicated protocol ensures availability and visibility of MindEarth\u2019s missions across both apps, with seamless redirection for mission execution. Please refer to the relevant documentation available at this link . Testing You can find here some sample output data We can give you early access to the APK and create sample missions around your location if you want to test the app and there are no active missions in your vicinity. Support and Contact If you have any questions or require technical support, do not hesitate to contact our support team at support@mindearth.ch .","title":"MindView Usage &amp; Integration Guide for Third-Party Services"},{"location":"tools/mindview/integration/#mindview-usage-integration-guide-for-third-party-services","text":"","title":"MindView Usage &amp; Integration Guide for Third-Party Services"},{"location":"tools/mindview/integration/#mindearth-app-greengage-edition","text":"If you're interested in participating in the MindView project by using the MindEarth App, here's a detailed guide to help you get started:","title":"MindEarth App (GREENGAGE Edition)"},{"location":"tools/mindview/integration/#step-1-downloading-and-installing-the-app","text":"Download the app : Download the MindEarth App on the Google Play Store, or if it\u2019s not yet available there, download the APK file from the link provided in the resources section. download app Install the App : Install the app on your Android smartphone. If you're installing via an APK file, make sure to allow installation from unknown sources in your phone's settings.","title":"Step 1. Downloading and Installing the App"},{"location":"tools/mindview/integration/#step-2-setting-up-your-account","text":"Launch the App : Open the MindEarth App on your device. Please note that for the app to function correctly, you need to give it permission to track your precise location, access your camera, microphone, and storage. Register or Log In : Create a new account if you're a first-time user, providing necessary information including payment details to receive compensation for completed missions. If you're a returning user, simply log in with your credentials.","title":"Step 2. Setting Up Your Account"},{"location":"tools/mindview/integration/#step-3-exploring-available-missions","text":"Browse Missions : Once logged in, you can browse through the list of available missions. These missions are typically commissioned by third parties for research, urban planning, or commercial purposes. View Mission Details : For each mission, you can view detailed information such as the location, the task involved, duration, the device required, deadline, and the compensation offered.","title":"Step 3. Exploring Available Missions"},{"location":"tools/mindview/integration/#step-4-selecting-and-carry-out-a-mission","text":"Choose a Mission : Select a mission that interests you and fits your availability. Carefully read the mission instructions, including any specific routes to follow or objects to photograph. Start the Mission : When you're ready and at the mission's start location, begin the mission by tapping the 'Start' button. This action automatically activates your camera, and it starts taking pictures as per the mission's requirements. Complete the Tasks : Follow the app's instructions closely, ensuring that all necessary photos are taken as per the mission guidelines.","title":"Step 4. Selecting and Carry out a Mission"},{"location":"tools/mindview/integration/#step-5-uploading-data-and-getting-rewards","text":"Upload Your Data : After completing the mission, follow the prompts to upload the images collected to the MindEarth servers. Receive Compensation : Based on the mission's guidelines and your performance, you will be compensated. This could be in monetary form or other rewards as specified in the mission details. Compensation for the mission will be processed on monthly bases based on the payment information provided during registration.","title":"Step 5. Uploading Data and Getting Rewards"},{"location":"tools/mindview/integration/#step-6-review-and-repeat","text":"Track Your Performance and Progress : You can review your completed missions and rewards in the app and track your performance statistics in terms of km walked, missions completed, accuracy and more. Participate in More Missions : Feel free to engage in more missions as they become available.","title":"Step 6. Review and Repeat"},{"location":"tools/mindview/integration/#additional-tips","text":"Stay Charged : Ensure your device is sufficiently charged, especially for longer missions. Follow Guidelines : Always adhere to the guidelines for each mission to ensure the quality of data and secure your reward. Privacy Compliance : Be mindful of privacy regulations, especially when photographing in public spaces. Do not deviate from the path associated with your mission.","title":"Additional Tips"},{"location":"tools/mindview/integration/#mission-control","text":"Mission Control serves as the back-end component of MindView, designed for the generation and monitoring of mapping campaigns and missions. The platform is typically managed directly by authorized MindEarth team members, or by authorized third parties, each granted access through a secure Keycloak authentication method, functioning as the operational hub of all active campaigns. If you want to use Mission Control, here's a detailed guide to help you get started:","title":"Mission Control"},{"location":"tools/mindview/integration/#step-1-accessing-mission-control","text":"Receive access credentials : You will need authorized credentials to log in, so ensure you have them beforehand. If you do not have access, please contact [support@mindearth.ch] (support@mindearth.ch) Log In : Access Mission Control through the platform link and using the login credentials provided.","title":"Step 1: Accessing Mission Control"},{"location":"tools/mindview/integration/#step-2-setting-up-a-new-mapping-campaign","text":"Create a New Campaign : Start by creating a new mapping campaign. This involves specifying the overarching goals and objectives of the campaign. Define Area of Interest (AOI) : Use the tools available in Mission Control to define the geographic boundaries of your campaign's Area of Interest.","title":"Step 2. Setting Up a New Mapping Campaign"},{"location":"tools/mindview/integration/#step-3-designing-missions","text":"Creating missions : Within a campaign, you can add individual missions. For each mission, provide specific details such as: A title of the mission A description of the tasks to be performed The path to be followed by the users, defined in a GeoJson format. Time constraints for the mission, including start and end dates and times.","title":"Step 3: Designing Missions"},{"location":"tools/mindview/integration/#step-4-setting-rewards-and-effort-estimations","text":"Effort Estimation : Time and distance effort associated with the mission is automatically calculated and displayed based on the provided path. Reward setting : A monetary reward can be manually assigned to the mission.","title":"Step 4. Setting Rewards and Effort Estimations"},{"location":"tools/mindview/integration/#step-5-publishing-missions","text":"Publish campaigns and missions : Once you have finalized campaigns and missions with all necessary details, publish them to make them available to MindEarth App users.","title":"Step 5. Publishing Missions"},{"location":"tools/mindview/integration/#step-6-monitoring-and-managing-missions","text":"Track Progress : Use Mission Control\u2019s dashboard to monitor the progress of each mission in real-time. Manage Data Collection : Ensure that the data collected meets the quality standards and requirements of the campaign.","title":"Step 6. Monitoring and Managing Missions"},{"location":"tools/mindview/integration/#step-7-confirm-mission-data","text":"Review and Analysis : Review data collected from completed missions and, if quality is satisfactory to the campaign goals, confirm it to send if for AI processing.","title":"Step 7. Confirm Mission Data"},{"location":"tools/mindview/integration/#dataview-rest-api","text":"DataView is an integral part of the MindView system, offering a suite of interactive REST APIs that enable interaction with data collected via the MindEarth App and processed by MindEarth. Note: DataView is still under development and no final link to the resource nor a final outline of the data endpoints is available.","title":"Dataview REST API"},{"location":"tools/mindview/integration/#authentication","text":"KeyCloak Authentication : Secure access control for using DataView APIs. SSL Encryption : All API interactions are SSL encrypted for data security during transmission.","title":"Authentication"},{"location":"tools/mindview/integration/#get-list-of-campaigns","text":"Description : Retrieve a list of available campaigns Endpoint : https://greengage-mindview.mindearth.ai/list/campaign Method : GET Header : Bearer Output Example : [ { \"id\": 1, \"campaignName\": \"Test 1\", \"features\": [ { \"id\": 6, \"type\": [1, 2] }, { \"id\": 2, \"type\": [2] } ] }, { \"id\": 2, \"campaignName\": \"Test 2\" } ]","title":"Get List of Campaigns"},{"location":"tools/mindview/integration/#accessing-geospatial-layers","text":"Description : Within the selected campaign, define the features of interest and other applicable parameters (e.g., results spatial aggregation methods). Endpoint : https://greengage-mindview.mindearth.ai/results/ / / Method : GET Header : Bearer Features ID (Example of typical socio-demographic features extractable through the GREENGAGE project, as discussed with Pilot cities): 9 \u2192 Footfall 10 \u2192 Gender 11 \u2192 Age \u2026 Geometry Type : Examples of different geometry types: 1 \u2192 Point 2 \u2192 Line 3 \u2192 Raster Grid 4 \u2192 Vector Polygon","title":"Accessing Geospatial Layers"},{"location":"tools/mindview/integration/#data-export","text":"DataView provides options to export data in formats like JSON, CSV, etc., facilitating easy integration with external data processing tools.","title":"Data Export"},{"location":"tools/mindview/integration/#single-sign-on-sso-service","text":"To facilitate a centralized user management system, MindEarth App utilizes the Keycloak system provided by Deusto as our identity and access management solution. Please refer to the relevant documentation available at this link .","title":"Single Sign On (SSO) Service"},{"location":"tools/mindview/integration/#integrating-mindview-with-the-greengage-app","text":"To streamline user interactions and mission management between different crowd-based data collection tools, a dedicated integration protocol of the MindEarth App and the GREENGAGE App is implemented. The implementation of a dedicated protocol ensures availability and visibility of MindEarth\u2019s missions across both apps, with seamless redirection for mission execution. Please refer to the relevant documentation available at this link .","title":"Integrating MindView with the Greengage App"},{"location":"tools/mindview/integration/#testing","text":"You can find here some sample output data We can give you early access to the APK and create sample missions around your location if you want to test the app and there are no active missions in your vicinity.","title":"Testing"},{"location":"tools/mindview/integration/#support-and-contact","text":"If you have any questions or require technical support, do not hesitate to contact our support team at support@mindearth.ch .","title":"Support and Contact"},{"location":"tools/mindview/examples/","text":"MindView <> Greengage App Integration To streamline user interactions and mission management between different crowd-based data collection tools, a dedicated integration protocol of the MindEarth App and the GREENGAGE App is implemented. The implementation of a dedicated protocol ensures availability and visibility of MindEarth\u2019s missions across both apps, with seamless redirection for mission execution. This means that any mission available in the MindEarth App will also be visible in the GREENGAGE App and that users of the GREENGAGE App will be able to select a MindEarth App mission and be redirected to the MindEarth app to complete the booking and mission execution and upload process, as well as to know, in real time the status, duration, reward and deadline of MindEarth App\u2019s missions. This involves using two main systems based on webhooks and deeplinks. Webhook A webhook functionality for actions like booking, canceling, and completing missions. These webhook interactions use the base path https://mindearth.greengage.dev/ (example url). Mission Add Interaction: Action: Adding new Mission on MindEarth App URL: https://mindearth.greengage.dev/mission/add Method: POST Header: Bearer payload: Mission Booking Interaction: Action: Booking a Mission on MindEarth App URL: https://mindearth.greengage.dev/mission/ /book Method: PUT Header: Bearer Mission Cancellation Interaction: Action: Canceling a Mission Reservation on MindEarth App URL: https://mindearth.greengage.dev/mission/ /book/cancel Method: PUT Header: Bearer Mission Completion Interaction: Action: Completing a Mission on MindEarth App URL: https://mindearth.greengage.dev/mission/ /complete Method: PUT Header: Bearer Back-Office Integration: Action: Adding New Missions URL: https://mindearth.greengage.dev/mission Method: POST Header: Bearer Body: Details provided in the relevant JSON structure documentation. Deep links Deeplinking is planned to facilitate user transition between GREENGAGE and MindEarth apps. Deep links in each mission would enable seamless transition from the GREENGAGE App and engagement with the MindEarth App. A query URL will provide a unique webhook link for each mission, eliminating the need for an Auth-Header due to the signed nature of the request. Example of Deep Link: mindearth://mission/23?callback=https%3A%2F%2Fmindearth.greengage.dev%2Fmission%2F23%3Fsignature%3Df834ed8570e05de6c50ad10bd6abcf71e9867fcb14bdf2670b4bf572ce346f3b Testing You can find here some [sample output data] (assets/sample_data) We can give you [early access to the APK] () and create sample missions around your location if you want to test the app and there are no active missions in your vicinity Support and Contact If you have any questions or require technical support, do not hesitate to contact our support team at [support@mindearth.ch] (support@mindearth.ch).","title":"MindView &lt;&gt; Greengage App Integration"},{"location":"tools/mindview/examples/#mindview-greengage-app-integration","text":"To streamline user interactions and mission management between different crowd-based data collection tools, a dedicated integration protocol of the MindEarth App and the GREENGAGE App is implemented. The implementation of a dedicated protocol ensures availability and visibility of MindEarth\u2019s missions across both apps, with seamless redirection for mission execution. This means that any mission available in the MindEarth App will also be visible in the GREENGAGE App and that users of the GREENGAGE App will be able to select a MindEarth App mission and be redirected to the MindEarth app to complete the booking and mission execution and upload process, as well as to know, in real time the status, duration, reward and deadline of MindEarth App\u2019s missions. This involves using two main systems based on webhooks and deeplinks.","title":"MindView &lt;&gt; Greengage App Integration"},{"location":"tools/mindview/examples/#webhook","text":"A webhook functionality for actions like booking, canceling, and completing missions. These webhook interactions use the base path https://mindearth.greengage.dev/ (example url). Mission Add Interaction: Action: Adding new Mission on MindEarth App URL: https://mindearth.greengage.dev/mission/add Method: POST Header: Bearer payload: Mission Booking Interaction: Action: Booking a Mission on MindEarth App URL: https://mindearth.greengage.dev/mission/ /book Method: PUT Header: Bearer Mission Cancellation Interaction: Action: Canceling a Mission Reservation on MindEarth App URL: https://mindearth.greengage.dev/mission/ /book/cancel Method: PUT Header: Bearer Mission Completion Interaction: Action: Completing a Mission on MindEarth App URL: https://mindearth.greengage.dev/mission/ /complete Method: PUT Header: Bearer Back-Office Integration: Action: Adding New Missions URL: https://mindearth.greengage.dev/mission Method: POST Header: Bearer Body: Details provided in the relevant JSON structure documentation.","title":"Webhook"},{"location":"tools/mindview/examples/#deep-links","text":"Deeplinking is planned to facilitate user transition between GREENGAGE and MindEarth apps. Deep links in each mission would enable seamless transition from the GREENGAGE App and engagement with the MindEarth App. A query URL will provide a unique webhook link for each mission, eliminating the need for an Auth-Header due to the signed nature of the request. Example of Deep Link: mindearth://mission/23?callback=https%3A%2F%2Fmindearth.greengage.dev%2Fmission%2F23%3Fsignature%3Df834ed8570e05de6c50ad10bd6abcf71e9867fcb14bdf2670b4bf572ce346f3b","title":"Deep links"},{"location":"tools/mindview/examples/#testing","text":"You can find here some [sample output data] (assets/sample_data) We can give you [early access to the APK] () and create sample missions around your location if you want to test the app and there are no active missions in your vicinity","title":"Testing"},{"location":"tools/mindview/examples/#support-and-contact","text":"If you have any questions or require technical support, do not hesitate to contact our support team at [support@mindearth.ch] (support@mindearth.ch).","title":"Support and Contact"},{"location":"tools/mode/","text":"MODE Introduction MODE software technology uses smartphones to automatically track the distances travelled and transport modes used, thus enabling software developers, system integrators and transport operators to design innovative mobility services. MODE provides the missing link for the development of innovative mobility services: automatic, reliable recognition of the means of transport used, and travelled routes using smartphones. Features of MODE MODE is a trip recording and mode detection framework. It consists of two main software components: 1. A data recording module for iOS and Android Apps 2. A backend data analysis module The above figure shows a high-level overview and demonstrates how to embed the MODE components into a system (AIT components are highlighted in blue, other components must be provided by a partner). Client-side tracking and data recording functionality can be added to an existing App by including the libmode library. Once the library is configured and activated (see Frontend ), it tracks user activity and records relevant movement data. The library transmits completed trips to a backend server using REST calls. On the server side, the transmitted trip data is processed using the AIT JMDCA library. The JMDCA library is written in Java and provides core data structures, processing routines and analysis algorithms which allow application programmers to easily analyse trips and perform mode detection on the recorded data (JMDCA API, see section Backend ). To deliver high-quality results, the JMDCA mode detection algorithm requires access to a third-party public transit routing engine (such as HERE or Google routing). The result of using MODE are trips. These trips can include multiple segments (\"activities\") which make up a trip. E.g. if a user walks to his or her car, then drives to a transit station and then takes the train to reach the destination it would yield three activities, each with: Start and end times The travel mode (e.g. bike, car, public transport, walking, etc.) The probability of the \"correctness\" (e.g. it could be \"LOW; Reason: Bad GPS signal\") Use Case Scenario The main usage of MODE is to be integrated in smartphone apps to track user travel behavior. E.g. it was used in apps which incentivize \"green\" transport modes where you start the app and if it detects that you did the trip via bicycle or public transport you could earn \"tokens\" which could be spent on culture events.","title":"MODE"},{"location":"tools/mode/#mode","text":"","title":"MODE"},{"location":"tools/mode/#introduction","text":"MODE software technology uses smartphones to automatically track the distances travelled and transport modes used, thus enabling software developers, system integrators and transport operators to design innovative mobility services. MODE provides the missing link for the development of innovative mobility services: automatic, reliable recognition of the means of transport used, and travelled routes using smartphones.","title":"Introduction"},{"location":"tools/mode/#features-of-mode","text":"MODE is a trip recording and mode detection framework. It consists of two main software components: 1. A data recording module for iOS and Android Apps 2. A backend data analysis module The above figure shows a high-level overview and demonstrates how to embed the MODE components into a system (AIT components are highlighted in blue, other components must be provided by a partner). Client-side tracking and data recording functionality can be added to an existing App by including the libmode library. Once the library is configured and activated (see Frontend ), it tracks user activity and records relevant movement data. The library transmits completed trips to a backend server using REST calls. On the server side, the transmitted trip data is processed using the AIT JMDCA library. The JMDCA library is written in Java and provides core data structures, processing routines and analysis algorithms which allow application programmers to easily analyse trips and perform mode detection on the recorded data (JMDCA API, see section Backend ). To deliver high-quality results, the JMDCA mode detection algorithm requires access to a third-party public transit routing engine (such as HERE or Google routing). The result of using MODE are trips. These trips can include multiple segments (\"activities\") which make up a trip. E.g. if a user walks to his or her car, then drives to a transit station and then takes the train to reach the destination it would yield three activities, each with: Start and end times The travel mode (e.g. bike, car, public transport, walking, etc.) The probability of the \"correctness\" (e.g. it could be \"LOW; Reason: Bad GPS signal\")","title":"Features of MODE"},{"location":"tools/mode/#use-case-scenario","text":"The main usage of MODE is to be integrated in smartphone apps to track user travel behavior. E.g. it was used in apps which incentivize \"green\" transport modes where you start the app and if it detects that you did the trip via bicycle or public transport you could earn \"tokens\" which could be spent on culture events.","title":"Use Case Scenario"},{"location":"tools/mode/integration/","text":"MODE Integration Guide for Third-Party Services Since MODE is a distributed system with multiple components, it can (or must) be implemented at various levels, mostly in the frontend (Android, iOS apps) and the backend (server) which processes the trips. Frontend The MODE smartphone libraries ( libmode for Android and iOS ) must be integrated in a host app which takes care about user authentication, and starting/stopping the MODE service. While MODE can run continuously in the background, it can adversely affect battery consumption (especially on iOS devices). It should therefore only be started when the user agrees to track a trip. When MODE is running, it will detect when activities start or end (e.g. due to GPS signals, or when it detects, that the phone is not moving anymore). Once an activity end, the binary data of the trip is sent to a REST server in the background, which does the processing. Backend The backend is provided as a Docker image which opens the rest service on port 8080. Its sole purpose is to listen to the binary data from the frontend, process the data, and return JSON output of trips. If advanced logic is needed, like storing the returned trips in a database, this must be done by some kind of middleware which details are yet to be determined, depending on the concrete use cases of MODE within the project. Testing Testing the setup with real data can be cumbersome, as someone would need to walk/drive around to see the data pushed from the frontend to the backend. Therefore we provide some sample data which can be sent to the backend to be processed using a simple test script . Note that the backend server requires an API key which must be changed in the test script and is provided on demand . Output If the front end and backend works correctly, the backend will spit out the resulting trips as a JSON file : { \"message\": \"success\", \"activities\": [ { \"begin\": \"2020-07-15T13:20:41.059Z\", \"end\": \"2020-07-15T13:45:37.059Z\", \"mode\": \"WALK\", \"modeDetails\": \" | Sensor mode Walk | GPSQuality(AVERAGE : time gap)\", \"distanceMeters\": 1253.2470899042082, \"route\": \"ue`eHkddcB??????????rCrAj@Vj@Vh@Th@Vp@Xj@Xp@Xh@Tl@Vl@XJDp@Xn@Xp@Vp@XJDl@Tn@Rp@RRB@@D@?@?@???@???@@????????@???A?????????????????????????????????@????????????????????????Nh@??@E??@??@@B@B@B@@ACAAAC?AM]EICGCE?A???@?@BFBH@DP`A@F?D???B????A@A???EA??A??A????A@??????A????AA????CAI@??A@????A????????@?????B@B?D?B?F@D?B?H@H?F?D@B?@?@????C?CAC?CAGAGCG?CAA?AAA@??ABABCBABAB?@?@?BBBBD@D@F?DCDCBCBEBEBC@EBEBC@EBEBC@EBC@CBCBCBCBCBEBC@EBE@C@C@ABCBCBEBCBE@E@G@G@EBEBEBGBEBG@GBG@G?G?E?ECCEAEAEAECEECECEEECEECECCCCECCECCACACAEACAECECEEECEECCEEECECECECECEEECECECEAECEAECGCGCEEECE?C@CBE@C?CCECC?E?CBCFADCFCDABC@A@A?@A????A?A??@A???@A??@A??????@??A?@@?@??????A?A?AA???????????A?@???????@??@???@A??@?A???A@A?A??????@A??A??@A@???@???@?B????@??A?AA????A?????????A???@?????@?@?@?@?@A??@??????A???@@?A?A???????????A???A?A??????????@??@???A??@A???A@?A??@?@A@?@?@????@????A?????A?A?A?????A??????A????DA??????A??@????A???@??@?????@??A?????????@??????A?????CAAA?AAAA@???@A?????????@@@@?@????@@???????????????A?????????A???AA????????A?????A???@@@??@A?????A?@?????@??A?A@A@A@????@I@????B?@?A@??@?@ABCBA@A???\" }, { \"begin\": \"2020-07-15T13:45:38.059Z\", \"end\": \"2020-07-15T13:51:20.059Z\", \"mode\": \"BUS\", \"modeDetails\": \"Line 69A (Hauptbahnhof - Simmering) (BUS) from Am Kanal to Arsenalsteg | Sensor mode Bus routed to Bus | GPSQuality(GOOD)\", \"distanceMeters\": 2143.843162687046, \"route\": \"ch_eHk_dcB????yA`ByAxA??gBbB??}AlA??{BrA??cBp@uBn@??cBZ??s@`C????????????????????AnD??ElC??EjCGlC??EtB??EtBGtB??IxD??KpC????????????????????A`D??IdE????c@xC??_AlB??CzC??m@lC??_AbB????????????qA`C????wBxD??_BfC??wAzB????}A~B????????????????iAbB??gAdB??gB`C??q@~@s@~@s@~@KW\" }, { \"begin\": \"2020-07-15T13:51:29.059Z\", \"end\": \"2020-07-15T14:32:45.059Z\", \"mode\": \"WALK\", \"modeDetails\": \" | Sensor mode Walk | GPSQuality(BAD : large time gap)\", \"distanceMeters\": 1435.4269300101091, \"route\": \"mhaeHq{_cBE^@?AAACEAEAE?E?C?CACCCAC?E?CACCAG?CBEBABA@CBCBAB?@D?@?BCFA@C@CBAB?B@B@@@A@C@C@A@@@B?DABAB?B?@@@??@?BABA@?@?@B?D?D@D@F@D@D@F@F@D@FBD@F@FBH@FBHBFBFBFBHBF@HBF@DBF@FBDBDBDBFBD@FBD@D?B?@???????A??????????????@@?B@DBDBF@DBFBDBFBD@FBDBFBFBDBDBDBFBDBFBFBDBFBD@F@FBFBDB@DAD?DAB@D@B@@@A@CBC@CB?BDBFBF@FBD@BDBB@F@F@H?H@H@H@HBHBF@DBF@DBFBBDDDBDBDBDDBDBFBDDFBD@FBFBD@FBFBDBFBDDDDFDFBDDDBDBDBBBBBDBBBBD@B@DABABEBCBCBE@G@G@G@GBE@EBG@E?G@G?E@G?E@GBE@IBGBGDGBGBEBG@E@E@G@E@G@G@G@E@G@G?E@G?G@EBEBEBEBAD?D@FBDBBDDD@F@B@D?D@B?B@B@B??DJ?@@@?@B@LLDBFB@@@?@@@???@@@?@@@???@@@???@@@?@?@@??@@@?@?@@??@?@@@@DBB@NZ?@?@@@?@@??@?@?@@??@??@B??????????????AA???????A????EM??AC????AC?A???A????CG?AAC????????CI?AAA?A???????A??A????A??AE?????AA????A?????AAA?AAC?????A??????AAAE???A????M_@AACEAA??A??AAA??AAA?AA??MKAACCCACCECCCCACCEECCEAE?E@EDEDEFCDAHCHAHAHAH?FAF?DADCBADADAF?FAHAFAFAFAF?H?FAF?FAFCFCDCFCDCDCDEDCBEDCBCDAFCF?FAF@F?FAD?DADAFADCBADCDCDCDAFCDAFADAFCDCDCDCBADADADCDADCDADCFAH?HAF?HAF?F?H?F@FAD?FCFADAHAFAFAFAHAFADCFAF?D?F?D?D?FADADAFAFCFCHAFCFCHCDCFCDEBEDEBEDC@E?EACCEECACCCACCCCCCACCCCCCECCACCCACACA@A@A@?@?@?@?@?????????A??A?A?A?A?@@@?????????@?????@A@ABU??EH?@A@?@ABEJ?@ABA@CHW|@K^ADAH?@?????C?????A???@?????A?E?A?C?????J?@????A?@????????@???A????????????\" } ] } The format uses the encoded polyline algorithm for encoding the polylines of the trips ( sample code ). We will, however, for the project also try to output GeoJSON files directly, which might be easier to consume. Support and Contact For any questions, contact Martin Stubenschrott directly at martin.stubenschrott@ait.ac.at Important Notes You might get an API key for third-party routing services (e.g. Google Router). Make sure to keep that confident and only use it for testing and sparsingly. Overuse of the API key may result in high fees, as only about 20.000 requests/per month are free! For production, we must use a different key being used for billing!","title":"MODE Integration Guide for Third-Party Services"},{"location":"tools/mode/integration/#mode-integration-guide-for-third-party-services","text":"Since MODE is a distributed system with multiple components, it can (or must) be implemented at various levels, mostly in the frontend (Android, iOS apps) and the backend (server) which processes the trips.","title":"MODE Integration Guide for Third-Party Services"},{"location":"tools/mode/integration/#frontend","text":"The MODE smartphone libraries ( libmode for Android and iOS ) must be integrated in a host app which takes care about user authentication, and starting/stopping the MODE service. While MODE can run continuously in the background, it can adversely affect battery consumption (especially on iOS devices). It should therefore only be started when the user agrees to track a trip. When MODE is running, it will detect when activities start or end (e.g. due to GPS signals, or when it detects, that the phone is not moving anymore). Once an activity end, the binary data of the trip is sent to a REST server in the background, which does the processing.","title":"Frontend"},{"location":"tools/mode/integration/#backend","text":"The backend is provided as a Docker image which opens the rest service on port 8080. Its sole purpose is to listen to the binary data from the frontend, process the data, and return JSON output of trips. If advanced logic is needed, like storing the returned trips in a database, this must be done by some kind of middleware which details are yet to be determined, depending on the concrete use cases of MODE within the project.","title":"Backend"},{"location":"tools/mode/integration/#testing","text":"Testing the setup with real data can be cumbersome, as someone would need to walk/drive around to see the data pushed from the frontend to the backend. Therefore we provide some sample data which can be sent to the backend to be processed using a simple test script . Note that the backend server requires an API key which must be changed in the test script and is provided on demand .","title":"Testing"},{"location":"tools/mode/integration/#output","text":"If the front end and backend works correctly, the backend will spit out the resulting trips as a JSON file : { \"message\": \"success\", \"activities\": [ { \"begin\": \"2020-07-15T13:20:41.059Z\", \"end\": \"2020-07-15T13:45:37.059Z\", \"mode\": \"WALK\", \"modeDetails\": \" | Sensor mode Walk | GPSQuality(AVERAGE : time gap)\", \"distanceMeters\": 1253.2470899042082, \"route\": \"ue`eHkddcB??????????rCrAj@Vj@Vh@Th@Vp@Xj@Xp@Xh@Tl@Vl@XJDp@Xn@Xp@Vp@XJDl@Tn@Rp@RRB@@D@?@?@???@???@@????????@???A?????????????????????????????????@????????????????????????Nh@??@E??@??@@B@B@B@@ACAAAC?AM]EICGCE?A???@?@BFBH@DP`A@F?D???B????A@A???EA??A??A????A@??????A????AA????CAI@??A@????A????????@?????B@B?D?B?F@D?B?H@H?F?D@B?@?@????C?CAC?CAGAGCG?CAA?AAA@??ABABCBABAB?@?@?BBBBD@D@F?DCDCBCBEBEBC@EBEBC@EBEBC@EBC@CBCBCBCBCBEBC@EBE@C@C@ABCBCBEBCBE@E@G@G@EBEBEBGBEBG@GBG@G?G?E?ECCEAEAEAECEECECEEECEECECCCCECCECCACACAEACAECECEEECEECCEEECECECECECEEECECECEAECEAECGCGCEEECE?C@CBE@C?CCECC?E?CBCFADCFCDABC@A@A?@A????A?A??@A???@A??@A??????@??A?@@?@??????A?A?AA???????????A?@???????@??@???@A??@?A???A@A?A??????@A??A??@A@???@???@?B????@??A?AA????A?????????A???@?????@?@?@?@?@A??@??????A???@@?A?A???????????A???A?A??????????@??@???A??@A???A@?A??@?@A@?@?@????@????A?????A?A?A?????A??????A????DA??????A??@????A???@??@?????@??A?????????@??????A?????CAAA?AAAA@???@A?????????@@@@?@????@@???????????????A?????????A???AA????????A?????A???@@@??@A?????A?@?????@??A?A@A@A@????@I@????B?@?A@??@?@ABCBA@A???\" }, { \"begin\": \"2020-07-15T13:45:38.059Z\", \"end\": \"2020-07-15T13:51:20.059Z\", \"mode\": \"BUS\", \"modeDetails\": \"Line 69A (Hauptbahnhof - Simmering) (BUS) from Am Kanal to Arsenalsteg | Sensor mode Bus routed to Bus | GPSQuality(GOOD)\", \"distanceMeters\": 2143.843162687046, \"route\": \"ch_eHk_dcB????yA`ByAxA??gBbB??}AlA??{BrA??cBp@uBn@??cBZ??s@`C????????????????????AnD??ElC??EjCGlC??EtB??EtBGtB??IxD??KpC????????????????????A`D??IdE????c@xC??_AlB??CzC??m@lC??_AbB????????????qA`C????wBxD??_BfC??wAzB????}A~B????????????????iAbB??gAdB??gB`C??q@~@s@~@s@~@KW\" }, { \"begin\": \"2020-07-15T13:51:29.059Z\", \"end\": \"2020-07-15T14:32:45.059Z\", \"mode\": \"WALK\", \"modeDetails\": \" | Sensor mode Walk | GPSQuality(BAD : large time gap)\", \"distanceMeters\": 1435.4269300101091, \"route\": \"mhaeHq{_cBE^@?AAACEAEAE?E?C?CACCCAC?E?CACCAG?CBEBABA@CBCBAB?@D?@?BCFA@C@CBAB?B@B@@@A@C@C@A@@@B?DABAB?B?@@@??@?BABA@?@?@B?D?D@D@F@D@D@F@F@D@FBD@F@FBH@FBHBFBFBFBHBF@HBF@DBF@FBDBDBDBFBD@FBD@D?B?@???????A??????????????@@?B@DBDBF@DBFBDBFBD@FBDBFBFBDBDBDBFBDBFBFBDBFBD@F@FBFBDB@DAD?DAB@D@B@@@A@CBC@CB?BDBFBF@FBD@BDBB@F@F@H?H@H@H@HBHBF@DBF@DBFBBDDDBDBDBDDBDBFBDDFBD@FBFBD@FBFBDBFBDDDDFDFBDDDBDBDBBBBBDBBBBD@B@DABABEBCBCBE@G@G@G@GBE@EBG@E?G@G?E@G?E@GBE@IBGBGDGBGBEBG@E@E@G@E@G@G@G@E@G@G?E@G?G@EBEBEBEBAD?D@FBDBBDDD@F@B@D?D@B?B@B@B??DJ?@@@?@B@LLDBFB@@@?@@@???@@@?@@@???@@@???@@@?@?@@??@@@?@?@@??@?@@@@DBB@NZ?@?@@@?@@??@?@?@@??@??@B??????????????AA???????A????EM??AC????AC?A???A????CG?AAC????????CI?AAA?A???????A??A????A??AE?????AA????A?????AAA?AAC?????A??????AAAE???A????M_@AACEAA??A??AAA??AAA?AA??MKAACCCACCECCCCACCEECCEAE?E@EDEDEFCDAHCHAHAHAH?FAF?DADCBADADAF?FAHAFAFAFAF?H?FAF?FAFCFCDCFCDCDCDEDCBEDCBCDAFCF?FAF@F?FAD?DADAFADCBADCDCDCDAFCDAFADAFCDCDCDCBADADADCDADCDADCFAH?HAF?HAF?F?H?F@FAD?FCFADAHAFAFAFAHAFADCFAF?D?F?D?D?FADADAFAFCFCHAFCFCHCDCFCDEBEDEBEDC@E?EACCEECACCCACCCCCCACCCCCCECCACCCACACA@A@A@?@?@?@?@?????????A??A?A?A?A?@@@?????????@?????@A@ABU??EH?@A@?@ABEJ?@ABA@CHW|@K^ADAH?@?????C?????A???@?????A?E?A?C?????J?@????A?@????????@???A????????????\" } ] } The format uses the encoded polyline algorithm for encoding the polylines of the trips ( sample code ). We will, however, for the project also try to output GeoJSON files directly, which might be easier to consume.","title":"Output"},{"location":"tools/mode/integration/#support-and-contact","text":"For any questions, contact Martin Stubenschrott directly at martin.stubenschrott@ait.ac.at","title":"Support and Contact"},{"location":"tools/mode/integration/#important-notes","text":"You might get an API key for third-party routing services (e.g. Google Router). Make sure to keep that confident and only use it for testing and sparsingly. Overuse of the API key may result in high fees, as only about 20.000 requests/per month are free! For production, we must use a different key being used for billing!","title":"Important Notes"},{"location":"tools/mode/examples/android/","text":"MODE Integration Guide for data collection on Android devices The MODE smartphone library ( libmode ) provides a complete, out-of-the-box solution for mobility data collection via smartphones. It is built as an autonomous component that can easily be integrated into third party apps. For the standard use-case (trip data collection) the library can be integrated into existing Apps with little effort: The most recent version of the MODE library will be provided by AIT upon request . You will also be granted Git access to https://gitlab.ait.ac.at/energy/commons/smartsurvey/android/smartsurvey_demo_app/ which hosts a very basic demo app, showing how to use the MODE library. Step 1: Add the library to the App as a build dependency The library can be added to the App by adding a corresponding dependency to the build. The following example shows how to add libmode to the build.gradle of an Android App. We will provide the library with enough meta-data according to the Maven standard. When putting the library into your ~/.m2/repository/ folder, you can add the dependency like this: dependencies { implementation 'at.ac.ait.mode:libmode:2.0.6' } Of course, you can also just copy the libmode.jar file to a libs folder in your project, and add the dependency like this: dependencies { implementation fileTree(dir: 'libs', include: ['*.jar']) } Step 2: Initialize the library and start data collection The library needs to be initialized before it can start recording data: private void initLibMode() { Context ctx = getApplicationContext(); Mode.initialize(ctx); // Optional: allow data uploading via cellular network (otherwise data only gets uploaded when on WiFi) Mode.getConfigurationEntry(Configuration.CONFIG_METERED_UPLOADING_ALLOWED).setBooleanValue(ctx, true); // set Backend Server Mode.setBaseServerURL(\"http://localhost:8080\"); // Set OAUTH2 token manually // This will be sent to the backend service on each request Mode.setAuthorizationToken(\"MY_OAUTH2_TOKEN\"); // Start Mode.startSurvey(); } Note that we need to ask first for permission to access the location. ( ANDROID_PERMISSION_ACCESS_FINE_LOCATION ). Only when this is granted, we should initialize the library as in the example above. The provided demo app shows how to handle this. Support and Contact For any questions, contact Martin Stubenschrott directly at martin.stubenschrott@ait.ac.at","title":"MODE Integration Guide for data collection on Android devices"},{"location":"tools/mode/examples/android/#mode-integration-guide-for-data-collection-on-android-devices","text":"The MODE smartphone library ( libmode ) provides a complete, out-of-the-box solution for mobility data collection via smartphones. It is built as an autonomous component that can easily be integrated into third party apps. For the standard use-case (trip data collection) the library can be integrated into existing Apps with little effort: The most recent version of the MODE library will be provided by AIT upon request . You will also be granted Git access to https://gitlab.ait.ac.at/energy/commons/smartsurvey/android/smartsurvey_demo_app/ which hosts a very basic demo app, showing how to use the MODE library. Step 1: Add the library to the App as a build dependency The library can be added to the App by adding a corresponding dependency to the build. The following example shows how to add libmode to the build.gradle of an Android App. We will provide the library with enough meta-data according to the Maven standard. When putting the library into your ~/.m2/repository/ folder, you can add the dependency like this: dependencies { implementation 'at.ac.ait.mode:libmode:2.0.6' } Of course, you can also just copy the libmode.jar file to a libs folder in your project, and add the dependency like this: dependencies { implementation fileTree(dir: 'libs', include: ['*.jar']) } Step 2: Initialize the library and start data collection The library needs to be initialized before it can start recording data: private void initLibMode() { Context ctx = getApplicationContext(); Mode.initialize(ctx); // Optional: allow data uploading via cellular network (otherwise data only gets uploaded when on WiFi) Mode.getConfigurationEntry(Configuration.CONFIG_METERED_UPLOADING_ALLOWED).setBooleanValue(ctx, true); // set Backend Server Mode.setBaseServerURL(\"http://localhost:8080\"); // Set OAUTH2 token manually // This will be sent to the backend service on each request Mode.setAuthorizationToken(\"MY_OAUTH2_TOKEN\"); // Start Mode.startSurvey(); } Note that we need to ask first for permission to access the location. ( ANDROID_PERMISSION_ACCESS_FINE_LOCATION ). Only when this is granted, we should initialize the library as in the example above. The provided demo app shows how to handle this.","title":"MODE Integration Guide for data collection on Android devices"},{"location":"tools/mode/examples/android/#support-and-contact","text":"For any questions, contact Martin Stubenschrott directly at martin.stubenschrott@ait.ac.at","title":"Support and Contact"},{"location":"tools/mode/examples/backend/","text":"MODE Integration Guide for backend data analysis We provide a basic REST entpoint as a Docker image which listens on an endpoint /analyze . The input is binary movement data gathered from the smartphone apps (see here and here ). Output is a JSON string of the taken activities. Using the image Step 1: Copy exported image to server Use your tool of choice. Step 2: Import image on server docker load -i modesrv-1.0.0.tar.gz Step 3: Create Docker container # docker stop modesrv # docker rm modesrv docker create --name modesrv -p 8080:8080 --restart=unless-stopped ait/modesrv:1.0.0 Step 4: Run Docker image and follow its logs docker start modesrv docker logs -f modesrv Step 5: Test the image Follow these instructions to send example data to localhost:8080 and see some JSON output. Important Notes The MODE backend service uses third-party routing services (e.g. Google Router). At this stage, a sample API key is hardcoded in the project. Make sure to keep that key confident and only use it for testing and sparsingly. Overuse of the API key may result in high fees, as only about 20.000 requests/per month are free! For production, we must use a different key being used for billing. This will be set via an ENV variable to the Docker image.","title":"MODE Integration Guide for backend data analysis"},{"location":"tools/mode/examples/backend/#mode-integration-guide-for-backend-data-analysis","text":"We provide a basic REST entpoint as a Docker image which listens on an endpoint /analyze . The input is binary movement data gathered from the smartphone apps (see here and here ). Output is a JSON string of the taken activities.","title":"MODE Integration Guide for backend data analysis"},{"location":"tools/mode/examples/backend/#using-the-image","text":"Step 1: Copy exported image to server Use your tool of choice. Step 2: Import image on server docker load -i modesrv-1.0.0.tar.gz Step 3: Create Docker container # docker stop modesrv # docker rm modesrv docker create --name modesrv -p 8080:8080 --restart=unless-stopped ait/modesrv:1.0.0 Step 4: Run Docker image and follow its logs docker start modesrv docker logs -f modesrv Step 5: Test the image Follow these instructions to send example data to localhost:8080 and see some JSON output.","title":"Using the image"},{"location":"tools/mode/examples/backend/#important-notes","text":"The MODE backend service uses third-party routing services (e.g. Google Router). At this stage, a sample API key is hardcoded in the project. Make sure to keep that key confident and only use it for testing and sparsingly. Overuse of the API key may result in high fees, as only about 20.000 requests/per month are free! For production, we must use a different key being used for billing. This will be set via an ENV variable to the Docker image.","title":"Important Notes"},{"location":"tools/mode/examples/ios/","text":"MODE Integration Guide for data collection on iOS devices The MODE smartphone library ( libmode ) provides a complete, out-of-the-box solution for mobility data collection via smartphones. It is built as an autonomous component that can easily be integrated into third party apps. For the standard use-case (trip data collection) the library can be integrated into existing Apps with little effort: The most recent version of the MODE library will be provided by AIT upon request . You will also be granted Git access to https://gitlab.ait.ac.at/energy/commons/smartsurvey/ios/SmartSurveyDemoApp/ which hosts a very basic demo app, showing how to use the MODE library. Step 1: Add the library to the App as a build dependency To make it easy to use libmode, we have published the library on our Gitlab in https://gitlab.ait.ac.at/energy/commons/smartsurvey/ios/Libmode.git You can then add the swift package as a dependency in XCode. Step 2: Initialize the library and start data collection The library needs to be initialized before it can start recording data: func initLibMode() { // Allow uploading of trips also on cellular connection (default is Wi-Fi only) UserDefaultsStore.cellularUploadAllowed = true // Set the OAUTH2 token and the server address where trips are uploaded and analyzed. Mode.setAuthorizationToken(token: \"myuser:abcdef123456\") // Define where the backend server is located, waiting for the trips to be // uploaded // // By default, access to non-https is disabled by iOS. For development purposes one can change // Info.plist like described here, but that might not allow the app to be // accepted in the app store, so only use during development or install a local // SSL certificate: // https://stackoverflow.com/questions/63597760/not-allowed-to-make-request-to-local-host-swift Mode.setBaseServerURL(surveyUrl: \"http://localhost:8080\") // This call starts recording trip data. Once it has detected a trip, it is uploaded and analyzed Mode.startSurvey() } Also make sure that you request background location access in the manifest of the project. Support and Contact For any questions, contact Martin Stubenschrott directly at martin.stubenschrott@ait.ac.at","title":"MODE Integration Guide for data collection on iOS devices"},{"location":"tools/mode/examples/ios/#mode-integration-guide-for-data-collection-on-ios-devices","text":"The MODE smartphone library ( libmode ) provides a complete, out-of-the-box solution for mobility data collection via smartphones. It is built as an autonomous component that can easily be integrated into third party apps. For the standard use-case (trip data collection) the library can be integrated into existing Apps with little effort: The most recent version of the MODE library will be provided by AIT upon request . You will also be granted Git access to https://gitlab.ait.ac.at/energy/commons/smartsurvey/ios/SmartSurveyDemoApp/ which hosts a very basic demo app, showing how to use the MODE library. Step 1: Add the library to the App as a build dependency To make it easy to use libmode, we have published the library on our Gitlab in https://gitlab.ait.ac.at/energy/commons/smartsurvey/ios/Libmode.git You can then add the swift package as a dependency in XCode. Step 2: Initialize the library and start data collection The library needs to be initialized before it can start recording data: func initLibMode() { // Allow uploading of trips also on cellular connection (default is Wi-Fi only) UserDefaultsStore.cellularUploadAllowed = true // Set the OAUTH2 token and the server address where trips are uploaded and analyzed. Mode.setAuthorizationToken(token: \"myuser:abcdef123456\") // Define where the backend server is located, waiting for the trips to be // uploaded // // By default, access to non-https is disabled by iOS. For development purposes one can change // Info.plist like described here, but that might not allow the app to be // accepted in the app store, so only use during development or install a local // SSL certificate: // https://stackoverflow.com/questions/63597760/not-allowed-to-make-request-to-local-host-swift Mode.setBaseServerURL(surveyUrl: \"http://localhost:8080\") // This call starts recording trip data. Once it has detected a trip, it is uploaded and analyzed Mode.startSurvey() } Also make sure that you request background location access in the manifest of the project.","title":"MODE Integration Guide for data collection on iOS devices"},{"location":"tools/mode/examples/ios/#support-and-contact","text":"For any questions, contact Martin Stubenschrott directly at martin.stubenschrott@ait.ac.at","title":"Support and Contact"},{"location":"tools/nifi/","text":"Apache NiFi Introduction Apache NiFi is a robust, open-source software solution designed for efficient and reliable data ingestion, processing, and distribution. It is particularly noted for its user-friendly graphical interface that simplifies the design, management, and monitoring of data flows. NiFi supports highly configurable and flexible data routing, system mediation, and transformation capabilities, enabling the handling of diverse data formats and systems. Key features include its scalability to accommodate large volumes of data, built-in data security measures, and extensive audit trails for data provenance. Primarily used in scenarios requiring complex data pipelines, such as real-time data processing and integration across varied systems, Apache NiFi stands out for its ease of use and comprehensive data management capabilities. Features of Apache NiFi Data Provenance: It tracks data provenance, recording a detailed history of data flowing through the system, which is key for auditing and understanding the lifecycle of the data. Flow Management: NiFi enables the management of data flows in real-time, allowing users to route, transform, and process data as it moves through the system. Extensible Architecture: NiFi can be extended with custom processors, services, and other components, making it adaptable to various data processing needs. Scalability: Designed to scale out across a cluster of servers to accommodate large volumes of data and high throughput requirements. Flexible Scheduling: Users can schedule when and how often data processors run, providing control over resource utilisation and timing of data flows. Back Pressure and Prioritisation: NiFi maintains system stability by applying back pressure and data prioritisation, ensuring smooth handling of data under load. Visual Command and Control: The visual command and control feature allows users to see exactly how data is flowing through their system in real-time. Data Buffering: NiFi buffers data between each processing step, ensuring that data is not lost in transit and can handle varying processing loads efficiently. Record-Oriented Data Handling: It supports record-oriented data handling, allowing for fine-grained data processing, which is particularly useful for complex and structured data types. Integration with Diverse Systems: NiFi integrates with a wide range of data sources and sinks, enabling interaction with numerous types of data systems and formats. Use Case Scenario Context: In an urban citizen observatory, members of the community actively participate in monitoring and analysing environmental conditions. One of the critical concerns in many cities is air quality, which directly impacts public health and quality of life. Objective: To monitor, analyse, and visualise air quality data across different parts of the city to identify pollution hotspots, understand temporal trends, and engage the public in environmental awareness and decision-making processes. Implementation Using Apache Druid: 1) Data Collection: - Deploy IoT sensors across the city to measure air quality indicators like PM2.5, PM10, NO2, CO2, and Ozone. 2) Data Ingestion and Storage with Apache NiFi: - If the data cant be collected directly by Apache Druid we should use Apache NiFi to ingest the data from the IoT sensors. - Configure NiFi to gather, preprocess and clean the data, ensuring it's in the correct format for analysis and storage. - Set up NiFi to handle the real-time flow of data, managing any necessary transformations or routing. 3) Data Storage and Real-time aggregation: - Stream the processed sensor data from Apache NiFi to Apache Druid for real-time ingestion and storage. - Configure Druid to handle the high volume of time-series data from the sensors, optimizing for fast access and query performance. Next steps in the process (Data integation, visualisation and data-driven decision making) are not handled by Apache NiFi, thus, we will not cover them in this guide. References Apache NiFi documentation GREENGAGE catalogue entry","title":"Apache NiFi"},{"location":"tools/nifi/#apache-nifi","text":"","title":"Apache NiFi"},{"location":"tools/nifi/#introduction","text":"Apache NiFi is a robust, open-source software solution designed for efficient and reliable data ingestion, processing, and distribution. It is particularly noted for its user-friendly graphical interface that simplifies the design, management, and monitoring of data flows. NiFi supports highly configurable and flexible data routing, system mediation, and transformation capabilities, enabling the handling of diverse data formats and systems. Key features include its scalability to accommodate large volumes of data, built-in data security measures, and extensive audit trails for data provenance. Primarily used in scenarios requiring complex data pipelines, such as real-time data processing and integration across varied systems, Apache NiFi stands out for its ease of use and comprehensive data management capabilities.","title":"Introduction"},{"location":"tools/nifi/#features-of-apache-nifi","text":"Data Provenance: It tracks data provenance, recording a detailed history of data flowing through the system, which is key for auditing and understanding the lifecycle of the data. Flow Management: NiFi enables the management of data flows in real-time, allowing users to route, transform, and process data as it moves through the system. Extensible Architecture: NiFi can be extended with custom processors, services, and other components, making it adaptable to various data processing needs. Scalability: Designed to scale out across a cluster of servers to accommodate large volumes of data and high throughput requirements. Flexible Scheduling: Users can schedule when and how often data processors run, providing control over resource utilisation and timing of data flows. Back Pressure and Prioritisation: NiFi maintains system stability by applying back pressure and data prioritisation, ensuring smooth handling of data under load. Visual Command and Control: The visual command and control feature allows users to see exactly how data is flowing through their system in real-time. Data Buffering: NiFi buffers data between each processing step, ensuring that data is not lost in transit and can handle varying processing loads efficiently. Record-Oriented Data Handling: It supports record-oriented data handling, allowing for fine-grained data processing, which is particularly useful for complex and structured data types. Integration with Diverse Systems: NiFi integrates with a wide range of data sources and sinks, enabling interaction with numerous types of data systems and formats.","title":"Features of Apache NiFi"},{"location":"tools/nifi/#use-case-scenario","text":"","title":"Use Case Scenario"},{"location":"tools/nifi/#context","text":"In an urban citizen observatory, members of the community actively participate in monitoring and analysing environmental conditions. One of the critical concerns in many cities is air quality, which directly impacts public health and quality of life.","title":"Context:"},{"location":"tools/nifi/#objective","text":"To monitor, analyse, and visualise air quality data across different parts of the city to identify pollution hotspots, understand temporal trends, and engage the public in environmental awareness and decision-making processes.","title":"Objective:"},{"location":"tools/nifi/#implementation-using-apache-druid","text":"1) Data Collection: - Deploy IoT sensors across the city to measure air quality indicators like PM2.5, PM10, NO2, CO2, and Ozone. 2) Data Ingestion and Storage with Apache NiFi: - If the data cant be collected directly by Apache Druid we should use Apache NiFi to ingest the data from the IoT sensors. - Configure NiFi to gather, preprocess and clean the data, ensuring it's in the correct format for analysis and storage. - Set up NiFi to handle the real-time flow of data, managing any necessary transformations or routing. 3) Data Storage and Real-time aggregation: - Stream the processed sensor data from Apache NiFi to Apache Druid for real-time ingestion and storage. - Configure Druid to handle the high volume of time-series data from the sensors, optimizing for fast access and query performance. Next steps in the process (Data integation, visualisation and data-driven decision making) are not handled by Apache NiFi, thus, we will not cover them in this guide.","title":"Implementation Using Apache Druid:"},{"location":"tools/nifi/#references","text":"Apache NiFi documentation GREENGAGE catalogue entry","title":"References"},{"location":"tools/nifi/usage/","text":"Apache NiFi Usage Guide These instructions will guide you through the basic process of setting up a simple data flow. Step 1: Log In - Access to the NiFi deploy of Greengage. - First of all, you should login using your Greengage login in the keycloak authentication webpage that will appear. - You will reach to the main view of the tool as shown in the figure below. Step 2: Creating a New Process Group: Drag and drop a Process Group onto the canvas by selecting the fourth icon from the top menu. Process groups are containers for data flows. They allow you to group together related processors and other components. Give your process group a meaningful name, this helps in organizing different data flows. When you create the group it will show in the canvas like this. Now you can double click on it to access it and start adding new processors. Step 3: Adding Processors to Your Flow Drag and drop processors from the components toolbar onto the canvas within your process group. Use processors like 'GetFile' to read data from a file system or 'ListenHTTP' for receiving data over HTTP. Double-click a processor to configure it. Set properties like file path for 'GetFile' or listening port for 'ListenHTTP'. Apply and save the configuration. Step 4: Connecting Processors Draw connections between processors by dragging the arrow from one processor to another. This defines the flow of data between processors. Step 5: Setting Up Processor Scheduling Configure how often each processor runs (e.g., every 5 seconds, on an event basis). This is crucial for managing the flow and processing of data. Step 6: Starting and Stopping Processors Start individual processors or the entire process group to initiate data flow. Stop processors to make configuration changes or for maintenance. Step 7: Securing Your Data Flow If dealing with sensitive data, explore NiFi\u2019s security options, like configuring users, groups, and policies for access control. Remember, Apache NiFi has a steep learning curve, but its flexibility and power are unmatched for data flow management. Experiment with different processors and configurations to understand how they impact your specific use case.","title":"Apache NiFi Usage Guide"},{"location":"tools/nifi/usage/#apache-nifi-usage-guide","text":"These instructions will guide you through the basic process of setting up a simple data flow. Step 1: Log In - Access to the NiFi deploy of Greengage. - First of all, you should login using your Greengage login in the keycloak authentication webpage that will appear. - You will reach to the main view of the tool as shown in the figure below. Step 2: Creating a New Process Group: Drag and drop a Process Group onto the canvas by selecting the fourth icon from the top menu. Process groups are containers for data flows. They allow you to group together related processors and other components. Give your process group a meaningful name, this helps in organizing different data flows. When you create the group it will show in the canvas like this. Now you can double click on it to access it and start adding new processors. Step 3: Adding Processors to Your Flow Drag and drop processors from the components toolbar onto the canvas within your process group. Use processors like 'GetFile' to read data from a file system or 'ListenHTTP' for receiving data over HTTP. Double-click a processor to configure it. Set properties like file path for 'GetFile' or listening port for 'ListenHTTP'. Apply and save the configuration. Step 4: Connecting Processors Draw connections between processors by dragging the arrow from one processor to another. This defines the flow of data between processors. Step 5: Setting Up Processor Scheduling Configure how often each processor runs (e.g., every 5 seconds, on an event basis). This is crucial for managing the flow and processing of data. Step 6: Starting and Stopping Processors Start individual processors or the entire process group to initiate data flow. Stop processors to make configuration changes or for maintenance. Step 7: Securing Your Data Flow If dealing with sensitive data, explore NiFi\u2019s security options, like configuring users, groups, and policies for access control. Remember, Apache NiFi has a steep learning curve, but its flexibility and power are unmatched for data flow management. Experiment with different processors and configurations to understand how they impact your specific use case.","title":"Apache NiFi Usage Guide"},{"location":"tools/nifi/examples/","text":"Apache NiFi Apache NiFi it's not directly integrated into other software tools in the traditional sense of built-in connectors or plugins, NiFi excels in interfacing with a wide range of systems and services through its extensive library of processors. These processors are adept at 'getting' data from various sources and 'putting' processed data to numerous destinations. For example, NiFi can extract data from web services, databases, file systems, and even message queues, process this data according to defined rules and workflows, and then deliver the processed data to different systems, locations, or storage solutions. This flexibility in interacting with external systems is achieved not through direct integration, but by leveraging NiFi's ability to handle diverse data formats and communication protocols, making it a powerful intermediary in any data-centric architecture.","title":"Apache NiFi"},{"location":"tools/nifi/examples/#apache-nifi","text":"Apache NiFi it's not directly integrated into other software tools in the traditional sense of built-in connectors or plugins, NiFi excels in interfacing with a wide range of systems and services through its extensive library of processors. These processors are adept at 'getting' data from various sources and 'putting' processed data to numerous destinations. For example, NiFi can extract data from web services, databases, file systems, and even message queues, process this data according to defined rules and workflows, and then deliver the processed data to different systems, locations, or storage solutions. This flexibility in interacting with external systems is achieved not through direct integration, but by leveraging NiFi's ability to handle diverse data formats and communication protocols, making it a powerful intermediary in any data-centric architecture.","title":"Apache NiFi"},{"location":"tools/sensorsIntegration/","text":"MQTT Introduction MQTT is one of the most wide IoT protocols used both due to its scalable capabilities and the number of platforms for Smart Cities accepting this protocol. For that reason, MQTT has been introduced in the Smart Spot to be a protocol to report values to 3rd party platforms. Nowadays, it can be used only to report values, while managing it will be carried out using the original Smart Spot protocol, which is LwM2M. The management can be used both from the platform provided by Libelium so called Homard. This version of MQTT also includes significant security and reliability improvements compared to earlier versions. For example, authentication and authorization mechanisms were added to enable safer and more controlled communication between devices. Session management was also improved, and a \"last will and testament\" mechanism was added to ensure that devices disconnect properly in case of a network failure. Overall, MQTT v3.1.1 is considered a mature and stable version of the MQTT protocol and is widely used in a variety of IoT applications, such as environmental monitoring, vehicle telemetry, asset tracking, and more. Features of MQTT Supported Security MQTT v3.1.1 supports the two main security mechanisms to ensure secure communication between devices. These include: User and password-based authentication: MQTT devices can be authenticated using user and password credentials stored on the MQTT server. 1.2 Digital certificates: MQTT devices can be authenticated using digital certificates, which provides greater security and mutual authentication between devices. Both of them can be configured using the remote device management platform Homard provided by Libelium. MQTT Ports MQTT uses by default two different ports for communication: TCP port 1883: This is the default port used for unencrypted connection. MQTT clients connect to the MQTT server using this port. The communication between the client and the server is performed without encryption, meaning that the exchanged data is not protected. TCP port 8883: This is the port used for encrypted connection. MQTT clients can connect to the MQTT server using this port and establish a secure connection over TLS (Transport Layer Security). TLS provides an additional layer of security to MQTT communication, protecting the exchanged data between the client and the server It is important to note that default ports may vary depending on the MQTT provider or the MQTT server configuration. Therefore, the MQTT server documentation should be consulted to confirm the ports used in MQTT communication and ensure that clients are properly configured to connect to the server. Configuration The vertical data concept involves grouping and organising the data reported by the device into different verticals depending on the transmitted data set. The grouping of this data aligns with the definition of the official Data Models of the FIWARE platform described on the website https://smartdatamodels.org/. Given the versatility of the Smart Spot to be configurable at the hardware level during the order with different types of sensors, not all data verticals have to be active on the device. The device transmits data using the necessary verticals depending on the hardware configuration of the device. Additionally, it is possible to enable or disable specific verticals separately through the Homard device remote platform as indicated in the next section. The definition of a vertical at MQTT integration level is mainly composed of two elements: Topic: The topic for publishing each data set of the vertical will be composed based on a default prefix or a topic prefix specified by the client through the Homard device remote platform. The topic prefix must have the following format /{base_apikey}/{device_id} <br> where 'base_apikey' is usually an identifying key for a set of devices and 'device_id' is a specific name for the device. If a topic prefix is not configured, the device will use a default one. This topic prefix will be internally modified by the SmartSpot depending on the vertical. Specifically, the SmartSpot will add both the \u2018/attrs\u2019 postfix corresponding to the one required by the FIWARE IoT Agents to receive data and the 'lower_vertical_suffix' and 'upper_vertical_suffix' fields within the topic prefix to separate the data into different topics, one for each vertical, resulting in the final topic being composed in the following way: /{base_apikey}{lower_vertical_suffix}/{device_id}_{upper_vertical_suffix}/attrs <br> The following table illustrates an example of the prefix formation according to the publication vertical. Message: The data published through JSON-encoded text messages on each of the specified topics contain at least the data from the different sensors included in your Smart Spot device for each vertical, and may also include extra information regarding how the device is configured. This extra information is provided as it may be useful to the user and may enable device configuration via MQTT protocol in the future, although this feature is not currently implemented. The following table illustrate the information that can be included in each vertical depending on your device's configuration. The example message will be: ```{ \"TimeInstant\": \"2023-03-20T13:50:00Z\", \"period\": 5, \"status\": \"connected\", \"no2-a4\": 0.640869140625, \"ox-a431\": 28.586013793945312, \"co-a4\": 91.760452270507812, \"so2-a4\": 0.927800178527832, \"pm10\": 49.334125518798828, \"pm2\": 15.33404541015625, \"pm1\": 2.1444158554077148 } ``` It is important to note that if a sensor is not connected or not working properly, the field is still included in the transmitted message, but its value is set to 'null'. The reasons for carrying out this procedure are mainly to: Help identify device malfunctions for maintenance tasks. Prevent the introduction of '0' or repetitive non-real values that can cause confusion when data persists in databases or is represented in graphs.","title":"MQTT"},{"location":"tools/sensorsIntegration/#mqtt","text":"","title":"MQTT"},{"location":"tools/sensorsIntegration/#introduction","text":"MQTT is one of the most wide IoT protocols used both due to its scalable capabilities and the number of platforms for Smart Cities accepting this protocol. For that reason, MQTT has been introduced in the Smart Spot to be a protocol to report values to 3rd party platforms. Nowadays, it can be used only to report values, while managing it will be carried out using the original Smart Spot protocol, which is LwM2M. The management can be used both from the platform provided by Libelium so called Homard. This version of MQTT also includes significant security and reliability improvements compared to earlier versions. For example, authentication and authorization mechanisms were added to enable safer and more controlled communication between devices. Session management was also improved, and a \"last will and testament\" mechanism was added to ensure that devices disconnect properly in case of a network failure. Overall, MQTT v3.1.1 is considered a mature and stable version of the MQTT protocol and is widely used in a variety of IoT applications, such as environmental monitoring, vehicle telemetry, asset tracking, and more.","title":"Introduction"},{"location":"tools/sensorsIntegration/#features-of-mqtt","text":"Supported Security MQTT v3.1.1 supports the two main security mechanisms to ensure secure communication between devices. These include: User and password-based authentication: MQTT devices can be authenticated using user and password credentials stored on the MQTT server. 1.2 Digital certificates: MQTT devices can be authenticated using digital certificates, which provides greater security and mutual authentication between devices. Both of them can be configured using the remote device management platform Homard provided by Libelium. MQTT Ports MQTT uses by default two different ports for communication: TCP port 1883: This is the default port used for unencrypted connection. MQTT clients connect to the MQTT server using this port. The communication between the client and the server is performed without encryption, meaning that the exchanged data is not protected. TCP port 8883: This is the port used for encrypted connection. MQTT clients can connect to the MQTT server using this port and establish a secure connection over TLS (Transport Layer Security). TLS provides an additional layer of security to MQTT communication, protecting the exchanged data between the client and the server It is important to note that default ports may vary depending on the MQTT provider or the MQTT server configuration. Therefore, the MQTT server documentation should be consulted to confirm the ports used in MQTT communication and ensure that clients are properly configured to connect to the server. Configuration The vertical data concept involves grouping and organising the data reported by the device into different verticals depending on the transmitted data set. The grouping of this data aligns with the definition of the official Data Models of the FIWARE platform described on the website https://smartdatamodels.org/. Given the versatility of the Smart Spot to be configurable at the hardware level during the order with different types of sensors, not all data verticals have to be active on the device. The device transmits data using the necessary verticals depending on the hardware configuration of the device. Additionally, it is possible to enable or disable specific verticals separately through the Homard device remote platform as indicated in the next section. The definition of a vertical at MQTT integration level is mainly composed of two elements: Topic: The topic for publishing each data set of the vertical will be composed based on a default prefix or a topic prefix specified by the client through the Homard device remote platform. The topic prefix must have the following format /{base_apikey}/{device_id} <br> where 'base_apikey' is usually an identifying key for a set of devices and 'device_id' is a specific name for the device. If a topic prefix is not configured, the device will use a default one. This topic prefix will be internally modified by the SmartSpot depending on the vertical. Specifically, the SmartSpot will add both the \u2018/attrs\u2019 postfix corresponding to the one required by the FIWARE IoT Agents to receive data and the 'lower_vertical_suffix' and 'upper_vertical_suffix' fields within the topic prefix to separate the data into different topics, one for each vertical, resulting in the final topic being composed in the following way: /{base_apikey}{lower_vertical_suffix}/{device_id}_{upper_vertical_suffix}/attrs <br> The following table illustrates an example of the prefix formation according to the publication vertical. Message: The data published through JSON-encoded text messages on each of the specified topics contain at least the data from the different sensors included in your Smart Spot device for each vertical, and may also include extra information regarding how the device is configured. This extra information is provided as it may be useful to the user and may enable device configuration via MQTT protocol in the future, although this feature is not currently implemented. The following table illustrate the information that can be included in each vertical depending on your device's configuration. The example message will be: ```{ \"TimeInstant\": \"2023-03-20T13:50:00Z\", \"period\": 5, \"status\": \"connected\", \"no2-a4\": 0.640869140625, \"ox-a431\": 28.586013793945312, \"co-a4\": 91.760452270507812, \"so2-a4\": 0.927800178527832, \"pm10\": 49.334125518798828, \"pm2\": 15.33404541015625, \"pm1\": 2.1444158554077148 } ``` It is important to note that if a sensor is not connected or not working properly, the field is still included in the transmitted message, but its value is set to 'null'. The reasons for carrying out this procedure are mainly to: Help identify device malfunctions for maintenance tasks. Prevent the introduction of '0' or repetitive non-real values that can cause confusion when data persists in databases or is represented in graphs.","title":"Features of MQTT"},{"location":"tools/sensorsIntegration/integration/","text":"Libelium IoT sensors Usage & Integration Guide for Third-Party Services Generic usage instructions for SmartSpot and other kind of sensors Instructions on how to use the tool. The user's manual is available at this link The importance of using MQTT connectors in the project is highlighted by the need to establish efficient and standardized communication in an environment where various technologies converge. MQTT (Message Queuing Telemetry Transport) stands out as a lightweight and robust protocol that enables real-time data transmission between devices, ensuring seamless connectivity in heterogeneous environments. In the context of air quality monitoring, data standardization through MQTT facilitates the integration of devices from different manufacturers and technologies, promoting interoperability and simplifying the development of IoT architecture. The efficiency of MQTT is particularly evident in the effective management of messages, minimizing bandwidth and optimizing the transmission of critical information about sensors. Thus, the choice of MQTT connectors not only improves the coherence and speed of communication but also establishes a solid foundation for scalability and future system expansion, ensuring robustness and reliability in projects that require efficient real-time data management. Step 1: Requesting Access Credentials Instructions on how to request access credentials. To get started with your MQTT broker, please contact a.pujante@libelium.com Step 2: Receiving Credentials Once you have your MQTT connector, the next step is to integrate it into your platform or architecture in order to be able to receive the data on it Step 4: Testing the Integration To check the integration, it will be necessary to see if the data is arriving correctly to the platform and can be displayed correctly.. Support and Contact Contact for support, issues and information: a.pujante@libelium.com, e.illueca@libelium.com and aj.jara@libelium.com","title":"Libelium IoT sensors Usage &amp; Integration Guide for Third-Party Services"},{"location":"tools/sensorsIntegration/integration/#libelium-iot-sensors-usage-integration-guide-for-third-party-services","text":"Generic usage instructions for SmartSpot and other kind of sensors Instructions on how to use the tool. The user's manual is available at this link The importance of using MQTT connectors in the project is highlighted by the need to establish efficient and standardized communication in an environment where various technologies converge. MQTT (Message Queuing Telemetry Transport) stands out as a lightweight and robust protocol that enables real-time data transmission between devices, ensuring seamless connectivity in heterogeneous environments. In the context of air quality monitoring, data standardization through MQTT facilitates the integration of devices from different manufacturers and technologies, promoting interoperability and simplifying the development of IoT architecture. The efficiency of MQTT is particularly evident in the effective management of messages, minimizing bandwidth and optimizing the transmission of critical information about sensors. Thus, the choice of MQTT connectors not only improves the coherence and speed of communication but also establishes a solid foundation for scalability and future system expansion, ensuring robustness and reliability in projects that require efficient real-time data management. Step 1: Requesting Access Credentials Instructions on how to request access credentials. To get started with your MQTT broker, please contact a.pujante@libelium.com Step 2: Receiving Credentials Once you have your MQTT connector, the next step is to integrate it into your platform or architecture in order to be able to receive the data on it Step 4: Testing the Integration To check the integration, it will be necessary to see if the data is arriving correctly to the platform and can be displayed correctly.. Support and Contact Contact for support, issues and information: a.pujante@libelium.com, e.illueca@libelium.com and aj.jara@libelium.com","title":"Libelium IoT sensors Usage &amp; Integration Guide for Third-Party Services"},{"location":"tools/superset/","text":"Apache Superset Introduction Apache Superset is a sophisticated, open-source data exploration and visualisation platform designed to transform the way analysts and businesses interact with their data. As a versatile web application, it empowers users with tools for deep data exploration and the creation of beautifully rendered visualisations and dashboards. Superset is renowned for its user-friendly interface, democratising data analysis by enabling users of all skill levels to engage with complex datasets, design compelling visual narratives, and derive actionable insights. Offering a rich array of features, including a variety of chart types, interactive dashboards, and an integrated SQL editor, Apache Superset stands out as a comprehensive solution for modern data analytics and business intelligence needs. Features of Superset Data Exploration: Superset allows users to explore datasets through simple, intuitive, and interactive visualisations. It offers a variety of chart types and the ability to dive deep into the details of the data. Easy Visualisation Creation: Users can easily create and share visualisations without the need for programming. It includes a rich set of visualisations like line charts, bar charts, and scatter plots, among others. Interactive Dashboards: Superset enables the creation of dynamic dashboards that are interactive and customisable. These dashboards can be shared with others and include various visualisations for comprehensive data analysis. SQL IDE: It comes with an integrated SQL IDE, which allows users to write, run, and visualise the results of SQL queries in a user-friendly environment. Security and Integration: Apache Superset offers robust security features, including role-based permission control. It can be integrated with most SQL-based data sources, including traditional databases and newer SQL-speaking data engines. Scalability and Performance: It is designed to be highly scalable, easily handling large-scale data exploration jobs. It also provides features for optimising query performance. Open Source and Extensible: Being open source allows for customisation and extension, enabling developers to contribute to its features or adapt it to specific needs. Community-Driven: Apache Superset is backed by a strong community, which contributes to its continuous development and improvement. Use Case Scenario Context: In an urban citizen observatory, members of the community actively participate in monitoring and analysing environmental conditions. One of the critical concerns in many cities is air quality, which directly impacts public health and quality of life. Objective: To monitor, analyse, and visualise air quality data across different parts of the city to identify pollution hotspots, understand temporal trends, and engage the public in environmental awareness and decision-making processes. Implementation Using Apache Superset: 1) Data Collection: - Deploy IoT sensors across the city to measure air quality indicators like PM2.5, PM10, NO2, CO2, and Ozone. - Sensors transmit data to a centralised database, storing time-stamped readings and location data. 2) Data Integration with Apache Superset: - Connect Superset to the database where sensor data is stored. - Ensure data is updated in real-time or at regular intervals for accurate analysis. 3) Exploratory Data Analysis: - Use Superset\u2019s SQL IDE to query specific aspects of the data, like identifying times of day with peak pollution levels. - Create visualisations to explore correlations, such as between traffic intensity and NO2 levels. 4) Dashboard Creation: - Develop an interactive dashboard in Superset, displaying various visualisations: - Maps showing real-time pollution levels across different city areas. - Line charts depicting temporal trends in air quality. - Bar charts comparing air quality on weekdays vs weekends. - Include filters to allow users to select specific dates, times, or areas for detailed analysis. 5) Data-Driven Decisions: - Use insights from the Superset dashboard to guide community actions and policy recommendations. - For example, advocate for traffic reduction measures in areas with consistently high pollution levels. References Apache Superset documentation GREENGAGE catalogue entry","title":"Apache Superset"},{"location":"tools/superset/#apache-superset","text":"","title":"Apache Superset"},{"location":"tools/superset/#introduction","text":"Apache Superset is a sophisticated, open-source data exploration and visualisation platform designed to transform the way analysts and businesses interact with their data. As a versatile web application, it empowers users with tools for deep data exploration and the creation of beautifully rendered visualisations and dashboards. Superset is renowned for its user-friendly interface, democratising data analysis by enabling users of all skill levels to engage with complex datasets, design compelling visual narratives, and derive actionable insights. Offering a rich array of features, including a variety of chart types, interactive dashboards, and an integrated SQL editor, Apache Superset stands out as a comprehensive solution for modern data analytics and business intelligence needs.","title":"Introduction"},{"location":"tools/superset/#features-of-superset","text":"Data Exploration: Superset allows users to explore datasets through simple, intuitive, and interactive visualisations. It offers a variety of chart types and the ability to dive deep into the details of the data. Easy Visualisation Creation: Users can easily create and share visualisations without the need for programming. It includes a rich set of visualisations like line charts, bar charts, and scatter plots, among others. Interactive Dashboards: Superset enables the creation of dynamic dashboards that are interactive and customisable. These dashboards can be shared with others and include various visualisations for comprehensive data analysis. SQL IDE: It comes with an integrated SQL IDE, which allows users to write, run, and visualise the results of SQL queries in a user-friendly environment. Security and Integration: Apache Superset offers robust security features, including role-based permission control. It can be integrated with most SQL-based data sources, including traditional databases and newer SQL-speaking data engines. Scalability and Performance: It is designed to be highly scalable, easily handling large-scale data exploration jobs. It also provides features for optimising query performance. Open Source and Extensible: Being open source allows for customisation and extension, enabling developers to contribute to its features or adapt it to specific needs. Community-Driven: Apache Superset is backed by a strong community, which contributes to its continuous development and improvement.","title":"Features of Superset"},{"location":"tools/superset/#use-case-scenario","text":"","title":"Use Case Scenario"},{"location":"tools/superset/#context","text":"In an urban citizen observatory, members of the community actively participate in monitoring and analysing environmental conditions. One of the critical concerns in many cities is air quality, which directly impacts public health and quality of life.","title":"Context:"},{"location":"tools/superset/#objective","text":"To monitor, analyse, and visualise air quality data across different parts of the city to identify pollution hotspots, understand temporal trends, and engage the public in environmental awareness and decision-making processes.","title":"Objective:"},{"location":"tools/superset/#implementation-using-apache-superset","text":"1) Data Collection: - Deploy IoT sensors across the city to measure air quality indicators like PM2.5, PM10, NO2, CO2, and Ozone. - Sensors transmit data to a centralised database, storing time-stamped readings and location data. 2) Data Integration with Apache Superset: - Connect Superset to the database where sensor data is stored. - Ensure data is updated in real-time or at regular intervals for accurate analysis. 3) Exploratory Data Analysis: - Use Superset\u2019s SQL IDE to query specific aspects of the data, like identifying times of day with peak pollution levels. - Create visualisations to explore correlations, such as between traffic intensity and NO2 levels. 4) Dashboard Creation: - Develop an interactive dashboard in Superset, displaying various visualisations: - Maps showing real-time pollution levels across different city areas. - Line charts depicting temporal trends in air quality. - Bar charts comparing air quality on weekdays vs weekends. - Include filters to allow users to select specific dates, times, or areas for detailed analysis. 5) Data-Driven Decisions: - Use insights from the Superset dashboard to guide community actions and policy recommendations. - For example, advocate for traffic reduction measures in areas with consistently high pollution levels.","title":"Implementation Using Apache Superset:"},{"location":"tools/superset/#references","text":"Apache Superset documentation GREENGAGE catalogue entry","title":"References"},{"location":"tools/superset/usage/","text":"Apache Superset Usage Guide Superset allows you to create charts and dashboards from a wide variety of data sources. In this guide we will show you how to create charts and dashboards from the data sources that we have available in Greengage: Step 1: Log In - Access to the Superset deploy of Greengage. - First of all, you should login using your Greengage login in the keycloak authentication webpage that will appear. - You will reach to the main view of the tool as shown in the figure below. Step 2: Creating datasets Click on the DATASETS button at the top-left menu and click on the +DATASET button at the top-right part of the view that you have arrived. Introduce the database, schema and table that you want to import from the possible sources and click CREATE DATASET AND CREATE CHART button. NOTE: In case that you want to connect Apache Superset to a new database or source contact with ruben.sanchez@deusto.es Step 3: Creating charts Click on the CHARTS button at the top-left menu and click on the +CHART button at the top-right part of the view that you have arrived. For the chart creation you should select a dataset and a chart type to show the data. Once in the chart editing view, you should add a name to the chart. In this view you have two columns: the left one has all the rows from the dataset yout selected and the right one has the settings for the chart. Once you have selected the right settings, click on the CREATE CHART button at the bottom of the second column. Note that to upload the chart you will have to click again in a button that updates it in the same position as the previous one. Step 4: Creating dashboards Click on the DASHBOARDS button at the top-left menu and click on the +DASHBOARD button at the top-right part of the view that you have arrived. Once in the dashboard creation view, you should add a name to the dashboard. In this view you can add already created charts that appear in the right side or create new charts. Furthermore, you can add new Layout elements such as headers, text or dividers. Extra: Executing SQL queries to imported datasets Accessing to the SQL view using the button at the top left menu you can access a SQL statement executor against the datasets imported in Superset. Creating a chart from geo sources First of all, create a chart with the desired data source. For geographical sources, the best is to put the \"map\" word into the search bar to find all the map-based charts. Once in the chart editing view, you should identify the fields regarding latitude and longitude. Note that if both fields are in one field they may be inverted. Once the settings are set, create or update the chart to visualise it.","title":"Apache Superset Usage Guide"},{"location":"tools/superset/usage/#apache-superset-usage-guide","text":"Superset allows you to create charts and dashboards from a wide variety of data sources. In this guide we will show you how to create charts and dashboards from the data sources that we have available in Greengage: Step 1: Log In - Access to the Superset deploy of Greengage. - First of all, you should login using your Greengage login in the keycloak authentication webpage that will appear. - You will reach to the main view of the tool as shown in the figure below. Step 2: Creating datasets Click on the DATASETS button at the top-left menu and click on the +DATASET button at the top-right part of the view that you have arrived. Introduce the database, schema and table that you want to import from the possible sources and click CREATE DATASET AND CREATE CHART button. NOTE: In case that you want to connect Apache Superset to a new database or source contact with ruben.sanchez@deusto.es Step 3: Creating charts Click on the CHARTS button at the top-left menu and click on the +CHART button at the top-right part of the view that you have arrived. For the chart creation you should select a dataset and a chart type to show the data. Once in the chart editing view, you should add a name to the chart. In this view you have two columns: the left one has all the rows from the dataset yout selected and the right one has the settings for the chart. Once you have selected the right settings, click on the CREATE CHART button at the bottom of the second column. Note that to upload the chart you will have to click again in a button that updates it in the same position as the previous one. Step 4: Creating dashboards Click on the DASHBOARDS button at the top-left menu and click on the +DASHBOARD button at the top-right part of the view that you have arrived. Once in the dashboard creation view, you should add a name to the dashboard. In this view you can add already created charts that appear in the right side or create new charts. Furthermore, you can add new Layout elements such as headers, text or dividers. Extra: Executing SQL queries to imported datasets Accessing to the SQL view using the button at the top left menu you can access a SQL statement executor against the datasets imported in Superset. Creating a chart from geo sources First of all, create a chart with the desired data source. For geographical sources, the best is to put the \"map\" word into the search bar to find all the map-based charts. Once in the chart editing view, you should identify the fields regarding latitude and longitude. Note that if both fields are in one field they may be inverted. Once the settings are set, create or update the chart to visualise it.","title":"Apache Superset Usage Guide"},{"location":"tools/superset/examples/","text":"Apache Superset Apache Superset cannot be integrated with third-party tools, however, new data sources can be integrated into Superset. Integrating new data sources to Apache Superset Apache Superset can ingest data from more than 40 sources. Some of the integrations have built-in configuration wizards, as seen in the image below. However, others, only request the connection URL as stated in Superset documentation Once you have added the details for the connection, Apache Superset will create a connection test and provide you with feedback on it. If the connection is successful, the new datasets are ready to be used.","title":"Apache Superset"},{"location":"tools/superset/examples/#apache-superset","text":"Apache Superset cannot be integrated with third-party tools, however, new data sources can be integrated into Superset.","title":"Apache Superset"},{"location":"tools/superset/examples/#integrating-new-data-sources-to-apache-superset","text":"Apache Superset can ingest data from more than 40 sources. Some of the integrations have built-in configuration wizards, as seen in the image below. However, others, only request the connection URL as stated in Superset documentation Once you have added the details for the connection, Apache Superset will create a connection test and provide you with feedback on it. If the connection is successful, the new datasets are ready to be used.","title":"Integrating new data sources to Apache Superset"},{"location":"tools/urbantep-visat/","text":"UrbanTEP / VISAT Introduction VISAT is standing for Visualization and Analytical Toolbox, is a technology provided by GISAT within the Urban Thematic and Exploration Platform (UrbanTEP). This tool is designed to facilitate the seamless integration, exploration, visualization, and analysis of diverse datasets. Moreover, VISAT is equipped with extensive collaborative features, enhancing data and result sharing among users. Developed on the robust foundation of GISAT's Panther framework, VISAT is presently undergoing substantial technological advancements. Within the GREENGAGE project, the VISAT technology, as an integral part of UrbanTEP, extends its support to Pilot solutions, ensuring capabilities such as data integration and exploration are as effective and efficient as they are in the UrbanTEP environment. UrbanTEP/VISAT plays a pivotal role in supporting Citizen Observatory co-production processes in three main areas: - data integration and provision, - data visualization, - data exploration and analysis The extent of support required from UrbanTEP/VISAT for different Citizen Observatory projects depends on the specific needs and user requirements. UrbanTEP / VISAT features VISAT visualization capabilities are powered by a robust combination of advanced libraries, chiefly Nivo charts and DeckGL, which are integrated into GISAT's Panther framework. This framework is designed to support both front-end and back-end development, providing a seamless web application experience. Users can effectively visualize geospatial data, aiding in comprehensive urban analysis. UrbanTEP supports the integration of large and complex datasets based on Earth Observation data. It also facilitates data provision for analysis. The platform itself allows sharing, exporting, and ingesting derived information for use beyond the platform. Charts Nivo charts, a rich library for creating a wide array of responsive and interactive data visualizations, is built on top of D3.js, a powerful tool for manipulating documents based on data. This foundation allows Nivo charts to offer a high level of detail and customization in visual representations. VISAT supports a diverse range of Nivo chart types, including but not limited to bar, line, pie, radar, and heatmap charts. Each chart type is designed to be highly customizable to suit different data visualization needs. The charts are interactive, enabling users to zoom, filter, and drill down into data for more detailed insights. They are also responsive, ensuring optimal viewing across different devices and screen sizes. Map components DeckGL is a WebGL-powered framework optimized for exploring and visualizing large-scale geospatial data. Its integration into VISAT enables the rendering of large data sets in a performant and visually appealing manner. DeckGL's layered approach allows for the creation of complex 2D visualizations, combining multiple layers of data (such as maps, scatter plots, and hexagon layers) to create rich, informative visual experiences. DeckGL supports interactive features like tooltips, clickable objects, and viewport transitions, enhancing the user's ability to engage with and understand spatial data. Architecture Visualization and Analytics Toolbox (VISAT) is based on an open extendable framework, designed as user-friendly and intuitive interface and allows data to be presented using dynamic and interlinked map and non-map component (e.g., charts, tables and graphs). The platform features two interfaces, a Web GUI, and a REST API that handles communication between multiple parts of the application, including front-end and back-end. BackEnd Backend services - set of microservices, written in JavaScript for the NodeJS environment and dockerized for easier maintenance and better scalability. At the same time, storage technologies are used according to the nature of the data + own spatial data in PostgreSQL / PostGIS. Complex metadata structures and dynamic database schemas are handled using MongoDB FrontEnd Frontend components - a set of packages written in ReactJS, using Redux for state management, D3JS for graph visualization and LeafletJS and DeckJS for map elements. The deployment strategy of the final product is characterized as a Cloud solution (SaaS) and can run on various platforms that support the Docker technology. The GISAT\u2019s framework, used for VISAT creation, is based on open-source development dependencies and includes hundreds of packages that are constantly updated by third-party developers. The most common technologies and dependencies used are NodeJS, Postgres, MongoDB, and S3 storage. Overall, the Urban TEP is a reliable and efficient tool for geospatial data analysis and visualization. Use Case Scenario Innovative data (based on EO) are available globally, providing detailed information on urban areas footprint evolution for a long time span. Data itself are huge, so user asked for following support via VISAT: users want to integrate these data users want to visualize these data user want to explore ana analyse data using user-defined indicators (SDG indicators) user want to compare indicators for different datasets, years or areas user want to share, export, ingest derived information for further use out of the VISAT UrbanTEP VISAT SDG application example References Tool specification document User manual from D4.2 page 76 GREENGAGE catalogue entry","title":"UrbanTEP / VISAT"},{"location":"tools/urbantep-visat/#urbantep-visat","text":"","title":"UrbanTEP / VISAT"},{"location":"tools/urbantep-visat/#introduction","text":"VISAT is standing for Visualization and Analytical Toolbox, is a technology provided by GISAT within the Urban Thematic and Exploration Platform (UrbanTEP). This tool is designed to facilitate the seamless integration, exploration, visualization, and analysis of diverse datasets. Moreover, VISAT is equipped with extensive collaborative features, enhancing data and result sharing among users. Developed on the robust foundation of GISAT's Panther framework, VISAT is presently undergoing substantial technological advancements. Within the GREENGAGE project, the VISAT technology, as an integral part of UrbanTEP, extends its support to Pilot solutions, ensuring capabilities such as data integration and exploration are as effective and efficient as they are in the UrbanTEP environment. UrbanTEP/VISAT plays a pivotal role in supporting Citizen Observatory co-production processes in three main areas: - data integration and provision, - data visualization, - data exploration and analysis The extent of support required from UrbanTEP/VISAT for different Citizen Observatory projects depends on the specific needs and user requirements.","title":"Introduction"},{"location":"tools/urbantep-visat/#urbantep-visat-features","text":"VISAT visualization capabilities are powered by a robust combination of advanced libraries, chiefly Nivo charts and DeckGL, which are integrated into GISAT's Panther framework. This framework is designed to support both front-end and back-end development, providing a seamless web application experience. Users can effectively visualize geospatial data, aiding in comprehensive urban analysis. UrbanTEP supports the integration of large and complex datasets based on Earth Observation data. It also facilitates data provision for analysis. The platform itself allows sharing, exporting, and ingesting derived information for use beyond the platform.","title":"UrbanTEP / VISAT features"},{"location":"tools/urbantep-visat/#charts","text":"Nivo charts, a rich library for creating a wide array of responsive and interactive data visualizations, is built on top of D3.js, a powerful tool for manipulating documents based on data. This foundation allows Nivo charts to offer a high level of detail and customization in visual representations. VISAT supports a diverse range of Nivo chart types, including but not limited to bar, line, pie, radar, and heatmap charts. Each chart type is designed to be highly customizable to suit different data visualization needs. The charts are interactive, enabling users to zoom, filter, and drill down into data for more detailed insights. They are also responsive, ensuring optimal viewing across different devices and screen sizes.","title":"Charts"},{"location":"tools/urbantep-visat/#map-components","text":"DeckGL is a WebGL-powered framework optimized for exploring and visualizing large-scale geospatial data. Its integration into VISAT enables the rendering of large data sets in a performant and visually appealing manner. DeckGL's layered approach allows for the creation of complex 2D visualizations, combining multiple layers of data (such as maps, scatter plots, and hexagon layers) to create rich, informative visual experiences. DeckGL supports interactive features like tooltips, clickable objects, and viewport transitions, enhancing the user's ability to engage with and understand spatial data.","title":"Map components"},{"location":"tools/urbantep-visat/#architecture","text":"Visualization and Analytics Toolbox (VISAT) is based on an open extendable framework, designed as user-friendly and intuitive interface and allows data to be presented using dynamic and interlinked map and non-map component (e.g., charts, tables and graphs). The platform features two interfaces, a Web GUI, and a REST API that handles communication between multiple parts of the application, including front-end and back-end.","title":"Architecture"},{"location":"tools/urbantep-visat/#backend","text":"Backend services - set of microservices, written in JavaScript for the NodeJS environment and dockerized for easier maintenance and better scalability. At the same time, storage technologies are used according to the nature of the data + own spatial data in PostgreSQL / PostGIS. Complex metadata structures and dynamic database schemas are handled using MongoDB","title":"BackEnd"},{"location":"tools/urbantep-visat/#frontend","text":"Frontend components - a set of packages written in ReactJS, using Redux for state management, D3JS for graph visualization and LeafletJS and DeckJS for map elements. The deployment strategy of the final product is characterized as a Cloud solution (SaaS) and can run on various platforms that support the Docker technology. The GISAT\u2019s framework, used for VISAT creation, is based on open-source development dependencies and includes hundreds of packages that are constantly updated by third-party developers. The most common technologies and dependencies used are NodeJS, Postgres, MongoDB, and S3 storage. Overall, the Urban TEP is a reliable and efficient tool for geospatial data analysis and visualization.","title":"FrontEnd"},{"location":"tools/urbantep-visat/#use-case-scenario","text":"Innovative data (based on EO) are available globally, providing detailed information on urban areas footprint evolution for a long time span. Data itself are huge, so user asked for following support via VISAT: users want to integrate these data users want to visualize these data user want to explore ana analyse data using user-defined indicators (SDG indicators) user want to compare indicators for different datasets, years or areas user want to share, export, ingest derived information for further use out of the VISAT UrbanTEP VISAT SDG application example","title":"Use Case Scenario"},{"location":"tools/urbantep-visat/#references","text":"Tool specification document User manual from D4.2 page 76 GREENGAGE catalogue entry","title":"References"},{"location":"tools/urbantep-visat/deployment/","text":"UrbanTEP / VISAT Overview The deployment of UrbanTEP/VISAT is streamlined through the use of Docker, a popular containerization platform that simplifies the process of deploying and managing applications. Docker containers package an application with all the parts it needs, such as libraries and other dependencies, and ship it all out as one package. This ensures consistency across various development, testing, and production environments, and simplifies scalability and updates. Prerequisites Before proceeding with the deployment, ensure the following prerequisites are met: Docker Installation : Docker must be installed on the host machine. For installation instructions, refer to the official Docker documentation . Docker Compose : For managing multi-container Docker applications, Docker Compose is recommended. Installation instructions can be found on the Docker Compose documentation page . Access to UrbanTEP/VISAT Docker Images : Ensure access to the repository containing the Docker images for UrbanTEP/VISAT. Configuration Files : Prepare the necessary configuration files that are specific to your deployment, including environment variables and any other necessary settings. Deployment UrbanTEP/VISAT employs a microservices architecture on the backend, ensuring easy interoperability and support for various data formats and sources. The frontend is modular, built on React architecture, and allows integration with both new and existing tools. Deployment Steps Clone Repository: Clone the repository containing the Docker Compose file and other necessary deployment scripts. git clone cd Configure Environment: Set up the environment variables as per your specific deployment requirements. This may include database connections, service endpoints, and other critical settings. Run Docker Compose: Use Docker Compose to build and start the containers. This can typically be done with a single command: docker-compose up -d This command will download the required Docker images, create containers, and start the services defined in the docker-compose.yml file. Verify Deployment: Once the containers are up and running, verify the deployment by accessing the UrbanTEP/VISAT application through a web browser. Ensure all services are functioning as expected and that there are no connectivity issues between the microservices. Post-Deployment Configuration After successfully deploying UrbanTEP/VISAT, perform any necessary post-deployment configurations. This may include: Setting up additional user accounts and permissions. Integrating with other systems or databases. Configuring backups and data retention policies. Updating the Application To update UrbanTEP/VISAT, pull the latest Docker images and re-run the Docker Compose: docker-compose pull docker-compose up -d --force-recreate This will ensure that you are running the latest version of the application with all the necessary updates and security patches. Troubleshooting In case of issues during deployment, consult the application logs and Docker container logs. Common issues might include: Network connectivity problems. Configuration errors. Incompatibilities between different versions of Docker images. For detailed troubleshooting, refer to the specific error messages in the logs and the UrbanTEP/VISAT documentation. Security All data transfer occurs over secure HTTPS connections, and authorization is implemented following OAuth 2.0 standards, ensuring robust security and privacy measures. Scalability & Performance The platform is designed to be scalable, with the ability to handle increasing data volumes, concurrent users, and system load. It is adaptable to different user needs and project requirements, whether deployed on a local machine, a demo environment, or production-like environments.","title":"UrbanTEP / VISAT"},{"location":"tools/urbantep-visat/deployment/#urbantep-visat","text":"","title":"UrbanTEP / VISAT"},{"location":"tools/urbantep-visat/deployment/#overview","text":"The deployment of UrbanTEP/VISAT is streamlined through the use of Docker, a popular containerization platform that simplifies the process of deploying and managing applications. Docker containers package an application with all the parts it needs, such as libraries and other dependencies, and ship it all out as one package. This ensures consistency across various development, testing, and production environments, and simplifies scalability and updates.","title":"Overview"},{"location":"tools/urbantep-visat/deployment/#prerequisites","text":"Before proceeding with the deployment, ensure the following prerequisites are met: Docker Installation : Docker must be installed on the host machine. For installation instructions, refer to the official Docker documentation . Docker Compose : For managing multi-container Docker applications, Docker Compose is recommended. Installation instructions can be found on the Docker Compose documentation page . Access to UrbanTEP/VISAT Docker Images : Ensure access to the repository containing the Docker images for UrbanTEP/VISAT. Configuration Files : Prepare the necessary configuration files that are specific to your deployment, including environment variables and any other necessary settings.","title":"Prerequisites"},{"location":"tools/urbantep-visat/deployment/#deployment","text":"UrbanTEP/VISAT employs a microservices architecture on the backend, ensuring easy interoperability and support for various data formats and sources. The frontend is modular, built on React architecture, and allows integration with both new and existing tools.","title":"Deployment"},{"location":"tools/urbantep-visat/deployment/#deployment-steps","text":"Clone Repository: Clone the repository containing the Docker Compose file and other necessary deployment scripts. git clone cd Configure Environment: Set up the environment variables as per your specific deployment requirements. This may include database connections, service endpoints, and other critical settings. Run Docker Compose: Use Docker Compose to build and start the containers. This can typically be done with a single command: docker-compose up -d This command will download the required Docker images, create containers, and start the services defined in the docker-compose.yml file. Verify Deployment: Once the containers are up and running, verify the deployment by accessing the UrbanTEP/VISAT application through a web browser. Ensure all services are functioning as expected and that there are no connectivity issues between the microservices.","title":"Deployment Steps"},{"location":"tools/urbantep-visat/deployment/#post-deployment-configuration","text":"After successfully deploying UrbanTEP/VISAT, perform any necessary post-deployment configurations. This may include: Setting up additional user accounts and permissions. Integrating with other systems or databases. Configuring backups and data retention policies.","title":"Post-Deployment Configuration"},{"location":"tools/urbantep-visat/deployment/#updating-the-application","text":"To update UrbanTEP/VISAT, pull the latest Docker images and re-run the Docker Compose: docker-compose pull docker-compose up -d --force-recreate This will ensure that you are running the latest version of the application with all the necessary updates and security patches.","title":"Updating the Application"},{"location":"tools/urbantep-visat/deployment/#troubleshooting","text":"In case of issues during deployment, consult the application logs and Docker container logs. Common issues might include: Network connectivity problems. Configuration errors. Incompatibilities between different versions of Docker images. For detailed troubleshooting, refer to the specific error messages in the logs and the UrbanTEP/VISAT documentation.","title":"Troubleshooting"},{"location":"tools/urbantep-visat/deployment/#security","text":"All data transfer occurs over secure HTTPS connections, and authorization is implemented following OAuth 2.0 standards, ensuring robust security and privacy measures.","title":"Security"},{"location":"tools/urbantep-visat/deployment/#scalability-performance","text":"The platform is designed to be scalable, with the ability to handle increasing data volumes, concurrent users, and system load. It is adaptable to different user needs and project requirements, whether deployed on a local machine, a demo environment, or production-like environments.","title":"Scalability &amp; Performance"},{"location":"tools/urbantep-visat/integration/","text":"UrbanTEP / VISAT Usage & Integration Guide for Third-Party Services The framework for building applications (such as UrbanTEP/VISAT) allowing visualisation and efficient analysis of geospatial data. To enable this, at the core of the framework there is a system of interlinked metadata structures, which enables the framework to understand the data on a conceptual level, facilitating easy access to the data tailored to its nature. For introducing new data sources into the system, the most important (and in case of automated addition of data necessary) aspect is consistent and unique identification of all concepts and attributes . Data templates The key concepts are templates - metadata structures that describe a certain type of data irrespective of its specific context such as time and place. Layer Template Describes a type of spatial layer. An example layer template could be \u201cLand cover\u201d, \u201cPopulation density\u201d, \u201cWorld Settlement Footprint\u201d etc. Any specific data file/table needs to be paired to a layer template, and the same kind of data, typically across different times and locations, needs to be paired with the same layer template each time. Identifying templates: - Names of layer types routinely change, for this reason using the name alone is often not enough - If communicating with our API directly, UUIDs are used. If we are accessing an external system, any form of unique identifier will do - If there are more variants of similar data (e.g. the same nature of data resulting from two different configurations of a processing algorithm) that are to be distinct, this needs to be reflected in distinct templates - If the layer type is found within a tree of layer types, it needs to be identified uniquely across the whole tree, not just within the parent node Parameters needed for basic function: - Id (see above, needs to be supplied) - Name (can be statically supplied or defined on our side) - Corresponding style/symbology (can be statically supplied or defined on our side) Attribute Describes a type of attribute / indicator connected to a spatial layer. For example, \u201cLand use\u201d, \u201cPrimary education rate\u201d, \u201cArea\u201d, Parameters needed for basic function: - Id (similar to layer template) - Name (can be statically supplied or defined on our side) - Units (can be statically supplied or defined on our side), e.g. \u2018km2\u2019, \u2018inhabitants\u2019 Data sources Data sources are technical metadata structures, used by components of the system to understand how to work with the data. They describe where the data is stored (local database, filesystem, remote API etc.) and what format it is in (we have DS types for e.g. PostGIS, Cloud Optimised GeoTIFF, WMTS etc.). Based on data source type, specific methods are used to work with the data and transform it for the rest of the system (which uses mostly custom GeoJSON, only in case of display in map formats more suited for work with large datasets). Additionally, data sources can also contain information useful for human understanding of the datasets (largely metadata in the usual sense). Parameters needed for basic function (apart from the data itself): - Id - Type - Type-specific information necessary for data retrieval Relations Relations, linking individual data sources to respective data templates, while not metadata structures per se, are what allows the system, thus the apps built on top of it, and thus ultimately the users to conceptually work with the underlying data according to its nature as opposed to just as datasets in a GIS application. They also drive the need for the parameters specified with each metadata type, for consistency and identifiability of each instance of them. Apart from just linking the data to its describing template, the system uses relations to make further sense in the data by utilising other complementary structures, which describe various aspects of the particular data, but mainly of the particular relation, owing to specific requirements of specific applications. That is, the same data can be described slightly differently in different contexts, which is why this is not coupled with either the data itself or the template. (In effect, these complementary structures modify the relationship between the data and the template.) Modifiers (common metadata objects) Modifiers are items representing a metadata concept (time, place,...) common to multiple data layers (and thus usable to connect data relating to it together), or distinguishing multiple variations of the same data. These are not required if not used in the application to filter available data or for navigation, but have to be defined in such cases. Period Describes a time period for which given data is valid. It is used mainly for larger periods common to more datasets (e.g. for time series, it would be used for the whole period covered, but not for individual measurement times) Parameters needed for basic function: - Id - Period (ISO 8601) - Name - if different from period, e.g. \u2018planting season 2015\u2019. It\u2019s not necessary to provide names if they are derived from the period in a consistent way, e.g. \u201cJanuary \u201823\u201d, \u201cFebruary \u201823\u201d, etc. Place Describes a geographic area to which given data corresponds. Parameters needed for basic function: - Id - Name - Geometry (polygon, bounding box, point coordinates) Case In case the same kind of data is available e.g. for multiple different entities, those are differentiated by this data type. A case can represent any kind of subject of the data - e.g. occurrence maps may be available for different species; in that case, \u2018occurrence\u2019 would be the layer template and different species would be different cases. This is so that the system can understand that all occurrence maps are the same in nature, yet differentiate among different species. (currently under development is possibility to link to multiple cases) Parameters needed for basic function: - Id - Name The last two modifier types, scope and scenario, are largely legacy types and their use cases may be better served by different means in newer versions of the framework. Be sure to consult with us if your specific use cases might benefit from using these. Scope Scope is used for separating large amounts of data and metadata into distinct logical collections (e.g. thematic datasets, worldwide data vs. locally specific data, etc.) Parameters needed for basic function: - Id - Name Scenario While the other types describe what the data is, scenarios describe what the data could be. They are used in specific use cases when slightly different versions of the same data (relating to all the same above structures) are prepared either by alternating some aspect of the input, or some parameters of an algorithm the output of which the data is. For example, we have used scenarios for computing heat islands based on changes in several features in land cover layers, or for comparing how different tolerance values affect the results of an algorithm. Parameters needed for basic function: - Id - Name The above mentioned metadata structure types are necessary in most of the cases to enable the majority of the system to effectively work with the data. As such, they are used in the majority of cases and/or by the majority of components of the system. In addition to those, the system utilises other types, either for more specific use cases, or in specific components of the system. Area trees Area trees represent administrative or statistical units, areas of interest, etc. In principle they work very similar to layer templates, with added relations between multiple levels of subdivision (granularity). Area tree The basic structure, defining a set (tree) of areas/units. This could, for example, be \u2018GADM\u2019. Each tree contains one or more levels. Parameters needed for basic function: - Id - Name Area tree level A level is in essence a specialised form of a layer template - it is connected to a data source, which can differ based on modifiers - with additional information about how individual spatial features relate to features in a higher level. An example of an area tree level would be \u2018GADM1\u2019, connected to a data source where each feature also has information about the parent feature from \u2018GADM0\u2019. Parameters needed for basic function: - Id - Name - Area tree - Level (index) Style Describes a style, or symbology, for a layer template. For a wider feature set, we use a custom format, but for simple styling, we\u2019re able to convert to it from e.g. an SLD. Multiple styles can be used with one layer template, and one style can be used by multiple layer templates. Parameters needed for basic function: - Id - Name (if styles can be changed by the user) Tag To allow for building custom structure or hierarchy upon any existing metadata structure type, any instance of those can be tagged. This enables the structure to be dynamically updated and administered. Parameters needed for basic function: - Id - Name Users Access rights are set for any metadata structure type, and for either users or user groups. The system is able to work with external authentication providers using OAuth 2 (under development). User A single user account. Parameters needed for basic function: - Id - e-mail/passphrase or external system and user identification User group A group of users, which usually holds most access rights. A user may be part of multiple groups. Parameters needed for basic function: - Id - Name URL access (under development) Apart from logged-in user accounts, access to content can be granted via a key in a link to the application. This can also be combined with standard user access rights. View Any state of the application, or partial state, can be saved, including the displayed map layers, visualisations, map position, open tool windows, etc. (under development) (The framework does not support externally loaded views).","title":"UrbanTEP / VISAT"},{"location":"tools/urbantep-visat/integration/#urbantep-visat","text":"","title":"UrbanTEP / VISAT"},{"location":"tools/urbantep-visat/integration/#usage-integration-guide-for-third-party-services","text":"The framework for building applications (such as UrbanTEP/VISAT) allowing visualisation and efficient analysis of geospatial data. To enable this, at the core of the framework there is a system of interlinked metadata structures, which enables the framework to understand the data on a conceptual level, facilitating easy access to the data tailored to its nature. For introducing new data sources into the system, the most important (and in case of automated addition of data necessary) aspect is consistent and unique identification of all concepts and attributes .","title":"Usage &amp; Integration Guide for Third-Party Services"},{"location":"tools/urbantep-visat/integration/#data-templates","text":"The key concepts are templates - metadata structures that describe a certain type of data irrespective of its specific context such as time and place. Layer Template Describes a type of spatial layer. An example layer template could be \u201cLand cover\u201d, \u201cPopulation density\u201d, \u201cWorld Settlement Footprint\u201d etc. Any specific data file/table needs to be paired to a layer template, and the same kind of data, typically across different times and locations, needs to be paired with the same layer template each time. Identifying templates: - Names of layer types routinely change, for this reason using the name alone is often not enough - If communicating with our API directly, UUIDs are used. If we are accessing an external system, any form of unique identifier will do - If there are more variants of similar data (e.g. the same nature of data resulting from two different configurations of a processing algorithm) that are to be distinct, this needs to be reflected in distinct templates - If the layer type is found within a tree of layer types, it needs to be identified uniquely across the whole tree, not just within the parent node Parameters needed for basic function: - Id (see above, needs to be supplied) - Name (can be statically supplied or defined on our side) - Corresponding style/symbology (can be statically supplied or defined on our side) Attribute Describes a type of attribute / indicator connected to a spatial layer. For example, \u201cLand use\u201d, \u201cPrimary education rate\u201d, \u201cArea\u201d, Parameters needed for basic function: - Id (similar to layer template) - Name (can be statically supplied or defined on our side) - Units (can be statically supplied or defined on our side), e.g. \u2018km2\u2019, \u2018inhabitants\u2019","title":"Data templates"},{"location":"tools/urbantep-visat/integration/#data-sources","text":"Data sources are technical metadata structures, used by components of the system to understand how to work with the data. They describe where the data is stored (local database, filesystem, remote API etc.) and what format it is in (we have DS types for e.g. PostGIS, Cloud Optimised GeoTIFF, WMTS etc.). Based on data source type, specific methods are used to work with the data and transform it for the rest of the system (which uses mostly custom GeoJSON, only in case of display in map formats more suited for work with large datasets). Additionally, data sources can also contain information useful for human understanding of the datasets (largely metadata in the usual sense). Parameters needed for basic function (apart from the data itself): - Id - Type - Type-specific information necessary for data retrieval","title":"Data sources"},{"location":"tools/urbantep-visat/integration/#relations","text":"Relations, linking individual data sources to respective data templates, while not metadata structures per se, are what allows the system, thus the apps built on top of it, and thus ultimately the users to conceptually work with the underlying data according to its nature as opposed to just as datasets in a GIS application. They also drive the need for the parameters specified with each metadata type, for consistency and identifiability of each instance of them. Apart from just linking the data to its describing template, the system uses relations to make further sense in the data by utilising other complementary structures, which describe various aspects of the particular data, but mainly of the particular relation, owing to specific requirements of specific applications. That is, the same data can be described slightly differently in different contexts, which is why this is not coupled with either the data itself or the template. (In effect, these complementary structures modify the relationship between the data and the template.)","title":"Relations"},{"location":"tools/urbantep-visat/integration/#modifiers-common-metadata-objects","text":"Modifiers are items representing a metadata concept (time, place,...) common to multiple data layers (and thus usable to connect data relating to it together), or distinguishing multiple variations of the same data. These are not required if not used in the application to filter available data or for navigation, but have to be defined in such cases. Period Describes a time period for which given data is valid. It is used mainly for larger periods common to more datasets (e.g. for time series, it would be used for the whole period covered, but not for individual measurement times) Parameters needed for basic function: - Id - Period (ISO 8601) - Name - if different from period, e.g. \u2018planting season 2015\u2019. It\u2019s not necessary to provide names if they are derived from the period in a consistent way, e.g. \u201cJanuary \u201823\u201d, \u201cFebruary \u201823\u201d, etc. Place Describes a geographic area to which given data corresponds. Parameters needed for basic function: - Id - Name - Geometry (polygon, bounding box, point coordinates) Case In case the same kind of data is available e.g. for multiple different entities, those are differentiated by this data type. A case can represent any kind of subject of the data - e.g. occurrence maps may be available for different species; in that case, \u2018occurrence\u2019 would be the layer template and different species would be different cases. This is so that the system can understand that all occurrence maps are the same in nature, yet differentiate among different species. (currently under development is possibility to link to multiple cases) Parameters needed for basic function: - Id - Name The last two modifier types, scope and scenario, are largely legacy types and their use cases may be better served by different means in newer versions of the framework. Be sure to consult with us if your specific use cases might benefit from using these. Scope Scope is used for separating large amounts of data and metadata into distinct logical collections (e.g. thematic datasets, worldwide data vs. locally specific data, etc.) Parameters needed for basic function: - Id - Name Scenario While the other types describe what the data is, scenarios describe what the data could be. They are used in specific use cases when slightly different versions of the same data (relating to all the same above structures) are prepared either by alternating some aspect of the input, or some parameters of an algorithm the output of which the data is. For example, we have used scenarios for computing heat islands based on changes in several features in land cover layers, or for comparing how different tolerance values affect the results of an algorithm. Parameters needed for basic function: - Id - Name The above mentioned metadata structure types are necessary in most of the cases to enable the majority of the system to effectively work with the data. As such, they are used in the majority of cases and/or by the majority of components of the system. In addition to those, the system utilises other types, either for more specific use cases, or in specific components of the system.","title":"Modifiers (common metadata objects)"},{"location":"tools/urbantep-visat/integration/#area-trees","text":"Area trees represent administrative or statistical units, areas of interest, etc. In principle they work very similar to layer templates, with added relations between multiple levels of subdivision (granularity). Area tree The basic structure, defining a set (tree) of areas/units. This could, for example, be \u2018GADM\u2019. Each tree contains one or more levels. Parameters needed for basic function: - Id - Name Area tree level A level is in essence a specialised form of a layer template - it is connected to a data source, which can differ based on modifiers - with additional information about how individual spatial features relate to features in a higher level. An example of an area tree level would be \u2018GADM1\u2019, connected to a data source where each feature also has information about the parent feature from \u2018GADM0\u2019. Parameters needed for basic function: - Id - Name - Area tree - Level (index)","title":"Area trees"},{"location":"tools/urbantep-visat/integration/#style","text":"Describes a style, or symbology, for a layer template. For a wider feature set, we use a custom format, but for simple styling, we\u2019re able to convert to it from e.g. an SLD. Multiple styles can be used with one layer template, and one style can be used by multiple layer templates. Parameters needed for basic function: - Id - Name (if styles can be changed by the user)","title":"Style"},{"location":"tools/urbantep-visat/integration/#tag","text":"To allow for building custom structure or hierarchy upon any existing metadata structure type, any instance of those can be tagged. This enables the structure to be dynamically updated and administered. Parameters needed for basic function: - Id - Name","title":"Tag"},{"location":"tools/urbantep-visat/integration/#users","text":"Access rights are set for any metadata structure type, and for either users or user groups. The system is able to work with external authentication providers using OAuth 2 (under development). User A single user account. Parameters needed for basic function: - Id - e-mail/passphrase or external system and user identification User group A group of users, which usually holds most access rights. A user may be part of multiple groups. Parameters needed for basic function: - Id - Name URL access (under development) Apart from logged-in user accounts, access to content can be granted via a key in a link to the application. This can also be combined with standard user access rights.","title":"Users"},{"location":"tools/urbantep-visat/integration/#view","text":"Any state of the application, or partial state, can be saved, including the displayed map layers, visualisations, map position, open tool windows, etc. (under development) (The framework does not support externally loaded views).","title":"View"},{"location":"tools/urbantep-visat/examples/","text":"UrbanTEP / VISAT Integrating technological product in GREEN Engine environment Introduction The technological offer provided by GISAT is a part of data analysis and insights generation pipeline inside the GREEN Engine. The purpose of the tool and significance as a part of GREEN Engine is to support main 3 things: - Integration - Data visualization - Analysys Objective : The primary objective of UrbanTEP/VISAT within the GREEN Engine is to empower users with a versatile toolset for handling complex urban datasets. This integration supports advanced data manipulation, insightful visualization, and in-depth analysis, thereby enhancing the overall functionality and security of applications within the GREEN Engine ecosystem. Prerequisites Hosting Platform: cloud-based services, local servers. Disk Space: Minimum disk space requirements for installation and operation depends on the final product complexity Technical Capabilities: Necessary technical specifications are requiring RAM, and CPU for optimal performance. Support and Additional Resources Support Channels : GISAT technical support - andrii.khrystodulov@gisat.cz GISAT UrbanTEP product owner - tomas.soukup@gisat.cz","title":"UrbanTEP / VISAT"},{"location":"tools/urbantep-visat/examples/#urbantep-visat","text":"","title":"UrbanTEP / VISAT"},{"location":"tools/urbantep-visat/examples/#integrating-technological-product-in-green-engine-environment","text":"","title":"Integrating technological product in GREEN Engine environment"},{"location":"tools/urbantep-visat/examples/#introduction","text":"The technological offer provided by GISAT is a part of data analysis and insights generation pipeline inside the GREEN Engine. The purpose of the tool and significance as a part of GREEN Engine is to support main 3 things: - Integration - Data visualization - Analysys Objective : The primary objective of UrbanTEP/VISAT within the GREEN Engine is to empower users with a versatile toolset for handling complex urban datasets. This integration supports advanced data manipulation, insightful visualization, and in-depth analysis, thereby enhancing the overall functionality and security of applications within the GREEN Engine ecosystem.","title":"Introduction"},{"location":"tools/urbantep-visat/examples/#prerequisites","text":"Hosting Platform: cloud-based services, local servers. Disk Space: Minimum disk space requirements for installation and operation depends on the final product complexity Technical Capabilities: Necessary technical specifications are requiring RAM, and CPU for optimal performance.","title":"Prerequisites"},{"location":"tools/urbantep-visat/examples/#support-and-additional-resources","text":"Support Channels : GISAT technical support - andrii.khrystodulov@gisat.cz GISAT UrbanTEP product owner - tomas.soukup@gisat.cz","title":"Support and Additional Resources"},{"location":"tools/wordpress/","text":"WordPress Introduction Description WordPress is a versatile Content Management System (CMS). It initially gained popularity for blog creation but has since evolved into a key tool for developing a wide range of websites, including commercial ones. Its flexibility and user-friendly interface make it a go-to solution for website creation. Objective The primary objective of WordPress is to simplify the process of website creation and management. Its user-friendly design, combined with powerful features, makes it an ideal choice for users ranging from novices to seasoned web developers. WordPress aims to enhance website functionality, improve user experience, and provide robust website management tools. Features of WordPress List of Features Easy Installation, Update, and Customization : Streamlines the setup and maintenance process. Automatic System Updates : Ensures the CMS stays current and secure. Multiple Authors and User Roles : Allows for collaborative work with various permission levels. Support for Multiple Blogs : Facilitates the management of several blogs within a single site. Static Page Creation Capability : Enhances the flexibility of content management. Additional Features : Includes categorization of articles and pages, password protection, WYSIWYG editor, email publishing, import options from various platforms, auto-save drafts, comment and communication tools, permalink support, content and comment distribution via RSS and Atom, link management, multimedia file management, plugin and widget support, integrated search functionality, and theme creation system. Use Case Scenario Description A typical use case for WordPress involves creating and managing a professional blog or a commercial website. In this scenario, WordPress serves as the backbone, offering tools for content creation, theme customization, and audience engagement through comments and social media integrations. The scenario demonstrates WordPress' versatility in handling diverse web development needs. References Description For comprehensive guides and technical details about WordPress, the following resources are available: Support guides : https://wordpress.org/documentation/support-guides/ - Provides step-by-step instructions for using WordPress effectively.","title":"WordPress"},{"location":"tools/wordpress/#wordpress","text":"","title":"WordPress"},{"location":"tools/wordpress/#introduction","text":"","title":"Introduction"},{"location":"tools/wordpress/#description","text":"WordPress is a versatile Content Management System (CMS). It initially gained popularity for blog creation but has since evolved into a key tool for developing a wide range of websites, including commercial ones. Its flexibility and user-friendly interface make it a go-to solution for website creation.","title":"Description"},{"location":"tools/wordpress/#objective","text":"The primary objective of WordPress is to simplify the process of website creation and management. Its user-friendly design, combined with powerful features, makes it an ideal choice for users ranging from novices to seasoned web developers. WordPress aims to enhance website functionality, improve user experience, and provide robust website management tools.","title":"Objective"},{"location":"tools/wordpress/#features-of-wordpress","text":"","title":"Features of WordPress"},{"location":"tools/wordpress/#list-of-features","text":"Easy Installation, Update, and Customization : Streamlines the setup and maintenance process. Automatic System Updates : Ensures the CMS stays current and secure. Multiple Authors and User Roles : Allows for collaborative work with various permission levels. Support for Multiple Blogs : Facilitates the management of several blogs within a single site. Static Page Creation Capability : Enhances the flexibility of content management. Additional Features : Includes categorization of articles and pages, password protection, WYSIWYG editor, email publishing, import options from various platforms, auto-save drafts, comment and communication tools, permalink support, content and comment distribution via RSS and Atom, link management, multimedia file management, plugin and widget support, integrated search functionality, and theme creation system.","title":"List of Features"},{"location":"tools/wordpress/#use-case-scenario","text":"","title":"Use Case Scenario"},{"location":"tools/wordpress/#description_1","text":"A typical use case for WordPress involves creating and managing a professional blog or a commercial website. In this scenario, WordPress serves as the backbone, offering tools for content creation, theme customization, and audience engagement through comments and social media integrations. The scenario demonstrates WordPress' versatility in handling diverse web development needs.","title":"Description"},{"location":"tools/wordpress/#references","text":"","title":"References"},{"location":"tools/wordpress/#description_2","text":"For comprehensive guides and technical details about WordPress, the following resources are available: Support guides : https://wordpress.org/documentation/support-guides/ - Provides step-by-step instructions for using WordPress effectively.","title":"Description"},{"location":"tools/wordpress/integration/","text":"Integration WordPress + Keycloak Integration Introduction Integrating Keycloak with WordPress enhances security by implementing Single Sign-On (SSO). This integration allows users to access WordPress using Keycloak credentials, streamlining the login process and improving security. Step 1: Integrating Keycloak in WordPress Access WordPress Admin : Log into wp-admin in WordPress using credentials defined in docker-compose ( WORDPRESS_EMAIL , WORDPRESS_PASSWORD ). Install OAuth SSO Plugin : Go to 'Plugins' > 'Add New'. Search for \"OAuth Single Sign On \u2013 SSO (OAuth Client)\" and click on \"Install Now\". Activate and Configure : After installation, click \"Activate\". You\u2019ll be redirected to the configuration page. Follow the steps in the \"Setup Wizard\". For more details on Keycloak integration, visit the Keycloak WordPress Integration Documentation . Step 2: Configuring Keycloak with WordPress Ensure you have a clientid and secret from Keycloak. Follow the instructions provided in the setup wizard to integrate these credentials into WordPress. WordPress + Discourse Integration Introduction Integrating Discourse with WordPress allows seamless connectivity between WordPress posts and Discourse discussions. This integration enhances community engagement by linking WordPress content with Discourse forums. Step 1: Setting Up API Key in Discourse Admin Access : Log into Discourse as an admin. Create API Key : Navigate to /admin/api/keys . Create an API Key (User Level: \"All Users\", Scope: \"Global\") and save it for later use in WordPress. Step 2: Integrating Discourse in WordPress Access WordPress Admin : Log into wp-admin using docker-compose credentials ( WORDPRESS_EMAIL , WORDPRESS_PASSWORD ). Install WP Discourse Plugin : Go to 'Plugins' > 'Add New'. Search for \"WP Discourse\" and click \"Install Now\". Activate and Configure : After installation, click \"Activate\". A \"Discourse\" option will appear in the sidebar. Configure Plugin Settings : In the \"Connection\" section - Set the 'Discourse URL'. - Use the API Key created in Discourse. - Define the \"Publishing Username\" In the \"Publishing\" section, enable the following options - \"Default Discourse Category\" - \"Use Full Post Content\" - \"Auto Publish\" - \"Send Email Notification on Publish Failure\" - (Optional) Set \"Email Address for Failure Notification\" - \"Auto Track Published Topics\" In the \"Commenting\" tab, enable the following - [IMPORTANT] \"Enable Discourse Comments\" - \"Show Existing WP Comments\" In the \"Webhook\" tab, enable \"Sync Comment Data\". For more information on configuring the WP Discourse plugin, visit the Discourse WordPress Plugin Guide . Important Notes: Ensure to securely store and manage your API keys and credentials. Regularly update your integration plugins to maintain security and functionality. In case of any security concerns or compromised credentials, contact the respective support teams immediately.","title":"Integration"},{"location":"tools/wordpress/integration/#integration","text":"","title":"Integration"},{"location":"tools/wordpress/integration/#wordpress-keycloak-integration","text":"","title":"WordPress + Keycloak Integration"},{"location":"tools/wordpress/integration/#introduction","text":"Integrating Keycloak with WordPress enhances security by implementing Single Sign-On (SSO). This integration allows users to access WordPress using Keycloak credentials, streamlining the login process and improving security.","title":"Introduction"},{"location":"tools/wordpress/integration/#step-1-integrating-keycloak-in-wordpress","text":"Access WordPress Admin : Log into wp-admin in WordPress using credentials defined in docker-compose ( WORDPRESS_EMAIL , WORDPRESS_PASSWORD ). Install OAuth SSO Plugin : Go to 'Plugins' > 'Add New'. Search for \"OAuth Single Sign On \u2013 SSO (OAuth Client)\" and click on \"Install Now\". Activate and Configure : After installation, click \"Activate\". You\u2019ll be redirected to the configuration page. Follow the steps in the \"Setup Wizard\". For more details on Keycloak integration, visit the Keycloak WordPress Integration Documentation .","title":"Step 1: Integrating Keycloak in WordPress"},{"location":"tools/wordpress/integration/#step-2-configuring-keycloak-with-wordpress","text":"Ensure you have a clientid and secret from Keycloak. Follow the instructions provided in the setup wizard to integrate these credentials into WordPress.","title":"Step 2: Configuring Keycloak with WordPress"},{"location":"tools/wordpress/integration/#wordpress-discourse-integration","text":"","title":"WordPress + Discourse Integration"},{"location":"tools/wordpress/integration/#introduction_1","text":"Integrating Discourse with WordPress allows seamless connectivity between WordPress posts and Discourse discussions. This integration enhances community engagement by linking WordPress content with Discourse forums.","title":"Introduction"},{"location":"tools/wordpress/integration/#step-1-setting-up-api-key-in-discourse","text":"Admin Access : Log into Discourse as an admin. Create API Key : Navigate to /admin/api/keys . Create an API Key (User Level: \"All Users\", Scope: \"Global\") and save it for later use in WordPress.","title":"Step 1: Setting Up API Key in Discourse"},{"location":"tools/wordpress/integration/#step-2-integrating-discourse-in-wordpress","text":"Access WordPress Admin : Log into wp-admin using docker-compose credentials ( WORDPRESS_EMAIL , WORDPRESS_PASSWORD ). Install WP Discourse Plugin : Go to 'Plugins' > 'Add New'. Search for \"WP Discourse\" and click \"Install Now\". Activate and Configure : After installation, click \"Activate\". A \"Discourse\" option will appear in the sidebar. Configure Plugin Settings : In the \"Connection\" section - Set the 'Discourse URL'. - Use the API Key created in Discourse. - Define the \"Publishing Username\" In the \"Publishing\" section, enable the following options - \"Default Discourse Category\" - \"Use Full Post Content\" - \"Auto Publish\" - \"Send Email Notification on Publish Failure\" - (Optional) Set \"Email Address for Failure Notification\" - \"Auto Track Published Topics\" In the \"Commenting\" tab, enable the following - [IMPORTANT] \"Enable Discourse Comments\" - \"Show Existing WP Comments\" In the \"Webhook\" tab, enable \"Sync Comment Data\". For more information on configuring the WP Discourse plugin, visit the Discourse WordPress Plugin Guide . Important Notes: Ensure to securely store and manage your API keys and credentials. Regularly update your integration plugins to maintain security and functionality. In case of any security concerns or compromised credentials, contact the respective support teams immediately.","title":"Step 2: Integrating Discourse in WordPress"},{"location":"tools/wordpress/examples/","text":"WordPress, Keycloak, and Discourse Integration Guide Introduction This guide details the process of deploying WordPress, Keycloak, and Discourse using Docker, and integrating them for a seamless user experience. This setup utilizes Keycloak for Single Sign-On (SSO) across WordPress and Discourse, enhancing security and streamlining user management. Prerequisites Docker and Docker Compose installed. Familiarity with Docker, YAML syntax, and basic network configurations. Access to a SMTP service for email functionalities. Docker Compose Configuration The provided docker-compose.yml file outlines the configuration for deploying WordPress, Keycloak, and Discourse. The structure includes: WordPress Service : Utilizes the bitnami/wordpress image, configured for web content management. MariaDB Service : A database for WordPress. Keycloak Database : PostgreSQL database dedicated to Keycloak. Keycloak Service : Manages SSO and user authentication. Discourse Services : Includes postgresqldiscourse , redis , discourse , and sidekiq for the forum platform. Deploying the Services Download or Clone the Docker Compose File : Obtain the provided docker-compose.yml . Configure Environment Variables : Replace placeholders in the file with actual values for SMTP, database credentials, and other settings. Run the Services : Execute docker-compose up in the directory containing the Docker Compose file. Access Services : WordPress: Accessible at http://localhost:9000 . Discourse: Accessible at http://localhost:3000 . Keycloak: Accessible at http://localhost:8080 . Integrating Keycloak with WordPress and Discourse WordPress + Keycloak Install OAuth SSO Plugin in WordPress : Access WordPress admin panel. Navigate to 'Plugins' > 'Add New'. Search and install \"OAuth Single Sign On \u2013 SSO (OAuth Client)\". Configure Keycloak Integration : Use Keycloak's clientid and secret . Follow the setup wizard in WordPress for Keycloak integration. Discourse + Keycloak Enable Keycloak SSO in Discourse : Access the Discourse container's command line. Install the OpenID Connect plugin and configure it to use Keycloak as the SSO provider. Configure API Key in Discourse : Generate an API key in Discourse admin panel for integration purposes. WordPress + Discourse Install WP Discourse Plugin in WordPress : Access WordPress admin panel. Navigate to 'Plugins' > 'Add New'. Search and install \"WP Discourse\". Configure Discourse Integration : Use the Discourse API key. Set up publishing and commenting settings in WordPress to synchronize with Discourse. Example of Post Integration from WordPress to Discourse This example demonstrates the seamless integration of posting on WordPress and how it reflects on Discourse. First, we create a post in WordPress. As soon as the post is published, it automatically generates a corresponding thread in Discourse. Now, the post made in WordPress can be viewed as a thread in Discourse. Additionally, users can engage with the post by commenting directly in the Discourse thread. Additional Configuration and Troubleshooting Email Setup : Ensure SMTP settings are correctly configured in both WordPress and Discourse for email functionalities. Security and Data Backup : Regularly update services and back up data. Troubleshooting : Consult the official documentation of WordPress, Keycloak, and Discourse for specific issues. Support and Additional Resources Community Forums : Engage with community forums for WordPress, Keycloak, and Discourse for support. External Documentation : Refer to the official documentation of each tool for detailed guides and updates. Note : Always ensure that you are working with the latest versions of the software and following the best practices for security and maintenance.","title":"WordPress, Keycloak, and Discourse Integration Guide"},{"location":"tools/wordpress/examples/#wordpress-keycloak-and-discourse-integration-guide","text":"","title":"WordPress, Keycloak, and Discourse Integration Guide"},{"location":"tools/wordpress/examples/#introduction","text":"This guide details the process of deploying WordPress, Keycloak, and Discourse using Docker, and integrating them for a seamless user experience. This setup utilizes Keycloak for Single Sign-On (SSO) across WordPress and Discourse, enhancing security and streamlining user management.","title":"Introduction"},{"location":"tools/wordpress/examples/#prerequisites","text":"Docker and Docker Compose installed. Familiarity with Docker, YAML syntax, and basic network configurations. Access to a SMTP service for email functionalities.","title":"Prerequisites"},{"location":"tools/wordpress/examples/#docker-compose-configuration","text":"The provided docker-compose.yml file outlines the configuration for deploying WordPress, Keycloak, and Discourse. The structure includes: WordPress Service : Utilizes the bitnami/wordpress image, configured for web content management. MariaDB Service : A database for WordPress. Keycloak Database : PostgreSQL database dedicated to Keycloak. Keycloak Service : Manages SSO and user authentication. Discourse Services : Includes postgresqldiscourse , redis , discourse , and sidekiq for the forum platform.","title":"Docker Compose Configuration"},{"location":"tools/wordpress/examples/#deploying-the-services","text":"Download or Clone the Docker Compose File : Obtain the provided docker-compose.yml . Configure Environment Variables : Replace placeholders in the file with actual values for SMTP, database credentials, and other settings. Run the Services : Execute docker-compose up in the directory containing the Docker Compose file. Access Services : WordPress: Accessible at http://localhost:9000 . Discourse: Accessible at http://localhost:3000 . Keycloak: Accessible at http://localhost:8080 .","title":"Deploying the Services"},{"location":"tools/wordpress/examples/#integrating-keycloak-with-wordpress-and-discourse","text":"","title":"Integrating Keycloak with WordPress and Discourse"},{"location":"tools/wordpress/examples/#wordpress-keycloak","text":"Install OAuth SSO Plugin in WordPress : Access WordPress admin panel. Navigate to 'Plugins' > 'Add New'. Search and install \"OAuth Single Sign On \u2013 SSO (OAuth Client)\". Configure Keycloak Integration : Use Keycloak's clientid and secret . Follow the setup wizard in WordPress for Keycloak integration.","title":"WordPress + Keycloak"},{"location":"tools/wordpress/examples/#discourse-keycloak","text":"Enable Keycloak SSO in Discourse : Access the Discourse container's command line. Install the OpenID Connect plugin and configure it to use Keycloak as the SSO provider. Configure API Key in Discourse : Generate an API key in Discourse admin panel for integration purposes.","title":"Discourse + Keycloak"},{"location":"tools/wordpress/examples/#wordpress-discourse","text":"Install WP Discourse Plugin in WordPress : Access WordPress admin panel. Navigate to 'Plugins' > 'Add New'. Search and install \"WP Discourse\". Configure Discourse Integration : Use the Discourse API key. Set up publishing and commenting settings in WordPress to synchronize with Discourse.","title":"WordPress + Discourse"},{"location":"tools/wordpress/examples/#example-of-post-integration-from-wordpress-to-discourse","text":"This example demonstrates the seamless integration of posting on WordPress and how it reflects on Discourse. First, we create a post in WordPress. As soon as the post is published, it automatically generates a corresponding thread in Discourse. Now, the post made in WordPress can be viewed as a thread in Discourse. Additionally, users can engage with the post by commenting directly in the Discourse thread.","title":"Example of Post Integration from WordPress to Discourse"},{"location":"tools/wordpress/examples/#additional-configuration-and-troubleshooting","text":"Email Setup : Ensure SMTP settings are correctly configured in both WordPress and Discourse for email functionalities. Security and Data Backup : Regularly update services and back up data. Troubleshooting : Consult the official documentation of WordPress, Keycloak, and Discourse for specific issues.","title":"Additional Configuration and Troubleshooting"},{"location":"tools/wordpress/examples/#support-and-additional-resources","text":"Community Forums : Engage with community forums for WordPress, Keycloak, and Discourse for support. External Documentation : Refer to the official documentation of each tool for detailed guides and updates. Note : Always ensure that you are working with the latest versions of the software and following the best practices for security and maintenance.","title":"Support and Additional Resources"}]}