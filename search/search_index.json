{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Technical Guide for GREENGAGE Project Integration Project The GREENGAGE project is a three-year-long pan-European initiative aimed at strengthening urban governance through citizen-based co-creation and crowd-sourcing. Funded under the Horizon Europe Framework Programme and co-funded by the UK government and Switzerland's State Secretariat for Education, Research and Innovation, this project is led by the Austrian Institute of Technology and comprises 17 research and industry partners from the EU and the UK. The project seeks to encourage innovative governance processes and assist public authorities in formulating climate mitigation and adaptation policies. By leveraging citizen participation and providing innovative digital solutions, GREENGAGE lays the groundwork for co-creating and co-designing innovative solutions to monitor environmental issues at the grassroots level. The developed Citizen Observatories (CO) are intended to be further integrated into urban planning decision-making processes. The project encompasses campaigns in four pilot cities across Europe, focusing on mobility, air quality, and healthy living, with citizens encouraged to observe and co-create data solutions for urban environments through Citizen Observatories. Tools The tools directory contains subdirectories for each tool utilized in the project. Each subdirectory includes a detailed description of the tool, its functionality, and its role within the project. Keycloak Directory : /tools Link : tools/keycloak Deployed in : https://auth1.demo.greengage-project.eu Description : Keycloak is an open-source Identity and Access Management solution aimed at modern applications and services. It facilitates the secure transmission of identity information between applications and services, even across different domains. Examples of programmatic integration/usage : Node Python Collaborative Environment Directory : /tools Link : tools/collaborativeEnvironment Deployed in : https://demo.greengage-project.eu Description : The Collaborative Environment is a comprehensive platform designed to facilitate co-production, asset management, and enhanced collaboration among users. It incorporates various interlinkers and modules to streamline workflows and improve organizational processes, supporting integration with different technologies and frameworks. Examples of programmatic integration/usage : Deploy example MODE Directory : /tools Link : tools/mode Description : This module allows to determine the transport mode of a user based on GPS data provided. Examples of programmatic integration/usage : iOS example Android example Backend example Discourse Directory : /tools Link : tools/Discourse Deployed in : https://discourse.16.171.94.204.nip.io Description : Discourse is a modern, open-source discussion platform designed for building and managing community forums. Known for its user-friendly interface and robust features, it supports real-time discussions, multimedia integration, and comprehensive moderation tools. Ideal for a wide range of communities, from small groups to large enterprises, Discourse fosters engagement, knowledge sharing, and collaboration. Examples of programmatic integration/usage : Deploy example DataHub Directory : /tools Link : tools/DataHub Deployed in : https://datahub.16.171.94.204.nip.io Description : DataHub is an open-source metadata platform designed to democratise data discovery. It acts as a centralised system for companies and teams to track, manage, and find data within their organisation. Its primary aim is to streamline the process of accessing and understanding data assets, making it easier for teams to derive insights and make informed decisions. Examples of programmatic integration : Integration example Apache Superset Directory : /tools Link : tools/Superset Deployed in : https://superset.16.171.94.204.nip.io Description : Apache Superset is a sophisticated, open-source data exploration and visualisation platform designed to transform the way analysts and businesses interact with their data. As a versatile web application, it empowers users with tools for deep data exploration and the creation of beautifully rendered visualisations and dashboards. Superset is renowned for its user-friendly interface, democratising data analysis by enabling users of all skill levels to engage with complex datasets, design compelling visual narratives, and derive actionable insights. Offering a rich array of features, including a variety of chart types, interactive dashboards, and an integrated SQL editor, Apache Superset stands out as a comprehensive solution for modern data analytics and business intelligence needs. Examples of integration of sources with Superset : Integration example Apache Druid Directory : /tools Link : tools/Druid Deployed in : https://druid.16.171.94.204.nip.io Description : Apache Druid is a high-performance, distributed data store designed for real-time analytics on large-scale datasets. It excels in scenarios where fast queries and ingest are crucial, such as in business intelligence applications, operational analytics, and real-time monitoring. Druid is known for its ability to handle high concurrency, its columnar storage format, and its ability to scale horizontally. It supports streaming and batch data ingestion, and its architecture allows for efficient data summarisation, fast aggregation queries, and low-latency data access. Druid is often used in conjunction with big data technologies like Hadoop and Kafka, making it a popular choice for organisations looking to implement real-time analytics solutions. Examples of integration of sources with Druid : Integration example Apache NiFi Directory : /tools Link : tools/NiFi Deployed in : https://nifi.16.171.94.204.nip.io Description : Apache NiFi is a robust, open-source software solution designed for efficient and reliable data ingestion, processing, and distribution. It is particularly noted for its user-friendly graphical interface that simplifies the design, management, and monitoring of data flows. NiFi supports highly configurable and flexible data routing, system mediation, and transformation capabilities, enabling the handling of diverse data formats and systems. Key features include its scalability to accommodate large volumes of data, built-in data security measures, and extensive audit trails for data provenance. Primarily used in scenarios requiring complex data pipelines, such as real-time data processing and integration across varied systems, Apache NiFi stands out for its ease of use and comprehensive data management capabilities. Examples of integration of sources with NiFi : Integration example Libelium MQTT sensor integration Directory : /tools Link : tools/sensorsIntegration Description : This documentation contains all the information necessary to connect and configure the IoT sensors provided by Libelium to MQTT broker and integrate it in other platforms or architectures. The above README provides a structured outline to document the technical aspects of the GREENGAGE project, offering a solid starting point to further elaborate on the tools and processes involved in the project. Greengage APP Directory : /tools Link : tools/greengage-app-api Description : The greengage app api is mesh of multiple services which are provided via a single gateway. This underlying docuemntation will show you in a simple way how you can interact with application and how you can adopt your application to fit in the ecosystem. Wordpress Directory : /tools Link : tools/wordpress Description : WordPress is a highly popular and versatile Content Management System (CMS) renowned for its ease of use and flexibility. It's an ideal platform for creating a wide range of websites, from personal blogs to large corporate portals. WordPress is known for its extensive theme and plugin ecosystem, which allows users to customize their sites to fit their specific needs and enhance functionality. Examples of programmatic integration/usage : With Keycloak With Discourse Source code in: https://github.com/Greengage-project/Documentation","title":"Home"},{"location":"#technical-guide-for-greengage-project-integration","text":"","title":"Technical Guide for GREENGAGE Project Integration"},{"location":"#project","text":"The GREENGAGE project is a three-year-long pan-European initiative aimed at strengthening urban governance through citizen-based co-creation and crowd-sourcing. Funded under the Horizon Europe Framework Programme and co-funded by the UK government and Switzerland's State Secretariat for Education, Research and Innovation, this project is led by the Austrian Institute of Technology and comprises 17 research and industry partners from the EU and the UK. The project seeks to encourage innovative governance processes and assist public authorities in formulating climate mitigation and adaptation policies. By leveraging citizen participation and providing innovative digital solutions, GREENGAGE lays the groundwork for co-creating and co-designing innovative solutions to monitor environmental issues at the grassroots level. The developed Citizen Observatories (CO) are intended to be further integrated into urban planning decision-making processes. The project encompasses campaigns in four pilot cities across Europe, focusing on mobility, air quality, and healthy living, with citizens encouraged to observe and co-create data solutions for urban environments through Citizen Observatories.","title":"Project"},{"location":"#tools","text":"The tools directory contains subdirectories for each tool utilized in the project. Each subdirectory includes a detailed description of the tool, its functionality, and its role within the project.","title":"Tools"},{"location":"#keycloak","text":"Directory : /tools Link : tools/keycloak Deployed in : https://auth1.demo.greengage-project.eu Description : Keycloak is an open-source Identity and Access Management solution aimed at modern applications and services. It facilitates the secure transmission of identity information between applications and services, even across different domains. Examples of programmatic integration/usage : Node Python","title":"Keycloak"},{"location":"#collaborative-environment","text":"Directory : /tools Link : tools/collaborativeEnvironment Deployed in : https://demo.greengage-project.eu Description : The Collaborative Environment is a comprehensive platform designed to facilitate co-production, asset management, and enhanced collaboration among users. It incorporates various interlinkers and modules to streamline workflows and improve organizational processes, supporting integration with different technologies and frameworks. Examples of programmatic integration/usage : Deploy example","title":"Collaborative Environment"},{"location":"#mode","text":"Directory : /tools Link : tools/mode Description : This module allows to determine the transport mode of a user based on GPS data provided. Examples of programmatic integration/usage : iOS example Android example Backend example","title":"MODE"},{"location":"#discourse","text":"Directory : /tools Link : tools/Discourse Deployed in : https://discourse.16.171.94.204.nip.io Description : Discourse is a modern, open-source discussion platform designed for building and managing community forums. Known for its user-friendly interface and robust features, it supports real-time discussions, multimedia integration, and comprehensive moderation tools. Ideal for a wide range of communities, from small groups to large enterprises, Discourse fosters engagement, knowledge sharing, and collaboration. Examples of programmatic integration/usage : Deploy example","title":"Discourse"},{"location":"#datahub","text":"Directory : /tools Link : tools/DataHub Deployed in : https://datahub.16.171.94.204.nip.io Description : DataHub is an open-source metadata platform designed to democratise data discovery. It acts as a centralised system for companies and teams to track, manage, and find data within their organisation. Its primary aim is to streamline the process of accessing and understanding data assets, making it easier for teams to derive insights and make informed decisions. Examples of programmatic integration : Integration example","title":"DataHub"},{"location":"#apache-superset","text":"Directory : /tools Link : tools/Superset Deployed in : https://superset.16.171.94.204.nip.io Description : Apache Superset is a sophisticated, open-source data exploration and visualisation platform designed to transform the way analysts and businesses interact with their data. As a versatile web application, it empowers users with tools for deep data exploration and the creation of beautifully rendered visualisations and dashboards. Superset is renowned for its user-friendly interface, democratising data analysis by enabling users of all skill levels to engage with complex datasets, design compelling visual narratives, and derive actionable insights. Offering a rich array of features, including a variety of chart types, interactive dashboards, and an integrated SQL editor, Apache Superset stands out as a comprehensive solution for modern data analytics and business intelligence needs. Examples of integration of sources with Superset : Integration example","title":"Apache Superset"},{"location":"#apache-druid","text":"Directory : /tools Link : tools/Druid Deployed in : https://druid.16.171.94.204.nip.io Description : Apache Druid is a high-performance, distributed data store designed for real-time analytics on large-scale datasets. It excels in scenarios where fast queries and ingest are crucial, such as in business intelligence applications, operational analytics, and real-time monitoring. Druid is known for its ability to handle high concurrency, its columnar storage format, and its ability to scale horizontally. It supports streaming and batch data ingestion, and its architecture allows for efficient data summarisation, fast aggregation queries, and low-latency data access. Druid is often used in conjunction with big data technologies like Hadoop and Kafka, making it a popular choice for organisations looking to implement real-time analytics solutions. Examples of integration of sources with Druid : Integration example","title":"Apache Druid"},{"location":"#apache-nifi","text":"Directory : /tools Link : tools/NiFi Deployed in : https://nifi.16.171.94.204.nip.io Description : Apache NiFi is a robust, open-source software solution designed for efficient and reliable data ingestion, processing, and distribution. It is particularly noted for its user-friendly graphical interface that simplifies the design, management, and monitoring of data flows. NiFi supports highly configurable and flexible data routing, system mediation, and transformation capabilities, enabling the handling of diverse data formats and systems. Key features include its scalability to accommodate large volumes of data, built-in data security measures, and extensive audit trails for data provenance. Primarily used in scenarios requiring complex data pipelines, such as real-time data processing and integration across varied systems, Apache NiFi stands out for its ease of use and comprehensive data management capabilities. Examples of integration of sources with NiFi : Integration example","title":"Apache NiFi"},{"location":"#libelium-mqtt-sensor-integration","text":"Directory : /tools Link : tools/sensorsIntegration Description : This documentation contains all the information necessary to connect and configure the IoT sensors provided by Libelium to MQTT broker and integrate it in other platforms or architectures. The above README provides a structured outline to document the technical aspects of the GREENGAGE project, offering a solid starting point to further elaborate on the tools and processes involved in the project.","title":"Libelium MQTT sensor integration"},{"location":"#greengage-app","text":"Directory : /tools Link : tools/greengage-app-api Description : The greengage app api is mesh of multiple services which are provided via a single gateway. This underlying docuemntation will show you in a simple way how you can interact with application and how you can adopt your application to fit in the ecosystem.","title":"Greengage APP"},{"location":"#wordpress","text":"Directory : /tools Link : tools/wordpress Description : WordPress is a highly popular and versatile Content Management System (CMS) renowned for its ease of use and flexibility. It's an ideal platform for creating a wide range of websites, from personal blogs to large corporate portals. WordPress is known for its extensive theme and plugin ecosystem, which allows users to customize their sites to fit their specific needs and enhance functionality. Examples of programmatic integration/usage : With Keycloak With Discourse","title":"Wordpress"},{"location":"#source-code-in-httpsgithubcomgreengage-projectdocumentation","text":"","title":"Source code in: https://github.com/Greengage-project/Documentation"},{"location":"HOWTO%20Thematic%20co-explorations/","text":"HOWTO Thematic co-exploration This document describes the collaborative process, i.e. set of steps that should be carried out in a logical sequence to co-deliver a thematic co-exploration. Importantly, these steps are indicative and there could be different journeys through them, i.e. some may be skipped and some new steps added if the co-producers of a thematic co-exploration so consider. Besides, a thematic co-exploration is a collaborative and iterative process, i.e. several iterations over some of the steps of the collaborative process may be needed. Indeed, the process indicated below should not be understood as a one way rigid top down process but as an iterative process where some steps may be executed more than once. Steps to follow to co-deliver thematic co-exploration 0. Choose thematic co-exploration area and define research questions&hypothesis to be validated to enable policy-making 1. Preparation of the co-production process, teams and community dialogue mechanism 2. Invitation, onboarding and training of selected users to take part in a Citizen Observatory\u2019s thematic co-exploration 3. Inventory of available datasets and data gaps identification to explore a problem domain 4. Co-preparation of the thematic co-exploration to make it ready for launch 5. Management of data workflow process for the thematic co-exploration 5a. Gather Copernicus and Open Data from government datasets to gain bird's eye view for thematic co-exploration 5b. Organise and gather crowdsourcing of data by citizens through data capture apps 6. Cataloguing and storage of captured data 7. Generation of data analysis workflows (Data cleaning, pipelining and analysis) 8. Generation of thematic co-exploration dashboards) 9. Generation of storylines for wide dissemination) 10. Generation of policy briefs' inputs for policy makers) Step 0: Choose thematic co-exploration area and define research questions&hypothesis to be validated to enable policy-making Responsible GREENGAGE Observers (GOs) and PST members Objective The primary objective of this step is to formulate precise research questions and hypotheses that are anchored in the needs and priorities of policy-making for a given area where thematic co-exploration could be advisable. Guided by pilot owners who are part of the PST team and know well what are the priorities of a given Citizen Observatory, an area of itnerest to research through a thematic co-exploration is chosen. The devised research questions and hypotheses should be actionable, measurable, and capable of being empirically tested within the thematic co-exploration\u2019sscope. They should be designed to fill knowledge gaps, address current challenges, and generate evidence to translate into effective policy interventions. This step will involve a thorough analysis of the current policy landscape, consultation with stakeholders of the GREENGAGE Observatory, and a review of the existing literature to ensure that the research is targeted and relevant. This foundational step sets the stage for the subsequent research and data collection efforts, ensuring that the thematic co-exploration\u2019s outcomes align with the strategic goals of influencing and enabling policy-making. How can we achieve it? After having chosen an area of high priority to tackle a thematic co-exploration, guided by the Thematic Co-Exploration for Citizen Observatory (COb) Specification document, a group of GREENGAGE Observers (GObs), citizens, civil servants and other stakeholders, who will manage and moderate the thematic co-exploration, answer, in collaboration with owners of the Citizen Observatory, the following core questions: WHY \u2013 Reason why this Citizen Observatory\u2019 thematic co-exploration is needed (arguments for promoting the execution of this Citizen Observatory\u2019s campaign) WHO \u2013 Involved and affected stakeholders\u2019 groups in Citizen Observatory\u2019s thematic co-exploration (describe the target groups and their possible motivation) WHAT \u2013 Actual endeavours of the Citizen Observatory\u2019s thematic co-exploration (describe what will exactly be done in this Citizen Observatory) WHEN \u2013 Planning of activities and period when Citizen Observatory\u2019s thematic co-exploration will be executed (indicate for how long, what activities, where and for whom will be realized) WHERE \u2013 Geographical locations where Citizen Observatory\u2019s thematic co-exploration will take place (actual geographical areas where data collection and analysis will be carried out) WHICH \u2013 Materials and resources (actual materials and resources needed to execute the Citizen Observatory\u2019s thematic co-exploration) HOW \u2013 Data analysis process to be able to capture, analyse and generate indicators and visualizations sought in Citizen Observatory\u2019s thematic co-exploration The Thematic Co-Exploration for Citizen Observatory (COb) Specification document does not need to be completed thoroughly in this step. It is paramount that the first questions of this document are filled in. It is the WHAT question the one that actually addresses the following aspects essential to start with the co-production of a thematic co-exploration: The problem (describe the problem statement and the challenge that you are addressing) Objectives (describe the intended result) Added value (describe the potential benefits for the Citizen Observatory\u2019s thematic co-exploration\u2019s stakeholders) Current and desired situation (describe the current approach including existing practices, and the desired situation) CS hypothesis and research questions (to be validated by the outcomes of the execution of the Citizen Observatory\u2019s thematic co-exploration) Metrics definition (of indicators of success for the Citizen Observatory\u2019s thematic co-exploration) Notice that some additional optional resources are listed below which could be used before filling in the thematic co-exploration\u2019s specification, to help narrowing the problem and reflecting about the suitability of the problem to be addressed through a thematic co-exploration. Resources Citizen Science problem statement analysis \u2013 this resource allows to perform a minimal viability analysis to determine whether a problem that wants to be tackled is approachable through Citizen Science or not. Questionnaire to analysise suitability of the problem for co-production of CS experiment * this second resource can help you dilucidate whether a collaborative process can be organized or not, to address a given problem, through a Citizen Science experiment. Thematic Co-Exploration for GREENGAGE Observatory (GO) Specification Template \u2013 this resource is the most important one in this stage since it allows to answer the WHY, WHO, WHAT, WHEN, WHERE, with WHICH and HOW questions associated to the organization and planning of a thematic co-exploration. It is designed to compile all the information needed to get started with a thematic co-exploration. It gets information from the CONCEPTUALISING and PREPARING phases of the GREENGAGE methodology for Citizen Observatories. Step 1: Preparation of the co-production process, teams and community dialogue mechanism Responsible GREENGAGE Observers (GOs) and PST members Objective The primary objective of this step is to establish a robust and effective co-production process in GREENGAGE\u2019s Collaborative Environment , that fosters collaborative engagement between various stakeholders within the GREENGAGEObservatory (GO)\u2019s thematic co-exploration. This involves the strategic configuration of various community teams to ensure a diverse representation of perspectives, skills, and expertise. The aim is to create a digital collaboration space where community members, experts, and stakeholders can collectively contribute to the co-production process underlying every thematic co-exploration, leveraging their unique insights and experiences. This process is designed to facilitate the seamless integration of community input into the thematic co-exploration and empower participants by providing them with the tools, training, and support, in the form of GO Enablers, necessary for meaningful participation. The ultimate goal is to enhance the quality, relevance, and impact of the thematic co-exploration\u2019s outcomes through a participatory approach that values and utilises the collective knowledge and capabilities of the GO\u2019s community teams, guided and supported by Pilot Support Team (PST) members, who coach and support them during the usage of technologies and application of the different stages part of the Citizen Science loop. How can we achieve it? To facilitate discussions and community engagement, Discourse will be integrated, providing a modern and user-friendly forum for exchange and collaboration. In parallel, WordPress will serve as our content management system, offering a versatile platform for information dissemination and interaction with the broader community. Finally and, most importantly, a new co-production process will be set up in GREENGAGE Collaborative Environment. Hence, the accomplishment of this step requires usage of all the tools belonging to the \u201cCommunity and co-production process management\u201d layer of GREEN Engine. A new entry should be created within the parent pilot / Citizen Observatory page, for the newly specified thematic co-exploration. The instructions to use Wordpress should be followed to achieve this. A new set of categories for topics of discussion following the instructions of usage of the Discourse tool , should be created by the GObs leading and managing the thematic co-exploration. This will serve to foster the collaboration and dialogue among participants in the thematic co-exploration. A new co-production process based on the \u201cGREENGAGE way\u201d co-production process schema or blueprint already loaded into Collaborative Environment should be instantiated. Resources WordPress \u2013 HOWTO for tool essential to communicate the existance of a thematic co-exploration and allow dissemination of its results to the broader community* Discourse \u2013 HOWTO for tool necessary to foster discussions and dialogue among GObs and allow them to request help and support each other Collaborative Environment \u2013 HOWTO for essential tool to govern, guide and trace the co-production process executed during the co-design and co-delivery of the thematic co-exploration CS-CO-schema-process-specification \u2013 to specify the process of the thematic co-exploration ala \u201cGREENGAGE way\u201d Problem Statement Questionnaire document \u2013 after instantiation of the co-production proces for the the thematic co-exploration, this main outcome of stage #0 will be uploaded into the new process. Important to review and further complete questions WHO, WHEN, WHERE and WHICH which will be tackled in the following steps to co-produce the thematic co-exploration. Stakeholder Mapping Canvas \u2013 document to help you understand who are the stakeholders of your thematic co-exploration Step 2: Invitation, onboarding and training of selected users to take part in a Citizen Observatory\u2019s thematic co-exploration Responsible Pilot support team members Objective A thematic co-exploration is structured to engage and empower citizens, enabling their active involvement in data collection, observation, and analysis. It strives to ensure that contributions from citizens are meaningful and pivotal in addressing key research questions. This step assumes that the following questions have been previously been answered when tackling the WHO question of the Problem Statement Questionnaire document: Who is the promoter and/or sponsor of this Citizen Observatory\u2019s thematic co-exploration? Who are responsible for the Citizen Observatory\u2019s thematic co-exploration ? Who are the domain experts and leaders of the Citizen Observatory\u2019s community to be involved in this thematic co-exploration? Who will be the participants in this CS campaign of the Citizen Observatory and what will be their collaborative duties ? What social collectives, influencers, societal representatives could help you disseminating the outcomes of this Citizen Observatory\u2019s thematic co-exploration? Who will be the multi stakeholder members affected by the Citizen Observatory\u2019s thematic co-exploration\u2019s results ? How can we achieve it? To achieve the objectives, the Collaborative Environment will be the key to enhanced collaboration and asset management and will be instrumental in coordinating the efforts of different stakeholders and managing campaign assets effectively. Indeed, different teams of stakeholder types will be configured in the environment and granted different rights over the phases, objectives and tasks of the co-production process. The thematic co-exploration promoters will co-design different communication assets based on the templates in the form of knowledge assets or GO enablers already loaded into the Collaborative Environment. For community engagement and discussion, Discourse will serve as our primary platform, fostering a space for open, structured communication among participants. Groups for each co-producer team will be assembled in the tool. Several topics to foster onboarding and training for the new thematic co-exploration will be promoted through Discourse. Training materials for the following topics have been made ready by the project and should be extended and customized for each thematic co-exploration: Data I Data II Co-design I Co-design II Co-design III Air quality & Mobility Monitoring Technology I Technology II Technology III Policy-making & Governance Analysis & Visualisation Resources Materials for thematic co-exploration and citizen onboarding \u2013 TO BE PREPARED Folder where trainings are currently uploaded which will be published as part of the Knowledge Base aka GREENGAGE Academy of the main project\u2019s website Step 3: Inventory of available datasets and data gaps identification to explore a problem domain Responsible: PST members and GREENGAGE Observers (GOs) Objective The primary goal of this step is to establish a comprehensive and organised system for managing datasets relevant to a thematic co-exploration. By requiring pilot owners to register existing and forthcoming datasets, we aim to create a centralised metadata repository that will serve as a foundational resource for the technical team being able to access, explore and analyse the data to be compiled for the thematic co-exploration. This registration process will facilitate the subsequent uploading and integration of these datasets into the GREENGAGE infrastructure for its usage by the thematic co-explorations of a given Citizen Observatory. The aim of this step is to develop a detailed and accessible catalogue of data, for the whole Citizen Observatory and where datasets will be categorized according to the thematic co-exploration they belong to, which will so streamline thematic co-exploration\u2019s operations and enhance the efficacy of data analysis and decision-making processes. This initiative is instrumental in ensuring that all stakeholders have a clear and consistent understanding of the available data resources, thereby optimising the utilisation and impact of the data within a given Citizen Observatory\u2019s s scope. Importantly, during the preparation of each thematic co-exploration, GObs will also contribute with new entries, categorized and linked to such thematic co-exploration, within the centralised catalogue of datasets in a Citizen Observatory. How can we achieve it? To achieve the following objective, PST members should comprehensively identify all existing and potential datasets within the scope of the Citizen Observatory and, particularly, those needed for the thematic co-exploration considered. This step aims to create an inventory of datasets, categorising them based on the current thematic co-exporation\u2019s ID, relevance, data type, source, and intended use. Furthermore, it is essential to specify how each dataset may be retrieved and accessed. This inventory serves as the foundation for the dataset metadata registration process in DataHub. Population of Dataset with the newly created inventory is the final action within this step. Resources Dataset Inventory template \u2013 knowledge asset with fields to identify every dataset\u2019s metadata that will be needed for the thematic co-explorations in each Citizen Observatory. It is recommended that for each thematic co-exploration a a copy of this template is performed and filled in. DataHub \u2013 HOWTO for tool used to register, search and view metatada about what datasets of a given thematic area are already available in the catalogue of such Citizen Observatory. The dataset inventory template completed for a given thematic co-exploration shoudl be used as input to complete the datasets metadata in DataHub. Step 4: Co-preparation of the thematic co-exploration to make it ready for launch Responsible PST members and GREENGAGE Observers (GOs) Objective The core objective of this task is to get the community of observers, their collaboration process management, the tools and reources needed for the succesful execution of the thematic co-exploration ready for operation. It will entail completing the following steps: Select GREENGAGE tools and knowledge assets to co-produce thematic co-exploration Agree on indicators, metrics and methods for measurement needed Define data protocol to be used in thematic co-exploration Adapt selected tools for data management Adapt selected communication and evaluation materials Test process and community management tools Test data capture tools and instruments Test data analysis tools and visualizations Test monitoring and evaluation instruments Test community support mechanisms Report lessons learnt and improvements needed detected in the preparation process Planning of work activities and engagement events for execution and evaluation Once all the steps above listed are completed, it can be considered that the GO\u2019s thematic co-exploration is ready for operation and evaluation. Central to GREENGAGE is the utilisation of innovative digital tools and platforms, which are devised for efficient data collection, management, and analysis. Besides, in GREENGAGE it is aimed to foster a community spirit and shared purpose among participants, thereby enhancing their understanding and engagement in scientific research. This approach is expected to yield reliable and high-quality data, instrumental in influencing policy-making, supporting community initiatives, and furthering scientific inquiries. Additionally, the CS campaign or thematic co-exploration co-designed should be scalable, replicable, and adaptable to a variety of contexts and research domains, setting a model for future citizen science endeavours. How can we achieve it? To realize this step, the Collaborative Environment (CE) and the \u201cGREENGAGE way\u201d blueprint instantiated for the thematic co-exploration, should guide the interaction and management of the campaign's activities. In this process, Discourse will also be used for community engagement and open communication, facilitating a structured and inclusive dialogue among participants. For data collection, tools such as GREENGAGE app, MindEarth app, or sensors for environmental monitoring will be employed. The data collected will be organized and analyzed using DataHub, Apache Superset, and Apache Druid for advanced data storage, analysis and visualization. Apache NiFi will be instrumental in ensuring a streamlined and efficient data processing pipelines. The synergy of these tools and platforms, as specified in our technical guide (layers \u201cCommunity and co-production process management\u201d, \u201cData crowdsourcing and capture\u201d and \u201cData analysis and insights generation\u201d), will ensure a comprehensive, secure, and effective Citizen Science campaign, addressing the research questions while empowering citizen participants. Once more, the Problem Statement Questionnaire document should be revisited. This time special attention should be paid to sections: WHEN \u2013 Planning of activities and period when Citizen Observatory\u2019s thematic co-exploration will be executed WHERE \u2013 Geographical locations where Citizen Observatory\u2019s thematic co-exploration will take place (actual geographical areas where data collection and analysis will be carried out) WHICH \u2013 Materials and resources (actual materials and resources needed to execute the Citizen Observatory\u2019s thematic co-exploration) HOW \u2013 Data analysis process to be able to capture, analyse and generate indicators and visualizations sought in Citizen Observatory\u2019s thematic co-exploration Each of the above mentioned tools contains a page in the Technical guide for GREENGAGE . Thematic co-exploration\u2019s participants guided by the PST should choose the best fitting tools for the considered thematic co-exploration. Following the examples included in each tool page, they should configure the tools and test their readiness for being used during the operation of the thematic co-exploration. Besides, a selection and adaptation of the materials/knowledge assets defined at Knowledge Base of the main project\u2019s website and also in the internal catalogue of the CE should be performed. Some of those materials should be adapted to the thematic co-exploration in stake. The evaluation strategy of the thematic co-exploration should be defined. This entails that KPIs are defined and evaluation mechanisms to populate such KPIs also defined. This will entail the design of evaluation questionnaires based on those already designed by the project and the collection of logs or issuing queries over the GREENGAGE and external tools used in the thematic co-exploration. Finally, detailed planning for the thematic co-exploration\u2019s piloting and the engagement plan to ensure continuous involvement of community members should also be established. Resources Collaborative Environment \u2013 tool to follow the process defined, have a centralised repository of resources and a collaborative space for the team to work together in the different phases and tasks of the thematic co-exploration Thematic Co-Exploration for Citizen Observatory (CO) Specification Template \u2013 knowledge asset to be used in the increasing formalization and clarification of the thematic co-exploration. Thematic co-exploration\u2019s workplan template \u2013 example of spreadsheet that should be completed by organizers of a thematic co-exploration to plan the activities that should be carried out for the succesful execution and evaluation of the thematic co-exploration. Community Building plan template \u2013 example of spreadsheet that should be completed by organizers of a thematic co-exploration to plan how the engagements of Citizen Observers and stakeholders will take place. Resources and budget template \u2013 convenient resource to help you planning what materials and budget will be needed to run the CS experiment or thematic co-exploration Example of metrics defined for a pilot \u2013 document illustrating the reasoning for the establishment of a set of metrics and measurement mechanisms. Step 5: Management of data workflow process for the thematic co-exploration Responsible PST members and GREENGAGE Observers (GOs) with technical skills Objective This step aims to establish a robust and efficient data management process for the Citizen Science campaign or thematic co-exploration. This process will involve the planning of data collection based on the data identified in Step 3 . The data management plan should depict the data collection process for each dataset, the stakeholders involved in their collection, and the tools and platforms used. Furthermore, the plan should include the workflow that will be followed for each dataset, from collection to analysis and visualisation. Through this process, the technical team and the PST will have a well-defined and structured plan for each dataset, facilitating the subsequent steps. The following aspects of data management for the thematic co-exploration will be defined at this step and its substeps: Data protocol for your Citizen Observatory\u2019s thematic co-exploration: What type of data do you need? Does it involve objective or subjective measurements? Do you focus on one or more parameters? Does it involve creating a new dataset or adding to an existing dataset? What is the geographical and temporal scope of the data collection process? Do you need a representative sample? How will participants have to collect the data: via one or several measuring instruments? Analytical methods which should be applied within the Citizen Observatory\u2019s thematic co-exploration. Visualization and metrics to generate to support the decision-making through Citizen Observatory\u2019s realization. How can we achieve it? PST members and the technical members of the thematic co-exploration\u2019s participants should work together to fulfil a template that defines the data management plan for each dataset in the data inventory realised in Step 3 . Resources Data Management Plan Template \u2013 to define the data management plan for each dataset and the associated data protocol to be followed by observers Step 5a: Gather Copernicus and Open Data from government datasets to gain bird's eye view for thematic co-exploration Responsible PST members and GREENGAGE Observers (GOs) with technical skills Objective This step aims to retrieve the datasets identified in Step 3 that are already available in Open Data portals or provided by Copernicus data . This process will involve the creation of the pipelines (ingestion sources) that will introduce each dataset in DataHub, which will allow us to retrieve the data from the source and store it in the centralised repository. The technical team should create the ingestion sources with the pilot owners' support. How can we achieve it? This step can be achieved in many ways depending on the data source to be retrieved. The ingestion sources can be created using Apache NiFi, allowing us to retrieve data from APIs, FTP servers, and other sources. Alternatively, other ingestion sources, such as Python scripts, may require programmatic solutions. Finally, the ingestion sources can also be configured using Apache Druid, allowing us to load data directly into the project's database. The technical team will be responsible for creating the ingestion sources, while the pilot owners will be responsible for providing the necessary information. Resources HOWTOs to gather data from Copernicus and from Open Data portals considered in the project (TBD) Apache NiFi official documentation \u2013 Official documentation for tool to create ingestion sources Apache NiFi GREENGAGE documentation \u2013 HOWTO for tool in GREENGAGE data stack configured for creating ingestion sources Apache Druid official documentation \u2013 HOWTO for tool to load data directly into the centralised datastore offered by GREENGAGE Apache Druid GREENGAGE documentation \u2013 HOWTO explaining how to load data directly into the project's database Step 5b: Organise and gather crowdsourcing of data by citizens through data capture apps Responsible PST members and GREENGAGE Observers (GOs) with technical skills Objective This step aims to design and implement a data crowdsourcing campaigns to collect data through human collaborationidentified, as data gaps, in Step 3 . This process will involve the creation of data gathering campaigns that include missions (tasks) to be completed by the public. The missions will be designed by the thematic co-exploration\u2019s moderators, with the support of the technical team, to retrieve specific data. The missions, thus, the data capture process, will involve using specific tools designed for this task, such as MODE, GREENGAGE app or MindEarth. Notice that mission is a GREENGAGE concept denoting then need to collect certain data in a given area and time period to address a certain data gap identified within a thematic co-exploration. How can we achieve it? First, the thematic co-exploration\u2019s moderators should identify and design the missions that citizens will have to complete to collect the data. Then, together with the technical team, they will configure the tools that will be used for the data capture process. Once the tools are configured and citizens trained to use them (allegedly carried out in Step 2 ), the data crowdsourcing campaigns should be launched and promoted. During the operation of the campaigns, the PST members should monitor their progress and performance, and the technical team should ensure that the data is being collected and stored correctly. Data capturing apps and sensors belonging to \u201cData crowdsourcing and capture layer\u201d of GREEN Engine, such as MODE, GREENGAGE app or MindEarth, will be used to collect the data. Resources Resources and budget template \u2013 convenient resource to help you planning what materials and budget will be needed to run the CS experiment or thematic co-exploration. This document should be updated in this step. MODEs GREENGAGE documentation \u2013 HOWTO for tool to collect commuting data MindViews GREENGAGE documentation \u2013 HOWTO for tool to collect geo-tagged street-level imagery GRENGAGE apps GREENGAGE documentation \u2013 HOWTO for tool to collect data via campaigns AtmoTube PRO sensor webpage \u2013 webpage of sensor selected to measure air quality data in pilots [Sensors integration in GREENGAGE] (https://greengage-project.github.io/Documentation/tools/sensorsIntegration/) \u2013 HOWTO to off-the-shelf sensors (ATMOtube and sensing apps, e.g. Noisetube ) to be used in the project Step 6: Cataloguing and storage of captured data Responsible PST members and GREENGAGE Observers (GOs) with technical skills Objective This step aims to maintain and continuously populate a centralised repository of metadata for all datasets within the scope of the GO, where datasets are tagged according to the thematic co-explorations where they apply. This repository will serve as a foundational resource for the stakeholders working in the GO, enabling them to manage and analyse the data efficiently. The process will involve the registration of datasets in DataHub, a centralised metadata platform that will act as a single source of truth for all project data. Furthermore, DataHub allows us to add documentation and metadata for each dataset catalogued this way. On the other hand, this step entails storing and/or configuring data access and query mechanisms to the datasets considered, with the support of Apache Druid. How can we achieve it? Althoughthe datasets were identified in Step 3 and some already is registered in DataHub, it is now time to revise and enrich the catalogue with the final datasets\u2019 metadata that will be used during the analysis of data. The technical team will conduct this process with the support of the pilot owners. The registration process will involve the creation or update (if already existed) of a metadata file for each dataset, which will be uploaded to DataHub. The metadata file will include information such as the dataset's name, description, source, and intended use (thematic co-explorations where it will be used). The technical team will be responsible for creating the ingestion sources for DataHub, while the pilot owners will be responsible for providing the necessary information. Besides, technical participants in the thematic co-exploration aided by the PST will configure mechanisms to store, access and query the gathered data through Apache Druid. Resources Documentation of DataHub \u2013 HOWTO for tool to register datasets in DataHub Video of how to create datahub sources \u2013 video explaining how to carry out the metadata declaration process in DataHub (TO BE DONE) Druid \u2013 video explaining how to store and/or access and query the actual data to be used within the themaci co-exploration through Apache Druid (TO BE DONE) Step 7: Generation of data analysis workflows (Data cleaning, pipelining and analysis) Responsible: PST members and GREENGAGE Observers (GOs) with technical skills Objective The objective of this step is a critical phase in the data management process, primarily focused on ensuring the quality and usability of datasets within the project's framework. This stage involves a series of preprocessing steps designed to cleanse the datasets, removing any inconsistencies, errors, or irrelevant information that might hinder analysis. Since some datasets are updated daily, it's essential to establish automated data ingestion workflows, using tools like Apache NiFi, to streamline the continuous flow of data into the main data store, namely Apache Druid. This approach facilitates the efficient and reliable updating of data and ensures that the datasets are in an optimal format and structure for analysis. The ultimate goal of this process is to enhance data accuracy and integrity, thereby enabling a deeper and more accurate understanding of the data, which is vital for informed decision-making and insightful analysis within the project. The outcome of this process will be a set of correlated datasets conveniently interlinked and using shared schemas so that the data analysis and visualization steps can be simplified. How can we achieve it? To successfully create workflows that can process the data into the desired format we have several ways to do it: creating a NiFi Flow, creating a python script or manually. NiFi flow: Apache NiFi is a powerful tool that allows us to create data pipelines that can be scheduled to run periodically. This tool is very useful for data ingestion, as it allows us to fetch data from APIs, FTP servers, and other sources. Additionally, NiFi can be used to clean and transform data, ensuring that it is in the desired format and structure. Finally, NiFi can be used to load data into Apache Druid, ensuring that the data is correctly ingested into the database. Python script: Develop Python scripts for more complex data transformations that might not be directly feasible within NiFi. This can include data cleaning, normalization, or feature engineering tasks. It can also be integrated with Apache NiFi to run as part of the flow. Furthermore, by using the Apache Druid API we can directly load data into the database. Manually: For simple data transformations, it might be more efficient to perform them manually. This can include tasks such as renaming columns, changing data types, or removing columns. However, this approach is not scalable and should only be used for simple tasks. Once the data is preprocessed and cleaned the analysis may be conducted by combining and exploring the data using Apache Superset and Apache Druid. Apache Superset is a powerful tool that allows us to create interactive dashboards and visualizations that will be explained in the next step. Apache Druid allows us to perform real-time analytics on large datasets. These tools will be used to explore the data and generate insights that will be used to answer the research questions. Resources Data quality VRVIS (TBD) Apache NiFi documentation \u2013 HOWTO for tool to create preprocessing flows Apache Druid documentation \u2013 HOWTO for tool to load data into the project's database Apache Superset documentation \u2013 HOWTO for tool to create dashboards and visualizations Step 8: Generation of thematic co-exploration dashboards Responsible PST members and GREENGAGE Observers (GOs) with technical skills Objective The objective of generating thematic co-exploration dashboards step of a thematic co-exploration is to develop interactive and insightful tools that transform complex data into accessible visual formats, enhancing comprehension and insight. These dashboards are designed to enable stakeholders, including policymakers, researchers, and citizens, to delve into various data themes and patterns, fostering an environment of collaborative exploration and dialogue. By providing real-time data monitoring and analysis, these dashboards will be instrumental in supporting quick and informed decision-making and policy development. The overarching goal is to ensure transparency and open access to a given thematic co-exploration\u2019s data, thereby promoting inclusive participation and a shared understanding among all project participants. How can we achieve it? To accomplish this, we will leverage tools like Apache Superset and Apache Druid, as outlined in GREENGAGE\u2019s technical documentation . Apache Superset will allow us to create and share rich data visualizations and dashboards, providing an intuitive interface for users to interact with the data. Apache Druid will be utilized for its real-time data analytics capabilities, ensuring that our dashboards can display the most current data efficiently. Additionally, the integration of Apache NiFi will streamline the data flow into these systems, ensuring a consistent and reliable data pipeline. The combination of these tools, supported by thorough documentation and examples of integration, will enable us to create dynamic and insightful thematic co-exploration dashboards that serve the diverse needs of our project stakeholders. Resources ThematicCoExplorationSpec \u2013 continously updated document where to revisit the objectives of the thematic co-exploration and generate dashboards that address them Data quality VRVIS (https://greengage-project.github.io/Documentation/tools/DataQualityDashboard/) \u2013 HOWTO for tool to handle data quality assurance on the gathered datasets Apache Superset documentation \u2013 official documentation for tool to create dashboards and visualizations HOWTO GISAT tool to do these visualizatons (TBD) Step 9: Generation of storylines for wide dissemination Responsible Pilot owners and GREENGAGE Observers (GOs) Objective The objective in the \"Generation of storylines for wide dissemination\" phase is to craft engaging and accessible narratives that convey the thematic co-exploration's key findings and insights to a broad audience. These storylines are designed to translate complex data and research outcomes into formats that are both engaging and easily understandable by the public. The aim is to underscore the thematic co-exploration's significance and its potential impact on policy-making, community initiatives, and scientific research. By doing so, the goal is to foster increased public awareness and interest in the project, thereby enhancing its overall reach and influence. To achieve wide visibility and impact, these storylines will be disseminated through various media channels, tailored to resonate with diverse audience segments, thereby ensuring the project's findings are communicated effectively and broadly. How can we achieve it? To effectively generate storylines for wide dissemination, we will utilize a range of tools specified in our project documentation. The Communications and Outreach Team will collaborate closely with the Data Analysis Team, utilizing Apache Superset for creating compelling data visualizations that bring complex data to life in an easily digestible format. These visual elements will be integral in crafting storylines that are both informative and engaging to the public. Additionally, we will leverage WordPress and Discourse platforms to draft, refine, and share these narratives, ensuring they are tailored to resonate with diverse audience segments. The integration of these tools will enable us to effectively communicate the thematic co-exploration's impact and findings through various media channels, from social media to press releases, thereby maximizing reach and fostering greater public engagement with the project's outcomes. Resources Course on Storytelling for Citizen Science \u2013 this course gives some tips about how to perform storytelling for Citizen Science projects Presentation of Storytelling and Citizen Science \u2013 this presentation brings about the value of storytelling in Citizen Science. It takes us through several viewpoints and tools you can use to tell your story, considering the needs of the audience, the human instinct for storytelling, how to use it to make your Citizen Science project exciting and meaningful to other people, and encourages you to think about ways to use these tools to reach more audiences Step 10: Generation of policy briefs' inputs for policy makers Responsible Policy Analysis Team, Pilot Owners and GREENGAGE Observers (GOs) Objective The generation of policy briefs for policymakers, a key objective of this project step, involves synthesizing research findings and data into clear, concise, and actionable recommendations. These briefs are crafted to effectively communicate pivotal insights and findings to policymakers, providing them with evidence-based guidance for policy interventions. By directly aligning with current policy debates and challenges, these briefs aim to bridge the gap between research and practical policymaking, offering solutions that are grounded in thorough data analysis. This approach not only enhances the relevancy of the research to ongoing policy discussions but also aims to significantly influence policy decisions and strategies, thus amplifying the overall impact of a given thematic co-exploration. How can we achieve it? To achieve this, we will first aggregate and analyze the data collected and insights generated throughout the project, using tools like Apache Druid for real-time analytics and Apache Superset for data visualization. These tools will help in identifying key trends and findings that are most relevant to policy-making. The policy analysis team will then work closely with research teams to distill these insights into policy briefs, ensuring that they are both data-driven and aligned with current policy needs, as were identified in Step 0 . Communication specialists will refine these briefs to be clear and compelling, making them accessible to non-specialist policymakers. Throughout this process, a given thematic co-exploration\u2019s coordinators will ensure that the briefs align with the strategic objectives of the thematic co-exploration and effectively convey its potential impact on policy and governance. Resources Collaborative Environment \u2013 HOWTO about environment to collaborate in the policy briefs' generation in a centralised repository. The CE will suggest templates to guide the ellaboration of policy briefs. Policy brief guideline \u2013 web document explaining how to format a policy brief Example of Policy brief \u2013 in the domain of Citizen Science that could inspire those embarking in the preparation of a policy brief","title":"HOWTO Thematic co-exploration"},{"location":"HOWTO%20Thematic%20co-explorations/#howto-thematic-co-exploration","text":"This document describes the collaborative process, i.e. set of steps that should be carried out in a logical sequence to co-deliver a thematic co-exploration. Importantly, these steps are indicative and there could be different journeys through them, i.e. some may be skipped and some new steps added if the co-producers of a thematic co-exploration so consider. Besides, a thematic co-exploration is a collaborative and iterative process, i.e. several iterations over some of the steps of the collaborative process may be needed. Indeed, the process indicated below should not be understood as a one way rigid top down process but as an iterative process where some steps may be executed more than once.","title":"HOWTO Thematic co-exploration"},{"location":"HOWTO%20Thematic%20co-explorations/#steps-to-follow-to-co-deliver-thematic-co-exploration","text":"0. Choose thematic co-exploration area and define research questions&hypothesis to be validated to enable policy-making 1. Preparation of the co-production process, teams and community dialogue mechanism 2. Invitation, onboarding and training of selected users to take part in a Citizen Observatory\u2019s thematic co-exploration 3. Inventory of available datasets and data gaps identification to explore a problem domain 4. Co-preparation of the thematic co-exploration to make it ready for launch 5. Management of data workflow process for the thematic co-exploration 5a. Gather Copernicus and Open Data from government datasets to gain bird's eye view for thematic co-exploration 5b. Organise and gather crowdsourcing of data by citizens through data capture apps 6. Cataloguing and storage of captured data 7. Generation of data analysis workflows (Data cleaning, pipelining and analysis) 8. Generation of thematic co-exploration dashboards) 9. Generation of storylines for wide dissemination) 10. Generation of policy briefs' inputs for policy makers)","title":"Steps to follow to co-deliver thematic co-exploration"},{"location":"HOWTO%20Thematic%20co-explorations/#step-0-choose-thematic-co-exploration-area-and-define-research-questionshypothesis-to-be-validated-to-enable-policy-making","text":"","title":"Step 0: Choose thematic co-exploration area and define research questions&amp;hypothesis to be validated to enable policy-making "},{"location":"HOWTO%20Thematic%20co-explorations/#responsible","text":"GREENGAGE Observers (GOs) and PST members","title":"Responsible"},{"location":"HOWTO%20Thematic%20co-explorations/#objective","text":"The primary objective of this step is to formulate precise research questions and hypotheses that are anchored in the needs and priorities of policy-making for a given area where thematic co-exploration could be advisable. Guided by pilot owners who are part of the PST team and know well what are the priorities of a given Citizen Observatory, an area of itnerest to research through a thematic co-exploration is chosen. The devised research questions and hypotheses should be actionable, measurable, and capable of being empirically tested within the thematic co-exploration\u2019sscope. They should be designed to fill knowledge gaps, address current challenges, and generate evidence to translate into effective policy interventions. This step will involve a thorough analysis of the current policy landscape, consultation with stakeholders of the GREENGAGE Observatory, and a review of the existing literature to ensure that the research is targeted and relevant. This foundational step sets the stage for the subsequent research and data collection efforts, ensuring that the thematic co-exploration\u2019s outcomes align with the strategic goals of influencing and enabling policy-making.","title":"Objective"},{"location":"HOWTO%20Thematic%20co-explorations/#how-can-we-achieve-it","text":"After having chosen an area of high priority to tackle a thematic co-exploration, guided by the Thematic Co-Exploration for Citizen Observatory (COb) Specification document, a group of GREENGAGE Observers (GObs), citizens, civil servants and other stakeholders, who will manage and moderate the thematic co-exploration, answer, in collaboration with owners of the Citizen Observatory, the following core questions: WHY \u2013 Reason why this Citizen Observatory\u2019 thematic co-exploration is needed (arguments for promoting the execution of this Citizen Observatory\u2019s campaign) WHO \u2013 Involved and affected stakeholders\u2019 groups in Citizen Observatory\u2019s thematic co-exploration (describe the target groups and their possible motivation) WHAT \u2013 Actual endeavours of the Citizen Observatory\u2019s thematic co-exploration (describe what will exactly be done in this Citizen Observatory) WHEN \u2013 Planning of activities and period when Citizen Observatory\u2019s thematic co-exploration will be executed (indicate for how long, what activities, where and for whom will be realized) WHERE \u2013 Geographical locations where Citizen Observatory\u2019s thematic co-exploration will take place (actual geographical areas where data collection and analysis will be carried out) WHICH \u2013 Materials and resources (actual materials and resources needed to execute the Citizen Observatory\u2019s thematic co-exploration) HOW \u2013 Data analysis process to be able to capture, analyse and generate indicators and visualizations sought in Citizen Observatory\u2019s thematic co-exploration The Thematic Co-Exploration for Citizen Observatory (COb) Specification document does not need to be completed thoroughly in this step. It is paramount that the first questions of this document are filled in. It is the WHAT question the one that actually addresses the following aspects essential to start with the co-production of a thematic co-exploration: The problem (describe the problem statement and the challenge that you are addressing) Objectives (describe the intended result) Added value (describe the potential benefits for the Citizen Observatory\u2019s thematic co-exploration\u2019s stakeholders) Current and desired situation (describe the current approach including existing practices, and the desired situation) CS hypothesis and research questions (to be validated by the outcomes of the execution of the Citizen Observatory\u2019s thematic co-exploration) Metrics definition (of indicators of success for the Citizen Observatory\u2019s thematic co-exploration) Notice that some additional optional resources are listed below which could be used before filling in the thematic co-exploration\u2019s specification, to help narrowing the problem and reflecting about the suitability of the problem to be addressed through a thematic co-exploration.","title":"How can we achieve it?"},{"location":"HOWTO%20Thematic%20co-explorations/#resources","text":"Citizen Science problem statement analysis \u2013 this resource allows to perform a minimal viability analysis to determine whether a problem that wants to be tackled is approachable through Citizen Science or not. Questionnaire to analysise suitability of the problem for co-production of CS experiment * this second resource can help you dilucidate whether a collaborative process can be organized or not, to address a given problem, through a Citizen Science experiment. Thematic Co-Exploration for GREENGAGE Observatory (GO) Specification Template \u2013 this resource is the most important one in this stage since it allows to answer the WHY, WHO, WHAT, WHEN, WHERE, with WHICH and HOW questions associated to the organization and planning of a thematic co-exploration. It is designed to compile all the information needed to get started with a thematic co-exploration. It gets information from the CONCEPTUALISING and PREPARING phases of the GREENGAGE methodology for Citizen Observatories.","title":"Resources"},{"location":"HOWTO%20Thematic%20co-explorations/#step-1-preparation-of-the-co-production-process-teams-and-community-dialogue-mechanism","text":"","title":"Step 1: Preparation of the co-production process, teams and community dialogue mechanism"},{"location":"HOWTO%20Thematic%20co-explorations/#responsible_1","text":"GREENGAGE Observers (GOs) and PST members","title":"Responsible"},{"location":"HOWTO%20Thematic%20co-explorations/#objective_1","text":"The primary objective of this step is to establish a robust and effective co-production process in GREENGAGE\u2019s Collaborative Environment , that fosters collaborative engagement between various stakeholders within the GREENGAGEObservatory (GO)\u2019s thematic co-exploration. This involves the strategic configuration of various community teams to ensure a diverse representation of perspectives, skills, and expertise. The aim is to create a digital collaboration space where community members, experts, and stakeholders can collectively contribute to the co-production process underlying every thematic co-exploration, leveraging their unique insights and experiences. This process is designed to facilitate the seamless integration of community input into the thematic co-exploration and empower participants by providing them with the tools, training, and support, in the form of GO Enablers, necessary for meaningful participation. The ultimate goal is to enhance the quality, relevance, and impact of the thematic co-exploration\u2019s outcomes through a participatory approach that values and utilises the collective knowledge and capabilities of the GO\u2019s community teams, guided and supported by Pilot Support Team (PST) members, who coach and support them during the usage of technologies and application of the different stages part of the Citizen Science loop.","title":"Objective"},{"location":"HOWTO%20Thematic%20co-explorations/#how-can-we-achieve-it_1","text":"To facilitate discussions and community engagement, Discourse will be integrated, providing a modern and user-friendly forum for exchange and collaboration. In parallel, WordPress will serve as our content management system, offering a versatile platform for information dissemination and interaction with the broader community. Finally and, most importantly, a new co-production process will be set up in GREENGAGE Collaborative Environment. Hence, the accomplishment of this step requires usage of all the tools belonging to the \u201cCommunity and co-production process management\u201d layer of GREEN Engine. A new entry should be created within the parent pilot / Citizen Observatory page, for the newly specified thematic co-exploration. The instructions to use Wordpress should be followed to achieve this. A new set of categories for topics of discussion following the instructions of usage of the Discourse tool , should be created by the GObs leading and managing the thematic co-exploration. This will serve to foster the collaboration and dialogue among participants in the thematic co-exploration. A new co-production process based on the \u201cGREENGAGE way\u201d co-production process schema or blueprint already loaded into Collaborative Environment should be instantiated.","title":"How can we achieve it?"},{"location":"HOWTO%20Thematic%20co-explorations/#resources_1","text":"WordPress \u2013 HOWTO for tool essential to communicate the existance of a thematic co-exploration and allow dissemination of its results to the broader community* Discourse \u2013 HOWTO for tool necessary to foster discussions and dialogue among GObs and allow them to request help and support each other Collaborative Environment \u2013 HOWTO for essential tool to govern, guide and trace the co-production process executed during the co-design and co-delivery of the thematic co-exploration CS-CO-schema-process-specification \u2013 to specify the process of the thematic co-exploration ala \u201cGREENGAGE way\u201d Problem Statement Questionnaire document \u2013 after instantiation of the co-production proces for the the thematic co-exploration, this main outcome of stage #0 will be uploaded into the new process. Important to review and further complete questions WHO, WHEN, WHERE and WHICH which will be tackled in the following steps to co-produce the thematic co-exploration. Stakeholder Mapping Canvas \u2013 document to help you understand who are the stakeholders of your thematic co-exploration","title":"Resources"},{"location":"HOWTO%20Thematic%20co-explorations/#step-2-invitation-onboarding-and-training-of-selected-users-to-take-part-in-a-citizen-observatorys-thematic-co-exploration","text":"","title":"Step 2: Invitation, onboarding and training of selected users to take part in a Citizen Observatory\u2019s thematic co-exploration"},{"location":"HOWTO%20Thematic%20co-explorations/#responsible_2","text":"Pilot support team members","title":"Responsible"},{"location":"HOWTO%20Thematic%20co-explorations/#objective_2","text":"A thematic co-exploration is structured to engage and empower citizens, enabling their active involvement in data collection, observation, and analysis. It strives to ensure that contributions from citizens are meaningful and pivotal in addressing key research questions. This step assumes that the following questions have been previously been answered when tackling the WHO question of the Problem Statement Questionnaire document: Who is the promoter and/or sponsor of this Citizen Observatory\u2019s thematic co-exploration? Who are responsible for the Citizen Observatory\u2019s thematic co-exploration ? Who are the domain experts and leaders of the Citizen Observatory\u2019s community to be involved in this thematic co-exploration? Who will be the participants in this CS campaign of the Citizen Observatory and what will be their collaborative duties ? What social collectives, influencers, societal representatives could help you disseminating the outcomes of this Citizen Observatory\u2019s thematic co-exploration? Who will be the multi stakeholder members affected by the Citizen Observatory\u2019s thematic co-exploration\u2019s results ?","title":"Objective"},{"location":"HOWTO%20Thematic%20co-explorations/#how-can-we-achieve-it_2","text":"To achieve the objectives, the Collaborative Environment will be the key to enhanced collaboration and asset management and will be instrumental in coordinating the efforts of different stakeholders and managing campaign assets effectively. Indeed, different teams of stakeholder types will be configured in the environment and granted different rights over the phases, objectives and tasks of the co-production process. The thematic co-exploration promoters will co-design different communication assets based on the templates in the form of knowledge assets or GO enablers already loaded into the Collaborative Environment. For community engagement and discussion, Discourse will serve as our primary platform, fostering a space for open, structured communication among participants. Groups for each co-producer team will be assembled in the tool. Several topics to foster onboarding and training for the new thematic co-exploration will be promoted through Discourse. Training materials for the following topics have been made ready by the project and should be extended and customized for each thematic co-exploration: Data I Data II Co-design I Co-design II Co-design III Air quality & Mobility Monitoring Technology I Technology II Technology III Policy-making & Governance Analysis & Visualisation","title":"How can we achieve it?"},{"location":"HOWTO%20Thematic%20co-explorations/#resources_2","text":"Materials for thematic co-exploration and citizen onboarding \u2013 TO BE PREPARED Folder where trainings are currently uploaded which will be published as part of the Knowledge Base aka GREENGAGE Academy of the main project\u2019s website","title":"Resources"},{"location":"HOWTO%20Thematic%20co-explorations/#step-3-inventory-of-available-datasets-and-data-gaps-identification-to-explore-a-problem-domain","text":"","title":"Step 3: Inventory of available datasets and data gaps identification to explore a problem domain"},{"location":"HOWTO%20Thematic%20co-explorations/#responsible_3","text":"PST members and GREENGAGE Observers (GOs)","title":"Responsible:"},{"location":"HOWTO%20Thematic%20co-explorations/#objective_3","text":"The primary goal of this step is to establish a comprehensive and organised system for managing datasets relevant to a thematic co-exploration. By requiring pilot owners to register existing and forthcoming datasets, we aim to create a centralised metadata repository that will serve as a foundational resource for the technical team being able to access, explore and analyse the data to be compiled for the thematic co-exploration. This registration process will facilitate the subsequent uploading and integration of these datasets into the GREENGAGE infrastructure for its usage by the thematic co-explorations of a given Citizen Observatory. The aim of this step is to develop a detailed and accessible catalogue of data, for the whole Citizen Observatory and where datasets will be categorized according to the thematic co-exploration they belong to, which will so streamline thematic co-exploration\u2019s operations and enhance the efficacy of data analysis and decision-making processes. This initiative is instrumental in ensuring that all stakeholders have a clear and consistent understanding of the available data resources, thereby optimising the utilisation and impact of the data within a given Citizen Observatory\u2019s s scope. Importantly, during the preparation of each thematic co-exploration, GObs will also contribute with new entries, categorized and linked to such thematic co-exploration, within the centralised catalogue of datasets in a Citizen Observatory.","title":"Objective"},{"location":"HOWTO%20Thematic%20co-explorations/#how-can-we-achieve-it_3","text":"To achieve the following objective, PST members should comprehensively identify all existing and potential datasets within the scope of the Citizen Observatory and, particularly, those needed for the thematic co-exploration considered. This step aims to create an inventory of datasets, categorising them based on the current thematic co-exporation\u2019s ID, relevance, data type, source, and intended use. Furthermore, it is essential to specify how each dataset may be retrieved and accessed. This inventory serves as the foundation for the dataset metadata registration process in DataHub. Population of Dataset with the newly created inventory is the final action within this step.","title":"How can we achieve it?"},{"location":"HOWTO%20Thematic%20co-explorations/#resources_3","text":"Dataset Inventory template \u2013 knowledge asset with fields to identify every dataset\u2019s metadata that will be needed for the thematic co-explorations in each Citizen Observatory. It is recommended that for each thematic co-exploration a a copy of this template is performed and filled in. DataHub \u2013 HOWTO for tool used to register, search and view metatada about what datasets of a given thematic area are already available in the catalogue of such Citizen Observatory. The dataset inventory template completed for a given thematic co-exploration shoudl be used as input to complete the datasets metadata in DataHub.","title":"Resources"},{"location":"HOWTO%20Thematic%20co-explorations/#step-4-co-preparation-of-the-thematic-co-exploration-to-make-it-ready-for-launch","text":"","title":"Step 4: Co-preparation of the thematic co-exploration to make it ready for launch"},{"location":"HOWTO%20Thematic%20co-explorations/#responsible_4","text":"PST members and GREENGAGE Observers (GOs)","title":"Responsible"},{"location":"HOWTO%20Thematic%20co-explorations/#objective_4","text":"The core objective of this task is to get the community of observers, their collaboration process management, the tools and reources needed for the succesful execution of the thematic co-exploration ready for operation. It will entail completing the following steps: Select GREENGAGE tools and knowledge assets to co-produce thematic co-exploration Agree on indicators, metrics and methods for measurement needed Define data protocol to be used in thematic co-exploration Adapt selected tools for data management Adapt selected communication and evaluation materials Test process and community management tools Test data capture tools and instruments Test data analysis tools and visualizations Test monitoring and evaluation instruments Test community support mechanisms Report lessons learnt and improvements needed detected in the preparation process Planning of work activities and engagement events for execution and evaluation Once all the steps above listed are completed, it can be considered that the GO\u2019s thematic co-exploration is ready for operation and evaluation. Central to GREENGAGE is the utilisation of innovative digital tools and platforms, which are devised for efficient data collection, management, and analysis. Besides, in GREENGAGE it is aimed to foster a community spirit and shared purpose among participants, thereby enhancing their understanding and engagement in scientific research. This approach is expected to yield reliable and high-quality data, instrumental in influencing policy-making, supporting community initiatives, and furthering scientific inquiries. Additionally, the CS campaign or thematic co-exploration co-designed should be scalable, replicable, and adaptable to a variety of contexts and research domains, setting a model for future citizen science endeavours.","title":"Objective"},{"location":"HOWTO%20Thematic%20co-explorations/#how-can-we-achieve-it_4","text":"To realize this step, the Collaborative Environment (CE) and the \u201cGREENGAGE way\u201d blueprint instantiated for the thematic co-exploration, should guide the interaction and management of the campaign's activities. In this process, Discourse will also be used for community engagement and open communication, facilitating a structured and inclusive dialogue among participants. For data collection, tools such as GREENGAGE app, MindEarth app, or sensors for environmental monitoring will be employed. The data collected will be organized and analyzed using DataHub, Apache Superset, and Apache Druid for advanced data storage, analysis and visualization. Apache NiFi will be instrumental in ensuring a streamlined and efficient data processing pipelines. The synergy of these tools and platforms, as specified in our technical guide (layers \u201cCommunity and co-production process management\u201d, \u201cData crowdsourcing and capture\u201d and \u201cData analysis and insights generation\u201d), will ensure a comprehensive, secure, and effective Citizen Science campaign, addressing the research questions while empowering citizen participants. Once more, the Problem Statement Questionnaire document should be revisited. This time special attention should be paid to sections: WHEN \u2013 Planning of activities and period when Citizen Observatory\u2019s thematic co-exploration will be executed WHERE \u2013 Geographical locations where Citizen Observatory\u2019s thematic co-exploration will take place (actual geographical areas where data collection and analysis will be carried out) WHICH \u2013 Materials and resources (actual materials and resources needed to execute the Citizen Observatory\u2019s thematic co-exploration) HOW \u2013 Data analysis process to be able to capture, analyse and generate indicators and visualizations sought in Citizen Observatory\u2019s thematic co-exploration Each of the above mentioned tools contains a page in the Technical guide for GREENGAGE . Thematic co-exploration\u2019s participants guided by the PST should choose the best fitting tools for the considered thematic co-exploration. Following the examples included in each tool page, they should configure the tools and test their readiness for being used during the operation of the thematic co-exploration. Besides, a selection and adaptation of the materials/knowledge assets defined at Knowledge Base of the main project\u2019s website and also in the internal catalogue of the CE should be performed. Some of those materials should be adapted to the thematic co-exploration in stake. The evaluation strategy of the thematic co-exploration should be defined. This entails that KPIs are defined and evaluation mechanisms to populate such KPIs also defined. This will entail the design of evaluation questionnaires based on those already designed by the project and the collection of logs or issuing queries over the GREENGAGE and external tools used in the thematic co-exploration. Finally, detailed planning for the thematic co-exploration\u2019s piloting and the engagement plan to ensure continuous involvement of community members should also be established.","title":"How can we achieve it?"},{"location":"HOWTO%20Thematic%20co-explorations/#resources_4","text":"Collaborative Environment \u2013 tool to follow the process defined, have a centralised repository of resources and a collaborative space for the team to work together in the different phases and tasks of the thematic co-exploration Thematic Co-Exploration for Citizen Observatory (CO) Specification Template \u2013 knowledge asset to be used in the increasing formalization and clarification of the thematic co-exploration. Thematic co-exploration\u2019s workplan template \u2013 example of spreadsheet that should be completed by organizers of a thematic co-exploration to plan the activities that should be carried out for the succesful execution and evaluation of the thematic co-exploration. Community Building plan template \u2013 example of spreadsheet that should be completed by organizers of a thematic co-exploration to plan how the engagements of Citizen Observers and stakeholders will take place. Resources and budget template \u2013 convenient resource to help you planning what materials and budget will be needed to run the CS experiment or thematic co-exploration Example of metrics defined for a pilot \u2013 document illustrating the reasoning for the establishment of a set of metrics and measurement mechanisms.","title":"Resources"},{"location":"HOWTO%20Thematic%20co-explorations/#step-5-management-of-data-workflow-process-for-the-thematic-co-exploration","text":"","title":"Step 5: Management of data workflow process for the thematic co-exploration"},{"location":"HOWTO%20Thematic%20co-explorations/#responsible_5","text":"PST members and GREENGAGE Observers (GOs) with technical skills","title":"Responsible"},{"location":"HOWTO%20Thematic%20co-explorations/#objective_5","text":"This step aims to establish a robust and efficient data management process for the Citizen Science campaign or thematic co-exploration. This process will involve the planning of data collection based on the data identified in Step 3 . The data management plan should depict the data collection process for each dataset, the stakeholders involved in their collection, and the tools and platforms used. Furthermore, the plan should include the workflow that will be followed for each dataset, from collection to analysis and visualisation. Through this process, the technical team and the PST will have a well-defined and structured plan for each dataset, facilitating the subsequent steps. The following aspects of data management for the thematic co-exploration will be defined at this step and its substeps: Data protocol for your Citizen Observatory\u2019s thematic co-exploration: What type of data do you need? Does it involve objective or subjective measurements? Do you focus on one or more parameters? Does it involve creating a new dataset or adding to an existing dataset? What is the geographical and temporal scope of the data collection process? Do you need a representative sample? How will participants have to collect the data: via one or several measuring instruments? Analytical methods which should be applied within the Citizen Observatory\u2019s thematic co-exploration. Visualization and metrics to generate to support the decision-making through Citizen Observatory\u2019s realization.","title":"Objective"},{"location":"HOWTO%20Thematic%20co-explorations/#how-can-we-achieve-it_5","text":"PST members and the technical members of the thematic co-exploration\u2019s participants should work together to fulfil a template that defines the data management plan for each dataset in the data inventory realised in Step 3 .","title":"How can we achieve it?"},{"location":"HOWTO%20Thematic%20co-explorations/#resources_5","text":"Data Management Plan Template \u2013 to define the data management plan for each dataset and the associated data protocol to be followed by observers","title":"Resources"},{"location":"HOWTO%20Thematic%20co-explorations/#step-5a-gather-copernicus-and-open-data-from-government-datasets-to-gain-birds-eye-view-for-thematic-co-exploration","text":"","title":"Step 5a: Gather Copernicus and Open Data from government datasets to gain bird's eye view for thematic co-exploration"},{"location":"HOWTO%20Thematic%20co-explorations/#responsible_6","text":"PST members and GREENGAGE Observers (GOs) with technical skills","title":"Responsible"},{"location":"HOWTO%20Thematic%20co-explorations/#objective_6","text":"This step aims to retrieve the datasets identified in Step 3 that are already available in Open Data portals or provided by Copernicus data . This process will involve the creation of the pipelines (ingestion sources) that will introduce each dataset in DataHub, which will allow us to retrieve the data from the source and store it in the centralised repository. The technical team should create the ingestion sources with the pilot owners' support.","title":"Objective"},{"location":"HOWTO%20Thematic%20co-explorations/#how-can-we-achieve-it_6","text":"This step can be achieved in many ways depending on the data source to be retrieved. The ingestion sources can be created using Apache NiFi, allowing us to retrieve data from APIs, FTP servers, and other sources. Alternatively, other ingestion sources, such as Python scripts, may require programmatic solutions. Finally, the ingestion sources can also be configured using Apache Druid, allowing us to load data directly into the project's database. The technical team will be responsible for creating the ingestion sources, while the pilot owners will be responsible for providing the necessary information.","title":"How can we achieve it?"},{"location":"HOWTO%20Thematic%20co-explorations/#resources_6","text":"HOWTOs to gather data from Copernicus and from Open Data portals considered in the project (TBD) Apache NiFi official documentation \u2013 Official documentation for tool to create ingestion sources Apache NiFi GREENGAGE documentation \u2013 HOWTO for tool in GREENGAGE data stack configured for creating ingestion sources Apache Druid official documentation \u2013 HOWTO for tool to load data directly into the centralised datastore offered by GREENGAGE Apache Druid GREENGAGE documentation \u2013 HOWTO explaining how to load data directly into the project's database","title":"Resources"},{"location":"HOWTO%20Thematic%20co-explorations/#step-5b-organise-and-gather-crowdsourcing-of-data-by-citizens-through-data-capture-apps","text":"","title":"Step 5b: Organise and gather crowdsourcing of data by citizens through data capture apps"},{"location":"HOWTO%20Thematic%20co-explorations/#responsible_7","text":"PST members and GREENGAGE Observers (GOs) with technical skills","title":"Responsible"},{"location":"HOWTO%20Thematic%20co-explorations/#objective_7","text":"This step aims to design and implement a data crowdsourcing campaigns to collect data through human collaborationidentified, as data gaps, in Step 3 . This process will involve the creation of data gathering campaigns that include missions (tasks) to be completed by the public. The missions will be designed by the thematic co-exploration\u2019s moderators, with the support of the technical team, to retrieve specific data. The missions, thus, the data capture process, will involve using specific tools designed for this task, such as MODE, GREENGAGE app or MindEarth. Notice that mission is a GREENGAGE concept denoting then need to collect certain data in a given area and time period to address a certain data gap identified within a thematic co-exploration.","title":"Objective"},{"location":"HOWTO%20Thematic%20co-explorations/#how-can-we-achieve-it_7","text":"First, the thematic co-exploration\u2019s moderators should identify and design the missions that citizens will have to complete to collect the data. Then, together with the technical team, they will configure the tools that will be used for the data capture process. Once the tools are configured and citizens trained to use them (allegedly carried out in Step 2 ), the data crowdsourcing campaigns should be launched and promoted. During the operation of the campaigns, the PST members should monitor their progress and performance, and the technical team should ensure that the data is being collected and stored correctly. Data capturing apps and sensors belonging to \u201cData crowdsourcing and capture layer\u201d of GREEN Engine, such as MODE, GREENGAGE app or MindEarth, will be used to collect the data.","title":"How can we achieve it?"},{"location":"HOWTO%20Thematic%20co-explorations/#resources_7","text":"Resources and budget template \u2013 convenient resource to help you planning what materials and budget will be needed to run the CS experiment or thematic co-exploration. This document should be updated in this step. MODEs GREENGAGE documentation \u2013 HOWTO for tool to collect commuting data MindViews GREENGAGE documentation \u2013 HOWTO for tool to collect geo-tagged street-level imagery GRENGAGE apps GREENGAGE documentation \u2013 HOWTO for tool to collect data via campaigns AtmoTube PRO sensor webpage \u2013 webpage of sensor selected to measure air quality data in pilots [Sensors integration in GREENGAGE] (https://greengage-project.github.io/Documentation/tools/sensorsIntegration/) \u2013 HOWTO to off-the-shelf sensors (ATMOtube and sensing apps, e.g. Noisetube ) to be used in the project","title":"Resources"},{"location":"HOWTO%20Thematic%20co-explorations/#step-6-cataloguing-and-storage-of-captured-data","text":"","title":"Step 6: Cataloguing and storage of captured data"},{"location":"HOWTO%20Thematic%20co-explorations/#responsible_8","text":"PST members and GREENGAGE Observers (GOs) with technical skills","title":"Responsible"},{"location":"HOWTO%20Thematic%20co-explorations/#objective_8","text":"This step aims to maintain and continuously populate a centralised repository of metadata for all datasets within the scope of the GO, where datasets are tagged according to the thematic co-explorations where they apply. This repository will serve as a foundational resource for the stakeholders working in the GO, enabling them to manage and analyse the data efficiently. The process will involve the registration of datasets in DataHub, a centralised metadata platform that will act as a single source of truth for all project data. Furthermore, DataHub allows us to add documentation and metadata for each dataset catalogued this way. On the other hand, this step entails storing and/or configuring data access and query mechanisms to the datasets considered, with the support of Apache Druid.","title":"Objective"},{"location":"HOWTO%20Thematic%20co-explorations/#how-can-we-achieve-it_8","text":"Althoughthe datasets were identified in Step 3 and some already is registered in DataHub, it is now time to revise and enrich the catalogue with the final datasets\u2019 metadata that will be used during the analysis of data. The technical team will conduct this process with the support of the pilot owners. The registration process will involve the creation or update (if already existed) of a metadata file for each dataset, which will be uploaded to DataHub. The metadata file will include information such as the dataset's name, description, source, and intended use (thematic co-explorations where it will be used). The technical team will be responsible for creating the ingestion sources for DataHub, while the pilot owners will be responsible for providing the necessary information. Besides, technical participants in the thematic co-exploration aided by the PST will configure mechanisms to store, access and query the gathered data through Apache Druid.","title":"How can we achieve it?"},{"location":"HOWTO%20Thematic%20co-explorations/#resources_8","text":"Documentation of DataHub \u2013 HOWTO for tool to register datasets in DataHub Video of how to create datahub sources \u2013 video explaining how to carry out the metadata declaration process in DataHub (TO BE DONE) Druid \u2013 video explaining how to store and/or access and query the actual data to be used within the themaci co-exploration through Apache Druid (TO BE DONE)","title":"Resources"},{"location":"HOWTO%20Thematic%20co-explorations/#step-7-generation-of-data-analysis-workflows-data-cleaning-pipelining-and-analysis","text":"","title":"Step 7: Generation of data analysis workflows (Data cleaning, pipelining and analysis)"},{"location":"HOWTO%20Thematic%20co-explorations/#responsible_9","text":"PST members and GREENGAGE Observers (GOs) with technical skills","title":"Responsible:"},{"location":"HOWTO%20Thematic%20co-explorations/#objective_9","text":"The objective of this step is a critical phase in the data management process, primarily focused on ensuring the quality and usability of datasets within the project's framework. This stage involves a series of preprocessing steps designed to cleanse the datasets, removing any inconsistencies, errors, or irrelevant information that might hinder analysis. Since some datasets are updated daily, it's essential to establish automated data ingestion workflows, using tools like Apache NiFi, to streamline the continuous flow of data into the main data store, namely Apache Druid. This approach facilitates the efficient and reliable updating of data and ensures that the datasets are in an optimal format and structure for analysis. The ultimate goal of this process is to enhance data accuracy and integrity, thereby enabling a deeper and more accurate understanding of the data, which is vital for informed decision-making and insightful analysis within the project. The outcome of this process will be a set of correlated datasets conveniently interlinked and using shared schemas so that the data analysis and visualization steps can be simplified.","title":"Objective"},{"location":"HOWTO%20Thematic%20co-explorations/#how-can-we-achieve-it_9","text":"To successfully create workflows that can process the data into the desired format we have several ways to do it: creating a NiFi Flow, creating a python script or manually. NiFi flow: Apache NiFi is a powerful tool that allows us to create data pipelines that can be scheduled to run periodically. This tool is very useful for data ingestion, as it allows us to fetch data from APIs, FTP servers, and other sources. Additionally, NiFi can be used to clean and transform data, ensuring that it is in the desired format and structure. Finally, NiFi can be used to load data into Apache Druid, ensuring that the data is correctly ingested into the database. Python script: Develop Python scripts for more complex data transformations that might not be directly feasible within NiFi. This can include data cleaning, normalization, or feature engineering tasks. It can also be integrated with Apache NiFi to run as part of the flow. Furthermore, by using the Apache Druid API we can directly load data into the database. Manually: For simple data transformations, it might be more efficient to perform them manually. This can include tasks such as renaming columns, changing data types, or removing columns. However, this approach is not scalable and should only be used for simple tasks. Once the data is preprocessed and cleaned the analysis may be conducted by combining and exploring the data using Apache Superset and Apache Druid. Apache Superset is a powerful tool that allows us to create interactive dashboards and visualizations that will be explained in the next step. Apache Druid allows us to perform real-time analytics on large datasets. These tools will be used to explore the data and generate insights that will be used to answer the research questions.","title":"How can we achieve it?"},{"location":"HOWTO%20Thematic%20co-explorations/#resources_9","text":"Data quality VRVIS (TBD) Apache NiFi documentation \u2013 HOWTO for tool to create preprocessing flows Apache Druid documentation \u2013 HOWTO for tool to load data into the project's database Apache Superset documentation \u2013 HOWTO for tool to create dashboards and visualizations","title":"Resources"},{"location":"HOWTO%20Thematic%20co-explorations/#step-8-generation-of-thematic-co-exploration-dashboards","text":"","title":"Step 8: Generation of thematic co-exploration dashboards"},{"location":"HOWTO%20Thematic%20co-explorations/#responsible_10","text":"PST members and GREENGAGE Observers (GOs) with technical skills","title":"Responsible"},{"location":"HOWTO%20Thematic%20co-explorations/#objective_10","text":"The objective of generating thematic co-exploration dashboards step of a thematic co-exploration is to develop interactive and insightful tools that transform complex data into accessible visual formats, enhancing comprehension and insight. These dashboards are designed to enable stakeholders, including policymakers, researchers, and citizens, to delve into various data themes and patterns, fostering an environment of collaborative exploration and dialogue. By providing real-time data monitoring and analysis, these dashboards will be instrumental in supporting quick and informed decision-making and policy development. The overarching goal is to ensure transparency and open access to a given thematic co-exploration\u2019s data, thereby promoting inclusive participation and a shared understanding among all project participants.","title":"Objective"},{"location":"HOWTO%20Thematic%20co-explorations/#how-can-we-achieve-it_10","text":"To accomplish this, we will leverage tools like Apache Superset and Apache Druid, as outlined in GREENGAGE\u2019s technical documentation . Apache Superset will allow us to create and share rich data visualizations and dashboards, providing an intuitive interface for users to interact with the data. Apache Druid will be utilized for its real-time data analytics capabilities, ensuring that our dashboards can display the most current data efficiently. Additionally, the integration of Apache NiFi will streamline the data flow into these systems, ensuring a consistent and reliable data pipeline. The combination of these tools, supported by thorough documentation and examples of integration, will enable us to create dynamic and insightful thematic co-exploration dashboards that serve the diverse needs of our project stakeholders.","title":"How can we achieve it?"},{"location":"HOWTO%20Thematic%20co-explorations/#resources_10","text":"ThematicCoExplorationSpec \u2013 continously updated document where to revisit the objectives of the thematic co-exploration and generate dashboards that address them Data quality VRVIS (https://greengage-project.github.io/Documentation/tools/DataQualityDashboard/) \u2013 HOWTO for tool to handle data quality assurance on the gathered datasets Apache Superset documentation \u2013 official documentation for tool to create dashboards and visualizations HOWTO GISAT tool to do these visualizatons (TBD)","title":"Resources"},{"location":"HOWTO%20Thematic%20co-explorations/#step-9-generation-of-storylines-for-wide-dissemination","text":"","title":"Step 9: Generation of storylines for wide dissemination"},{"location":"HOWTO%20Thematic%20co-explorations/#responsible_11","text":"Pilot owners and GREENGAGE Observers (GOs)","title":"Responsible"},{"location":"HOWTO%20Thematic%20co-explorations/#objective_11","text":"The objective in the \"Generation of storylines for wide dissemination\" phase is to craft engaging and accessible narratives that convey the thematic co-exploration's key findings and insights to a broad audience. These storylines are designed to translate complex data and research outcomes into formats that are both engaging and easily understandable by the public. The aim is to underscore the thematic co-exploration's significance and its potential impact on policy-making, community initiatives, and scientific research. By doing so, the goal is to foster increased public awareness and interest in the project, thereby enhancing its overall reach and influence. To achieve wide visibility and impact, these storylines will be disseminated through various media channels, tailored to resonate with diverse audience segments, thereby ensuring the project's findings are communicated effectively and broadly.","title":"Objective"},{"location":"HOWTO%20Thematic%20co-explorations/#how-can-we-achieve-it_11","text":"To effectively generate storylines for wide dissemination, we will utilize a range of tools specified in our project documentation. The Communications and Outreach Team will collaborate closely with the Data Analysis Team, utilizing Apache Superset for creating compelling data visualizations that bring complex data to life in an easily digestible format. These visual elements will be integral in crafting storylines that are both informative and engaging to the public. Additionally, we will leverage WordPress and Discourse platforms to draft, refine, and share these narratives, ensuring they are tailored to resonate with diverse audience segments. The integration of these tools will enable us to effectively communicate the thematic co-exploration's impact and findings through various media channels, from social media to press releases, thereby maximizing reach and fostering greater public engagement with the project's outcomes.","title":"How can we achieve it?"},{"location":"HOWTO%20Thematic%20co-explorations/#resources_11","text":"Course on Storytelling for Citizen Science \u2013 this course gives some tips about how to perform storytelling for Citizen Science projects Presentation of Storytelling and Citizen Science \u2013 this presentation brings about the value of storytelling in Citizen Science. It takes us through several viewpoints and tools you can use to tell your story, considering the needs of the audience, the human instinct for storytelling, how to use it to make your Citizen Science project exciting and meaningful to other people, and encourages you to think about ways to use these tools to reach more audiences","title":"Resources"},{"location":"HOWTO%20Thematic%20co-explorations/#step-10-generation-of-policy-briefs-inputs-for-policy-makers","text":"","title":"Step 10: Generation of policy briefs' inputs for policy makers"},{"location":"HOWTO%20Thematic%20co-explorations/#responsible_12","text":"Policy Analysis Team, Pilot Owners and GREENGAGE Observers (GOs)","title":"Responsible"},{"location":"HOWTO%20Thematic%20co-explorations/#objective_12","text":"The generation of policy briefs for policymakers, a key objective of this project step, involves synthesizing research findings and data into clear, concise, and actionable recommendations. These briefs are crafted to effectively communicate pivotal insights and findings to policymakers, providing them with evidence-based guidance for policy interventions. By directly aligning with current policy debates and challenges, these briefs aim to bridge the gap between research and practical policymaking, offering solutions that are grounded in thorough data analysis. This approach not only enhances the relevancy of the research to ongoing policy discussions but also aims to significantly influence policy decisions and strategies, thus amplifying the overall impact of a given thematic co-exploration.","title":"Objective"},{"location":"HOWTO%20Thematic%20co-explorations/#how-can-we-achieve-it_12","text":"To achieve this, we will first aggregate and analyze the data collected and insights generated throughout the project, using tools like Apache Druid for real-time analytics and Apache Superset for data visualization. These tools will help in identifying key trends and findings that are most relevant to policy-making. The policy analysis team will then work closely with research teams to distill these insights into policy briefs, ensuring that they are both data-driven and aligned with current policy needs, as were identified in Step 0 . Communication specialists will refine these briefs to be clear and compelling, making them accessible to non-specialist policymakers. Throughout this process, a given thematic co-exploration\u2019s coordinators will ensure that the briefs align with the strategic objectives of the thematic co-exploration and effectively convey its potential impact on policy and governance.","title":"How can we achieve it?"},{"location":"HOWTO%20Thematic%20co-explorations/#resources_12","text":"Collaborative Environment \u2013 HOWTO about environment to collaborate in the policy briefs' generation in a centralised repository. The CE will suggest templates to guide the ellaboration of policy briefs. Policy brief guideline \u2013 web document explaining how to format a policy brief Example of Policy brief \u2013 in the domain of Citizen Science that could inspire those embarking in the preparation of a policy brief","title":"Resources"},{"location":"guidelines/","text":"Comprehensive Guidelines for Documenting and Integrating Tools 1. Duplicating the Documentation Template Action : Begin by duplicating the folder docs/tools/TOOLNAME_template . Purpose : This template serves as a foundational structure for documenting new tools, ensuring consistency across your documentation. 2. Describing the Tool in index.md Content Requirements : In index.md , offer a comprehensive overview of the tool. This should include its purpose, key features, and any unique aspects that make it beneficial for your project. Incorporating External Documentation : If there are external resources or official documentation for the tool, link them. This helps users understand the broader context and capabilities of the tool. External resources should be listed in the References section at the end of index.md, check the template provided. Notice that resources like images or PDFs should be stored in subfolder assets . Example Structure : ```markdown # Tool Name ## Introduction to Tool Name [Detailed introduction, including the tool's purpose and general functionality.] ## Features [Detailed list of features, explaining how each aspect adds value to the project.] ## Use Case Illustration [Detailed description of a use case, demonstrating how the tool integrates into workflows or solves specific problems.] ## References [Detailed list of references, include as one reference PDF version of catalogue entry for your tool.] ``` 3. Usage and Integration Guide in integration.md Content : Provide some brief description on how the tools is supposed to be used. If your tool is going to be offered as a SaaS extend such usage highlights, always referring to further documentation. In case your app is designed to be integrated with third party tools, then provide a clear, step-by-step guide for integrating the tool with third-party services. Remember provide a channel to ask for support about the tool. Structure : Usage instructions : Brief usage instructions of the tool. Add references to external resources where further details about usage are given if available. Otherwise, try to make these descriptions self-contained, including snaphshots, URLs and other usage tips. Requesting Credentials : Outline the process for obtaining necessary access credentials. Configuration Steps : Detail the steps for integrating the tool within their systems, including any specific settings or parameters. Testing and Validation : Emphasize the importance of testing the integration and provide guidance on troubleshooting common issues. 4. Proof of Concept Code in examples Purpose : Supply ready-to-use code examples in various programming languages to demonstrate practical integration or programmatic consumption/usage of the tool documented. Details : Each example should include comprehensive comments explaining each step and its relevance to the integration/usage process. 5. Documenting Examples in examples/index.md Structure : For each example, include objective of example, prerequisites, a detailed walkthrough of the project structure, and step-by-step instructions for setup and execution. Clarity : Ensure that the instructions are clear, concise, and suitable for users with varying levels of expertise. 6. Summarizing Tool in docs/index.md Overview : Provide a brief yet informative summary of the tool, highlighting its significance and utility within the project. Example Entry : ```markdown ### Tool Name Directory : /tools/toolname Documentation Link : Tool Documentation Description : A concise summary highlighting the key features and benefits of the tool. Insight into how it integrates with and enhances the project. Integration Example: Node.js Integration Example: Python ``` These guidelines aim to provide a thorough and user-friendly approach to documenting and integrating tools, thereby enhancing the accessibility and usability of your project's documentation.","title":"Guidelines"},{"location":"guidelines/#comprehensive-guidelines-for-documenting-and-integrating-tools","text":"","title":"Comprehensive Guidelines for Documenting and Integrating Tools"},{"location":"guidelines/#1-duplicating-the-documentation-template","text":"Action : Begin by duplicating the folder docs/tools/TOOLNAME_template . Purpose : This template serves as a foundational structure for documenting new tools, ensuring consistency across your documentation.","title":"1. Duplicating the Documentation Template"},{"location":"guidelines/#2-describing-the-tool-in-indexmd","text":"Content Requirements : In index.md , offer a comprehensive overview of the tool. This should include its purpose, key features, and any unique aspects that make it beneficial for your project. Incorporating External Documentation : If there are external resources or official documentation for the tool, link them. This helps users understand the broader context and capabilities of the tool. External resources should be listed in the References section at the end of index.md, check the template provided. Notice that resources like images or PDFs should be stored in subfolder assets . Example Structure : ```markdown # Tool Name ## Introduction to Tool Name [Detailed introduction, including the tool's purpose and general functionality.] ## Features [Detailed list of features, explaining how each aspect adds value to the project.] ## Use Case Illustration [Detailed description of a use case, demonstrating how the tool integrates into workflows or solves specific problems.] ## References [Detailed list of references, include as one reference PDF version of catalogue entry for your tool.] ```","title":"2. Describing the Tool in index.md"},{"location":"guidelines/#3-usage-and-integration-guide-in-integrationmd","text":"Content : Provide some brief description on how the tools is supposed to be used. If your tool is going to be offered as a SaaS extend such usage highlights, always referring to further documentation. In case your app is designed to be integrated with third party tools, then provide a clear, step-by-step guide for integrating the tool with third-party services. Remember provide a channel to ask for support about the tool. Structure : Usage instructions : Brief usage instructions of the tool. Add references to external resources where further details about usage are given if available. Otherwise, try to make these descriptions self-contained, including snaphshots, URLs and other usage tips. Requesting Credentials : Outline the process for obtaining necessary access credentials. Configuration Steps : Detail the steps for integrating the tool within their systems, including any specific settings or parameters. Testing and Validation : Emphasize the importance of testing the integration and provide guidance on troubleshooting common issues.","title":"3. Usage and Integration Guide in integration.md"},{"location":"guidelines/#4-proof-of-concept-code-in-examples","text":"Purpose : Supply ready-to-use code examples in various programming languages to demonstrate practical integration or programmatic consumption/usage of the tool documented. Details : Each example should include comprehensive comments explaining each step and its relevance to the integration/usage process.","title":"4. Proof of Concept Code in examples"},{"location":"guidelines/#5-documenting-examples-in-examplesindexmd","text":"Structure : For each example, include objective of example, prerequisites, a detailed walkthrough of the project structure, and step-by-step instructions for setup and execution. Clarity : Ensure that the instructions are clear, concise, and suitable for users with varying levels of expertise.","title":"5. Documenting Examples in examples/index.md"},{"location":"guidelines/#6-summarizing-tool-in-docsindexmd","text":"Overview : Provide a brief yet informative summary of the tool, highlighting its significance and utility within the project. Example Entry : ```markdown ### Tool Name Directory : /tools/toolname Documentation Link : Tool Documentation Description : A concise summary highlighting the key features and benefits of the tool. Insight into how it integrates with and enhances the project. Integration Example: Node.js Integration Example: Python ``` These guidelines aim to provide a thorough and user-friendly approach to documenting and integrating tools, thereby enhancing the accessibility and usability of your project's documentation.","title":"6. Summarizing Tool in docs/index.md"},{"location":"thematic_coexploration_example/","text":"Thematic co-exploration example Thematic co-exploration contextualisation Thematic co-exploration in the context of Citizen Observatories refers to a collaborative approach where citizens actively participate alongside scientists and other stakeholders in exploring specific themes or topics related to environmental monitoring and observation. Citizen Observatories are organizations that leverage the collective efforts of individuals, often non-scientists, to gather, share, and analyse environmental data, typically facilitated by digital tools and technologies. In a thematic co-exploration, the focus is on a particular theme or subject matter, such as air quality, water pollution, biodiversity, or climate change phenomena. The key aspects of this approach include: Collaborative Research: Citizens collaborate with scientists, environmental experts, policy-makers, and other stakeholders. This collaboration is not just about data collection but also involves jointly defining research questions, methodologies, and analysis. Citizen Engagement: Citizens are not merely data collectors; they are integral to the research process. They contribute through observations, local knowledge, and experiences, thereby adding valuable context and richness to scientific data. Mutual Learning: There is a reciprocal transfer of knowledge. Scientists can learn from the lived experiences and local knowledge of citizens, while citizens gain a better understanding of scientific methods and environmental issues. Empowerment and Ownership: By engaging in the research process, citizens gain a sense of ownership and empowerment. This can lead to increased awareness and action on environmental issues at the local level. Technology Use: Digital tools such as mobile apps, collaborative online platforms, and sensor technologies, e.g. wearables, often facilitate thematic co-exploration. These tools enable easy data collection, sharing, and visualisation. Community Building: This approach fosters community engagement and networking, as it brings together people with shared interests in specific environmental issues. Policy Impact: The insights gained from thematic co-exploration can inform environmental policies and decision-making processes, making them more reflective of local needs and conditions. In summary, a thematic co-exploration in Citizen Observatories represents a participatory, inclusive approach to environmental research and monitoring, emphasising collaboration, mutual learning, and community engagement. Citizen Observer Journey In GREENGAGE, we propose a Citizen Observer Journey that represents the pathway that the citizen observers follow when conducting a thematic co-exploration. This journey is a structured pathway designed to empower citizens and stakeholders to actively engage in environmental observation and decision-making processes. It unfolds in three distinct yet interconnected phases, each integral to the overall success and impact of the GREENGAGE thematic co-explorations. These phases are: Community and Co-production Process Management: Throughout this phase, the emphasis is on building a strong, informed, and active community which collaborates through a co-production process. Data Crowdsourcing and Capture: Based on the groundwork of the previous phase, e.g. definition of hypothesis, research questions formulation or datasets selection, among others, this phase materialises into concrete data collection activities. It is characterized by active participation, leveraging technology to gather vital environmental data. Data Analysis and Insights Generation: In this phase the collected data is transformed into actionable insights. This phase is where the data, once transformed in actionable information, becomes a powerful tool for understanding and influencing environmental policy. The following figure shows the Citizen Observer Journey and the tools that are used in the different phases that it comprises: Example of a simple thematic co-exploration This example explains the steps to conduct a complete thematic co-exploration process using some of the tools provided by the GREENGAGE platform. To support the explanation, we will employ a video, however, in the video there are only covered the steps belonging to the third phase of the Citizen Observer Journey. Thus, we will complement the video with a textual explanation of the steps covered in the first and second phases. Objective The main objective of this thematic co-exploration is to analyse the air quality data from the city of Bristol. The analysis is conducted to identify the air pollution in the city and the different factors that may affect it. Thus, we gather the data from the different sensors deployed in the city and load them into the database. Then we analyse the data and create visualisations to facilitate the understanding of the data. Finally, we load the data and visualisations into the data catalogue to facilitate the access to them and foster the dissemination and adaption of the results to maximize impact. Dataset For this example we employed a single dataset from the Air Quality Data Continuous dataset. This dataset contains the air quality data from the city of Bristol. The data is collected from the different sensors deployed in the city. The data is collected every hour and it is stored in the dataset. The dataset contains the following fields: Date_Time Site_ID NOx NO2 NO PM10 O3 Temperature ObjectId ObjectId2 NVPM10 VPM10 NVPM2_5 PM2_5 VPM2_5 CO RH Pressure SO2 Note that not all the fields are present in all the rows. Video Some of the steps below are linked to the time in the video where they are covered. You can find the video in the following link or by clicking on the image below: Steps covered in this thematic co-exploration Thematic co-exploration specification : In this step, we should specify the thematic co-exploration that we want to conduct. By employing the Collaborative Environment tool, the thematic co-exploration owners may specify the phases, objectives and tasks that should be conducted in the thematic co-exploration. Furthermore, the tool allows to specify the different roles upon which they will participate in the thematic co-exploration and the users assigned to each role. Community building : Through Discourse and Wordpress tools, the thematic co-exploration owners may create a community around the thematic co-exploration. The community thus could discuss the different aspects of the thematic co-exploration and the different tasks that should be conducted. Furthermore, through it the community should share the results of the thematic co-exploration and discuss them. Data collection : In this step, we collect the data from the different sensors that deployed in the city. In this example, we use the Apache NiFi tool to collect the data from the open API because it cannot be accessed by Apache Druid directly. Thus, we created a NiFi flow that iteratively gathers the data from the API and stores in a local file. However, in another scenario, tools as MindEarth , MODE or GREENGAGEs IoT sensors could be used to collect the data. Data ingestion and storage : In this step, we ingest the data from the local file to the database. In this example, we use the Apache Druid datastore tool to ingest the data from the local file. Data visualisation and dashboard creation : In this step, we load the data from the database into Apache Superset to create visualisations that may support the decision-making process. Furthermore, we create a dashboard aggregating the different visualisations to facilitate the understanding of the data. This visualisations, alternatively, can also be created using the Data Quality Dashboard tool. Data catalogue : In this step, we add references to the data and the visualisations created in previous steps to the DataHub tool. DataHub allows to create a catalog of data that shows the schema, documentation, lineage, properties and common queries that can be done to the data. Furthermore, the visualisations can also be referenced from this catalog and the connections between them can be established. It is a good tool where a given thematic co-exploration results can be traced. Insights generation : In this step the participants of the thematic co-exploration analyse the data and the visualisations generated from it and extract insights from it. These insights may be shared with the community by using the tools covered in the Community and Co-production Process Management phase of the Citizen Observer Journey.","title":"Thematic co-exploration example"},{"location":"thematic_coexploration_example/#thematic-co-exploration-example","text":"","title":"Thematic co-exploration example"},{"location":"thematic_coexploration_example/#thematic-co-exploration-contextualisation","text":"Thematic co-exploration in the context of Citizen Observatories refers to a collaborative approach where citizens actively participate alongside scientists and other stakeholders in exploring specific themes or topics related to environmental monitoring and observation. Citizen Observatories are organizations that leverage the collective efforts of individuals, often non-scientists, to gather, share, and analyse environmental data, typically facilitated by digital tools and technologies. In a thematic co-exploration, the focus is on a particular theme or subject matter, such as air quality, water pollution, biodiversity, or climate change phenomena. The key aspects of this approach include: Collaborative Research: Citizens collaborate with scientists, environmental experts, policy-makers, and other stakeholders. This collaboration is not just about data collection but also involves jointly defining research questions, methodologies, and analysis. Citizen Engagement: Citizens are not merely data collectors; they are integral to the research process. They contribute through observations, local knowledge, and experiences, thereby adding valuable context and richness to scientific data. Mutual Learning: There is a reciprocal transfer of knowledge. Scientists can learn from the lived experiences and local knowledge of citizens, while citizens gain a better understanding of scientific methods and environmental issues. Empowerment and Ownership: By engaging in the research process, citizens gain a sense of ownership and empowerment. This can lead to increased awareness and action on environmental issues at the local level. Technology Use: Digital tools such as mobile apps, collaborative online platforms, and sensor technologies, e.g. wearables, often facilitate thematic co-exploration. These tools enable easy data collection, sharing, and visualisation. Community Building: This approach fosters community engagement and networking, as it brings together people with shared interests in specific environmental issues. Policy Impact: The insights gained from thematic co-exploration can inform environmental policies and decision-making processes, making them more reflective of local needs and conditions. In summary, a thematic co-exploration in Citizen Observatories represents a participatory, inclusive approach to environmental research and monitoring, emphasising collaboration, mutual learning, and community engagement.","title":"Thematic co-exploration contextualisation"},{"location":"thematic_coexploration_example/#citizen-observer-journey","text":"In GREENGAGE, we propose a Citizen Observer Journey that represents the pathway that the citizen observers follow when conducting a thematic co-exploration. This journey is a structured pathway designed to empower citizens and stakeholders to actively engage in environmental observation and decision-making processes. It unfolds in three distinct yet interconnected phases, each integral to the overall success and impact of the GREENGAGE thematic co-explorations. These phases are: Community and Co-production Process Management: Throughout this phase, the emphasis is on building a strong, informed, and active community which collaborates through a co-production process. Data Crowdsourcing and Capture: Based on the groundwork of the previous phase, e.g. definition of hypothesis, research questions formulation or datasets selection, among others, this phase materialises into concrete data collection activities. It is characterized by active participation, leveraging technology to gather vital environmental data. Data Analysis and Insights Generation: In this phase the collected data is transformed into actionable insights. This phase is where the data, once transformed in actionable information, becomes a powerful tool for understanding and influencing environmental policy. The following figure shows the Citizen Observer Journey and the tools that are used in the different phases that it comprises:","title":"Citizen Observer Journey"},{"location":"thematic_coexploration_example/#example-of-a-simple-thematic-co-exploration","text":"This example explains the steps to conduct a complete thematic co-exploration process using some of the tools provided by the GREENGAGE platform. To support the explanation, we will employ a video, however, in the video there are only covered the steps belonging to the third phase of the Citizen Observer Journey. Thus, we will complement the video with a textual explanation of the steps covered in the first and second phases.","title":"Example of a simple thematic co-exploration"},{"location":"thematic_coexploration_example/#objective","text":"The main objective of this thematic co-exploration is to analyse the air quality data from the city of Bristol. The analysis is conducted to identify the air pollution in the city and the different factors that may affect it. Thus, we gather the data from the different sensors deployed in the city and load them into the database. Then we analyse the data and create visualisations to facilitate the understanding of the data. Finally, we load the data and visualisations into the data catalogue to facilitate the access to them and foster the dissemination and adaption of the results to maximize impact.","title":"Objective"},{"location":"thematic_coexploration_example/#dataset","text":"For this example we employed a single dataset from the Air Quality Data Continuous dataset. This dataset contains the air quality data from the city of Bristol. The data is collected from the different sensors deployed in the city. The data is collected every hour and it is stored in the dataset. The dataset contains the following fields: Date_Time Site_ID NOx NO2 NO PM10 O3 Temperature ObjectId ObjectId2 NVPM10 VPM10 NVPM2_5 PM2_5 VPM2_5 CO RH Pressure SO2 Note that not all the fields are present in all the rows.","title":"Dataset"},{"location":"thematic_coexploration_example/#video","text":"Some of the steps below are linked to the time in the video where they are covered. You can find the video in the following link or by clicking on the image below:","title":"Video"},{"location":"thematic_coexploration_example/#steps-covered-in-this-thematic-co-exploration","text":"Thematic co-exploration specification : In this step, we should specify the thematic co-exploration that we want to conduct. By employing the Collaborative Environment tool, the thematic co-exploration owners may specify the phases, objectives and tasks that should be conducted in the thematic co-exploration. Furthermore, the tool allows to specify the different roles upon which they will participate in the thematic co-exploration and the users assigned to each role. Community building : Through Discourse and Wordpress tools, the thematic co-exploration owners may create a community around the thematic co-exploration. The community thus could discuss the different aspects of the thematic co-exploration and the different tasks that should be conducted. Furthermore, through it the community should share the results of the thematic co-exploration and discuss them. Data collection : In this step, we collect the data from the different sensors that deployed in the city. In this example, we use the Apache NiFi tool to collect the data from the open API because it cannot be accessed by Apache Druid directly. Thus, we created a NiFi flow that iteratively gathers the data from the API and stores in a local file. However, in another scenario, tools as MindEarth , MODE or GREENGAGEs IoT sensors could be used to collect the data. Data ingestion and storage : In this step, we ingest the data from the local file to the database. In this example, we use the Apache Druid datastore tool to ingest the data from the local file. Data visualisation and dashboard creation : In this step, we load the data from the database into Apache Superset to create visualisations that may support the decision-making process. Furthermore, we create a dashboard aggregating the different visualisations to facilitate the understanding of the data. This visualisations, alternatively, can also be created using the Data Quality Dashboard tool. Data catalogue : In this step, we add references to the data and the visualisations created in previous steps to the DataHub tool. DataHub allows to create a catalog of data that shows the schema, documentation, lineage, properties and common queries that can be done to the data. Furthermore, the visualisations can also be referenced from this catalog and the connections between them can be established. It is a good tool where a given thematic co-exploration results can be traced. Insights generation : In this step the participants of the thematic co-exploration analyse the data and the visualisations generated from it and extract insights from it. These insights may be shared with the community by using the tools covered in the Community and Co-production Process Management phase of the Citizen Observer Journey.","title":"Steps covered in this thematic co-exploration"},{"location":"tools/DataQualityDashboard/","text":"Data Quality Dashboard Introduction The data quality dashboard is a web-based tool that allows users to monitor the quality of collected data. It provides both visual and numeric representations of data quality through visual cues, quality metrics, and statistics, allowing users to quickly identify potential issues. The dashboard also offers a variety of features for data exploration and analysis, including the ability to filter data by time, location, and other parameters. Features of Data Quality Dashboard Interactive Dashboard : The dashboard reacts to user input, allowing them to interact with the data and the selection of graphs and charts. Data Quality Visualisation: The dashboard provides visual cues to highlight data quality issues, such as missing values, outliers, and inconsistencies. Data Preview: Users can preview current data in a tabular format, allowing them to quickly identify potential issues. Data Quality Metrics: It offers a variety of data quality metrics, including completeness, accuracy and consistency. Descriptive Statistics: The dashboard provides descriptive statistics window for better understanding of the data in question. Data Filtering: Users can filter data by time, location, and other parameters to perform in-depth analysis. Data Export: The dashboard allows users to export data in various formats, including CSV, JSON, and Excel. Use Case Scenario Context: In an urban citizen observatory, members of the community actively participate in monitoring and analysing environmental conditions. One of the critical concerns in many cities is air quality, which directly impacts public health and quality of life. Objective: To monitor, analyse, and visualise air quality data across different parts of the city to identify pollution hotspots, understand temporal trends, and engage the public in environmental awareness and decision-making processes. Implementation Using Data Quality Dashboard: 1) Data Collection: - Deploy HOPU sensors across the city to measure air quality indicators like PM2.5, PM10, NO2, CO2, and Ozone. - Sensors transmit data to a centralised database, storing time-stamped readings and location data. 2) Data Integration with Data Quality Dashboard: - Connect the dashboard to the database where sensor data is stored. 3) Exploratory Data Analysis: - Use the dashboard to explore data quality issues, such as missing values, outliers, and inconsistencies. - Identify potential causes of data quality issues, such as sensor malfunction or data transmission errors. - Use the insights to improve data collection processes and ensure data quality.","title":"Data Quality Dashboard"},{"location":"tools/DataQualityDashboard/#data-quality-dashboard","text":"","title":"Data Quality Dashboard"},{"location":"tools/DataQualityDashboard/#introduction","text":"The data quality dashboard is a web-based tool that allows users to monitor the quality of collected data. It provides both visual and numeric representations of data quality through visual cues, quality metrics, and statistics, allowing users to quickly identify potential issues. The dashboard also offers a variety of features for data exploration and analysis, including the ability to filter data by time, location, and other parameters.","title":"Introduction"},{"location":"tools/DataQualityDashboard/#features-of-data-quality-dashboard","text":"Interactive Dashboard : The dashboard reacts to user input, allowing them to interact with the data and the selection of graphs and charts. Data Quality Visualisation: The dashboard provides visual cues to highlight data quality issues, such as missing values, outliers, and inconsistencies. Data Preview: Users can preview current data in a tabular format, allowing them to quickly identify potential issues. Data Quality Metrics: It offers a variety of data quality metrics, including completeness, accuracy and consistency. Descriptive Statistics: The dashboard provides descriptive statistics window for better understanding of the data in question. Data Filtering: Users can filter data by time, location, and other parameters to perform in-depth analysis. Data Export: The dashboard allows users to export data in various formats, including CSV, JSON, and Excel.","title":"Features of Data Quality Dashboard"},{"location":"tools/DataQualityDashboard/#use-case-scenario","text":"","title":"Use Case Scenario"},{"location":"tools/DataQualityDashboard/#context","text":"In an urban citizen observatory, members of the community actively participate in monitoring and analysing environmental conditions. One of the critical concerns in many cities is air quality, which directly impacts public health and quality of life.","title":"Context:"},{"location":"tools/DataQualityDashboard/#objective","text":"To monitor, analyse, and visualise air quality data across different parts of the city to identify pollution hotspots, understand temporal trends, and engage the public in environmental awareness and decision-making processes.","title":"Objective:"},{"location":"tools/DataQualityDashboard/#implementation-using-data-quality-dashboard","text":"1) Data Collection: - Deploy HOPU sensors across the city to measure air quality indicators like PM2.5, PM10, NO2, CO2, and Ozone. - Sensors transmit data to a centralised database, storing time-stamped readings and location data. 2) Data Integration with Data Quality Dashboard: - Connect the dashboard to the database where sensor data is stored. 3) Exploratory Data Analysis: - Use the dashboard to explore data quality issues, such as missing values, outliers, and inconsistencies. - Identify potential causes of data quality issues, such as sensor malfunction or data transmission errors. - Use the insights to improve data collection processes and ensure data quality.","title":"Implementation Using Data Quality Dashboard:"},{"location":"tools/Discourse/","text":"Discourse Introduction Description : Discourse is a free, open-source forum software that plays a crucial role in modern tech environments by enabling thoughtful discussion and meaningful connections. It integrates seamlessly with various platforms, enhancing both application functionality and community engagement. Objective : The integration of Discourse can significantly benefit development projects by offering robust collaboration and communication tools. It enhances application security with features like spam blocking and two-factor authentication, ensuring a safe and productive environment for users. Features of Discourse List of Features : Conversations, Not Pages : Utilizes just-in-time loading for seamless conversation flow. Real-Time Chat : Enables informal chats alongside more structured discussions. AI Integration : Enhances user experience and moderation with AI capabilities. Customizable User Experience : Offers a personalized interface with adjustable preferences. Mobile Compatibility : Fully functional on high-resolution touch devices. Link Expansion : Automatically enhances shared links with additional context. Single Sign-On : Facilitates easy integration with existing login systems. Trust System : Rewards regular members with additional abilities. Community Moderation : Allows community-driven content moderation. Spam Blocking : Integrates Akismet spam protection. Social Login : Supports common social media logins. Topic Summarization : Condenses long discussions to highlights. Badges : Encourages positive behaviors with a badge system. Emoji Support : Offers a searchable list of emojis. Email Interaction : Allows replying to notifications via email. Two-Factor Authentication : Enhances account security. Admin Dashboard : Provides essential community health metrics. Official Plugins : Supports various additional features. Comprehensive API : Facilitates broad functionality access. Open Source : Ensures transparency and adaptability. One-Click Upgrades : Simplifies version updates. Additional Features : Includes support for multiple languages, SEO optimization, various content formatting options, automatic backups, blog integration, and more","title":"Discourse"},{"location":"tools/Discourse/#discourse","text":"","title":"Discourse"},{"location":"tools/Discourse/#introduction","text":"Description : Discourse is a free, open-source forum software that plays a crucial role in modern tech environments by enabling thoughtful discussion and meaningful connections. It integrates seamlessly with various platforms, enhancing both application functionality and community engagement. Objective : The integration of Discourse can significantly benefit development projects by offering robust collaboration and communication tools. It enhances application security with features like spam blocking and two-factor authentication, ensuring a safe and productive environment for users.","title":"Introduction"},{"location":"tools/Discourse/#features-of-discourse","text":"List of Features : Conversations, Not Pages : Utilizes just-in-time loading for seamless conversation flow. Real-Time Chat : Enables informal chats alongside more structured discussions. AI Integration : Enhances user experience and moderation with AI capabilities. Customizable User Experience : Offers a personalized interface with adjustable preferences. Mobile Compatibility : Fully functional on high-resolution touch devices. Link Expansion : Automatically enhances shared links with additional context. Single Sign-On : Facilitates easy integration with existing login systems. Trust System : Rewards regular members with additional abilities. Community Moderation : Allows community-driven content moderation. Spam Blocking : Integrates Akismet spam protection. Social Login : Supports common social media logins. Topic Summarization : Condenses long discussions to highlights. Badges : Encourages positive behaviors with a badge system. Emoji Support : Offers a searchable list of emojis. Email Interaction : Allows replying to notifications via email. Two-Factor Authentication : Enhances account security. Admin Dashboard : Provides essential community health metrics. Official Plugins : Supports various additional features. Comprehensive API : Facilitates broad functionality access. Open Source : Ensures transparency and adaptability. One-Click Upgrades : Simplifies version updates. Additional Features : Includes support for multiple languages, SEO optimization, various content formatting options, automatic backups, blog integration, and more","title":"Features of Discourse"},{"location":"tools/Discourse/integration/","text":"Discourse Plugin Integration Guide with Keycloak SSO on Bitnami Discourse is a popular open-source discussion platform designed for modern community engagement. It is highly customizable, supporting numerous plugins for extended functionality. For detailed usage instructions and deployment options, refer to the Discourse User Manual . Integrating Discourse into the GREENGAGE engine amplifies its capabilities by enabling seamless Single Sign-On (SSO) authentication through Keycloak. This guide provides steps to integrate Discourse with Keycloak SSO, ensuring a smooth authentication process for users across various GREENGAGE services. Requesting Access Credentials To integrate Discourse with Keycloak SSO, you'll need to request access credentials. ( Follow this guide ) Receiving Credentials Following your request, you'll receive the necessary credentials via email. These credentials will enable the setup of SSO with Keycloak in your Discourse installation. Configuring Your Service Installing OpenID Connect Plugin for Keycloak Integration : Navigate to the Discourse installation directory: cd /opt/bitnami/discourse Install the OpenID Connect plugin: RAILS_ENV=production bundle exec rake plugin:install repo=https://github.com/discourse/discourse-openid-connect Precompile the assets for the plugin to take effect: RAILS_ENV=production bundle exec rake assets:precompile Detailed guide for installing plugins on Discourse over Bitnami can be found here . Disabling Email Verification : To disable the email verification typically sent upon first-time registration in Discourse, use the following plugin: cd /opt/bitnami/discourse/plugins git clone https://github.com/codergautam/disable-email-verification-discourse.git cd .. RAILS_ENV=production bundle install RAILS_ENV=production bundle exec rake assets:precompile These steps will set up Discourse for Keycloak SSO and disable the default email verification, aligning the authentication process with the Keycloak-managed user accounts. Step 4: Testing the Integration Test the SSO integration by attempting to log in to Discourse using credentials managed by Keycloak. For testing API integrations, verify if you can call the Discourse API using the token obtained from Keycloak. Support and Contact For additional support, please reach out to greengage.admin@deusto.es. Important Notes: Always keep your credentials confidential and secure. In case of any security concerns or compromised credentials, immediately contact the support team. Regularly update your integration setup to comply with the latest security standards and feature enhancements.","title":"Discourse Plugin Integration Guide with Keycloak SSO on Bitnami"},{"location":"tools/Discourse/integration/#discourse-plugin-integration-guide-with-keycloak-sso-on-bitnami","text":"Discourse is a popular open-source discussion platform designed for modern community engagement. It is highly customizable, supporting numerous plugins for extended functionality. For detailed usage instructions and deployment options, refer to the Discourse User Manual . Integrating Discourse into the GREENGAGE engine amplifies its capabilities by enabling seamless Single Sign-On (SSO) authentication through Keycloak. This guide provides steps to integrate Discourse with Keycloak SSO, ensuring a smooth authentication process for users across various GREENGAGE services. Requesting Access Credentials To integrate Discourse with Keycloak SSO, you'll need to request access credentials. ( Follow this guide ) Receiving Credentials Following your request, you'll receive the necessary credentials via email. These credentials will enable the setup of SSO with Keycloak in your Discourse installation. Configuring Your Service Installing OpenID Connect Plugin for Keycloak Integration : Navigate to the Discourse installation directory: cd /opt/bitnami/discourse Install the OpenID Connect plugin: RAILS_ENV=production bundle exec rake plugin:install repo=https://github.com/discourse/discourse-openid-connect Precompile the assets for the plugin to take effect: RAILS_ENV=production bundle exec rake assets:precompile Detailed guide for installing plugins on Discourse over Bitnami can be found here . Disabling Email Verification : To disable the email verification typically sent upon first-time registration in Discourse, use the following plugin: cd /opt/bitnami/discourse/plugins git clone https://github.com/codergautam/disable-email-verification-discourse.git cd .. RAILS_ENV=production bundle install RAILS_ENV=production bundle exec rake assets:precompile These steps will set up Discourse for Keycloak SSO and disable the default email verification, aligning the authentication process with the Keycloak-managed user accounts. Step 4: Testing the Integration Test the SSO integration by attempting to log in to Discourse using credentials managed by Keycloak. For testing API integrations, verify if you can call the Discourse API using the token obtained from Keycloak. Support and Contact For additional support, please reach out to greengage.admin@deusto.es. Important Notes: Always keep your credentials confidential and secure. In case of any security concerns or compromised credentials, immediately contact the support team. Regularly update your integration setup to comply with the latest security standards and feature enhancements.","title":"Discourse Plugin Integration Guide with Keycloak SSO on Bitnami"},{"location":"tools/Discourse/examples/","text":"Discourse Deployment Guide Integrating Discourse in Docker Environment Introduction Description : Discourse is a versatile forum software ideal for creating and managing online communities. Its significance in development lies in its ability to foster engagement and provide a platform for discussions. Objective : Deploying Discourse using Docker enhances application scalability, security, and ease of maintenance. Prerequisites Docker and Docker Compose installed. Basic knowledge of Docker and YAML syntax. Project Structure Source : Discourse Docker Compose Example Directory Layout : postgresql : PostgreSQL container configuration. redis : Redis container configuration. discourse : Main Discourse application container setup. sidekiq : Sidekiq container for background jobs. Step-by-Step Setup and Configuration Download or Clone the Docker Compose YAML File : Obtain the Docker Compose file for Discourse deployment. Replace Placeholders with Actual Values : [HERE_YOUR_DOMAIN] : Enter the domain where your Discourse instance will be accessible. [HERE_ADMIN_EMAIL] : Provide the email address for the Discourse administrator account. [HERE_ADMIN_USERNAME] : Set the username for the Discourse administrator. [HERE_ADMIN_PASSWORD] : Choose a secure password for the Discourse administrator account. [HERE_SMTP_HOST] : Specify the SMTP server host for sending emails. [HERE_SMTP_PORT] : Enter the port number used by your SMTP server. [HERE_SMTP_USER] : Provide the SMTP username for email authentication. [HERE_SMTP_PASSWORD] : Set the SMTP password for the email account. [HERE_SMTP_PROTOCOL] : Indicate the email protocol used by your SMTP server (e.g., TLS). Each of these placeholders corresponds to essential configuration settings for your Discourse deployment, ensuring proper functionality and access to the application. Running the Services Run docker-compose up to start all services. Accessing the Application Access Discourse at the specified domain after successful deployment. Additional Configuration (Optional) Integrating Keycloak for Single Sign-On (SSO) : To enable SSO with Keycloak in Discourse: cd /opt/bitnami/discourse RAILS_ENV=production bundle exec rake plugin:install repo=https://github.com/discourse/discourse-openid-connect RAILS_ENV=production bundle exec rake assets:precompile This configures Discourse to use Keycloak as the authentication provider, allowing seamless login across connected services. Disabling Email Verification : Given that Keycloak is used for user management, you might want to disable the default email verification sent by Discourse: cd /opt/bitnami/discourse/plugins git clone https://github.com/codergautam/disable-email-verification-discourse.git cd .. RAILS_ENV=production bundle install RAILS_ENV=production bundle exec rake assets:precompile This step ensures that the email verification step is bypassed, streamlining the registration process in a Keycloak-managed environment. Troubleshooting Issues with User and Site Configuration : If you encounter problems related to configuring users or site settings in your Docker-deployed Discourse instance, refer to the Bitnami Discourse Container Configuration Guide . This resource provides detailed instructions and examples for configuring various aspects of the Discourse application within a Docker environment. General Docker Deployment Issues : For common Docker-related problems, such as container networking, volume management, or image retrieval issues, consult Docker's official documentation or community forums for guidance and solutions. Support and Additional Resources Support Channels : Community forums, email support. External Documentation : Docker Documentation , Discourse Official Documentation . Docker Integration for Discourse Introduction This section focuses on integrating the Discourse forum software within a Docker containerized environment, leveraging Docker's capabilities for efficient deployment and management. Prerequisites Knowledge of Docker and Docker Compose. Familiarity with YAML syntax. Project Structure The Docker Compose file defines multiple services: postgresql for the database, redis for caching, discourse for the main application, and sidekiq for background jobs. The volumes and networks sections manage data persistence and inter-container communication. Steps for Integration Clone or create a Docker Compose file similar to the provided example. Customize the environment variables and service settings as per your requirements. Running the Example Execute docker-compose up in the directory containing your Docker Compose file to start the Discourse environment. Accessing the Application Once the containers are running, access Discourse through the specified domain configured in the DISCOURSE_HOST environment variable of the discourse service. For specific details, always refer to the latest official documentation of the tools involved.","title":"Discourse Deployment Guide"},{"location":"tools/Discourse/examples/#discourse-deployment-guide","text":"","title":"Discourse Deployment Guide"},{"location":"tools/Discourse/examples/#integrating-discourse-in-docker-environment","text":"","title":"Integrating Discourse in Docker Environment"},{"location":"tools/Discourse/examples/#introduction","text":"Description : Discourse is a versatile forum software ideal for creating and managing online communities. Its significance in development lies in its ability to foster engagement and provide a platform for discussions. Objective : Deploying Discourse using Docker enhances application scalability, security, and ease of maintenance.","title":"Introduction"},{"location":"tools/Discourse/examples/#prerequisites","text":"Docker and Docker Compose installed. Basic knowledge of Docker and YAML syntax.","title":"Prerequisites"},{"location":"tools/Discourse/examples/#project-structure","text":"Source : Discourse Docker Compose Example Directory Layout : postgresql : PostgreSQL container configuration. redis : Redis container configuration. discourse : Main Discourse application container setup. sidekiq : Sidekiq container for background jobs.","title":"Project Structure"},{"location":"tools/Discourse/examples/#step-by-step-setup-and-configuration","text":"Download or Clone the Docker Compose YAML File : Obtain the Docker Compose file for Discourse deployment. Replace Placeholders with Actual Values : [HERE_YOUR_DOMAIN] : Enter the domain where your Discourse instance will be accessible. [HERE_ADMIN_EMAIL] : Provide the email address for the Discourse administrator account. [HERE_ADMIN_USERNAME] : Set the username for the Discourse administrator. [HERE_ADMIN_PASSWORD] : Choose a secure password for the Discourse administrator account. [HERE_SMTP_HOST] : Specify the SMTP server host for sending emails. [HERE_SMTP_PORT] : Enter the port number used by your SMTP server. [HERE_SMTP_USER] : Provide the SMTP username for email authentication. [HERE_SMTP_PASSWORD] : Set the SMTP password for the email account. [HERE_SMTP_PROTOCOL] : Indicate the email protocol used by your SMTP server (e.g., TLS). Each of these placeholders corresponds to essential configuration settings for your Discourse deployment, ensuring proper functionality and access to the application.","title":"Step-by-Step Setup and Configuration"},{"location":"tools/Discourse/examples/#running-the-services","text":"Run docker-compose up to start all services.","title":"Running the Services"},{"location":"tools/Discourse/examples/#accessing-the-application","text":"Access Discourse at the specified domain after successful deployment.","title":"Accessing the Application"},{"location":"tools/Discourse/examples/#additional-configuration-optional","text":"Integrating Keycloak for Single Sign-On (SSO) : To enable SSO with Keycloak in Discourse: cd /opt/bitnami/discourse RAILS_ENV=production bundle exec rake plugin:install repo=https://github.com/discourse/discourse-openid-connect RAILS_ENV=production bundle exec rake assets:precompile This configures Discourse to use Keycloak as the authentication provider, allowing seamless login across connected services. Disabling Email Verification : Given that Keycloak is used for user management, you might want to disable the default email verification sent by Discourse: cd /opt/bitnami/discourse/plugins git clone https://github.com/codergautam/disable-email-verification-discourse.git cd .. RAILS_ENV=production bundle install RAILS_ENV=production bundle exec rake assets:precompile This step ensures that the email verification step is bypassed, streamlining the registration process in a Keycloak-managed environment.","title":"Additional Configuration (Optional)"},{"location":"tools/Discourse/examples/#troubleshooting","text":"Issues with User and Site Configuration : If you encounter problems related to configuring users or site settings in your Docker-deployed Discourse instance, refer to the Bitnami Discourse Container Configuration Guide . This resource provides detailed instructions and examples for configuring various aspects of the Discourse application within a Docker environment. General Docker Deployment Issues : For common Docker-related problems, such as container networking, volume management, or image retrieval issues, consult Docker's official documentation or community forums for guidance and solutions.","title":"Troubleshooting"},{"location":"tools/Discourse/examples/#support-and-additional-resources","text":"Support Channels : Community forums, email support. External Documentation : Docker Documentation , Discourse Official Documentation .","title":"Support and Additional Resources"},{"location":"tools/Discourse/examples/#docker-integration-for-discourse","text":"","title":"Docker Integration for Discourse"},{"location":"tools/Discourse/examples/#introduction_1","text":"This section focuses on integrating the Discourse forum software within a Docker containerized environment, leveraging Docker's capabilities for efficient deployment and management.","title":"Introduction"},{"location":"tools/Discourse/examples/#prerequisites_1","text":"Knowledge of Docker and Docker Compose. Familiarity with YAML syntax.","title":"Prerequisites"},{"location":"tools/Discourse/examples/#project-structure_1","text":"The Docker Compose file defines multiple services: postgresql for the database, redis for caching, discourse for the main application, and sidekiq for background jobs. The volumes and networks sections manage data persistence and inter-container communication.","title":"Project Structure"},{"location":"tools/Discourse/examples/#steps-for-integration","text":"Clone or create a Docker Compose file similar to the provided example. Customize the environment variables and service settings as per your requirements.","title":"Steps for Integration"},{"location":"tools/Discourse/examples/#running-the-example","text":"Execute docker-compose up in the directory containing your Docker Compose file to start the Discourse environment.","title":"Running the Example"},{"location":"tools/Discourse/examples/#accessing-the-application_1","text":"Once the containers are running, access Discourse through the specified domain configured in the DISCOURSE_HOST environment variable of the discourse service. For specific details, always refer to the latest official documentation of the tools involved.","title":"Accessing the Application"},{"location":"tools/TOOLNAME_template/","text":"Tool Name Introduction Description: Provide an overview of the tool's purpose and its importance in modern tech environments. Objective: Explain how integrating this tool can benefit development and enhance application security or functionality. Features of [Tool Name] List of Features: Outline key features and capabilities of the tool, emphasizing how each contributes to its effectiveness and utility in development projects. Use Case Scenario Description: Detail a typical use case scenario, demonstrating how the tool operates within a workflow or system. (if possible) Visual Aid: Include a diagram or flowchart to illustrate the use case, making it easier to understand the tool's application in a real-world scenario. References Description: list here a set of external references where User Manual of further information to tool is provide. It is suggested to include: a) User manual link and b) Link to PDF of description of tool in GREENGAGE catalogue User manual GREENGAGE catalogue entry","title":"Tool Name"},{"location":"tools/TOOLNAME_template/#tool-name","text":"","title":"Tool Name"},{"location":"tools/TOOLNAME_template/#introduction","text":"Description: Provide an overview of the tool's purpose and its importance in modern tech environments. Objective: Explain how integrating this tool can benefit development and enhance application security or functionality.","title":"Introduction"},{"location":"tools/TOOLNAME_template/#features-of-tool-name","text":"List of Features: Outline key features and capabilities of the tool, emphasizing how each contributes to its effectiveness and utility in development projects.","title":"Features of [Tool Name]"},{"location":"tools/TOOLNAME_template/#use-case-scenario","text":"Description: Detail a typical use case scenario, demonstrating how the tool operates within a workflow or system. (if possible) Visual Aid: Include a diagram or flowchart to illustrate the use case, making it easier to understand the tool's application in a real-world scenario.","title":"Use Case Scenario"},{"location":"tools/TOOLNAME_template/#references","text":"Description: list here a set of external references where User Manual of further information to tool is provide. It is suggested to include: a) User manual link and b) Link to PDF of description of tool in GREENGAGE catalogue User manual GREENGAGE catalogue entry","title":"References"},{"location":"tools/TOOLNAME_template/integration/","text":"[Tool Name] Usage & Integration Guide for Third-Party Services Generic usage instructions for [Tool Name] - Instructions on how to use the tool. - In case there is external user manual make a small summary and refer to it. - Otherwise, provide enough explanations and snapshots to let a user get started with the tool usage. Include links to URLs where the tool might be deployed. Introduction explaining the importance of integrating [Tool Name] to make your tool part of the GREENGAGE engine. This implies that your tool has to use the single sign on (SSO) service provided by GREENGAGE. However, you should also describe not only how your tool is integrated with GREENGAGE's SSO but also how other GREENGAGE tools may integrate with your tools. Feel free to add and rename the integrate steps listed below: Step 1: Requesting Access Credentials Instructions on how to request access credentials. Contact details (e.g., email address) for credential requests. Information to be provided in the request (e.g., tool name, description, contact details). Step 2: Receiving Credentials Process description after the request is made. Information on how credentials will be received. Step 3: Configuring Your Service Steps to incorporate the received credentials into the service. Guidelines on setting up the service for proper communication with the [Tool Name]. Step 4: Testing the Integration Instructions for testing the integration to ensure everything is set up correctly. Contact information for support in case of issues during testing. Support and Contact Additional support contact details. Encouragement to reach out with questions or for further assistance. Important Notes: Security reminders and best practices (e.g., keeping credentials confidential). Instructions on what to do if credentials are compromised.","title":"[Tool Name] Usage & Integration Guide for Third-Party Services"},{"location":"tools/TOOLNAME_template/integration/#tool-name-usage-integration-guide-for-third-party-services","text":"Generic usage instructions for [Tool Name] - Instructions on how to use the tool. - In case there is external user manual make a small summary and refer to it. - Otherwise, provide enough explanations and snapshots to let a user get started with the tool usage. Include links to URLs where the tool might be deployed. Introduction explaining the importance of integrating [Tool Name] to make your tool part of the GREENGAGE engine. This implies that your tool has to use the single sign on (SSO) service provided by GREENGAGE. However, you should also describe not only how your tool is integrated with GREENGAGE's SSO but also how other GREENGAGE tools may integrate with your tools. Feel free to add and rename the integrate steps listed below: Step 1: Requesting Access Credentials Instructions on how to request access credentials. Contact details (e.g., email address) for credential requests. Information to be provided in the request (e.g., tool name, description, contact details). Step 2: Receiving Credentials Process description after the request is made. Information on how credentials will be received. Step 3: Configuring Your Service Steps to incorporate the received credentials into the service. Guidelines on setting up the service for proper communication with the [Tool Name]. Step 4: Testing the Integration Instructions for testing the integration to ensure everything is set up correctly. Contact information for support in case of issues during testing. Support and Contact Additional support contact details. Encouragement to reach out with questions or for further assistance. Important Notes: Security reminders and best practices (e.g., keeping credentials confidential). Instructions on what to do if credentials are compromised.","title":"[Tool Name] Usage &amp; Integration Guide for Third-Party Services"},{"location":"tools/TOOLNAME_template/examples/","text":"[Tool Name] Integrating [Tool Name] in [Environment] Introduction Description: Provide an overview of the tool's purpose and significance in development. Objective: Explain how this tool enhances the application's functionality or security. Prerequisites List the prerequisites needed to use the tool effectively. Project Structure Source: Link to the example project. Directory Layout: Describe the structure of the example project. Step-by-Step Setup and Configuration Outline the steps to set up the tool in the chosen environment. Detailed Configuration Steps Provide detailed instructions for each configuration file and setting. Running the Services Instructions for starting the services on different operating systems. Accessing the Application Guide on how to access the application once it is running. Additional Configuration (Optional) Discuss any optional configuration steps for advanced users. Troubleshooting Provide common troubleshooting steps or issues and their solutions. Support and Additional Resources Support Channels: List available support channels. External Documentation: Link to the official documentation or other helpful resources. [Language/Framework Integration] (e.g., Python, Node.js) Introduction Brief introduction to integrating the tool with the specific language or framework. Prerequisites List any prerequisites specific to this language/framework. Project Structure Describe the structure of the example integration project. Steps for Integration Step-by-step guide for integrating the tool into projects using this language/framework. Running the Example Instructions on how to run the example project. Accessing the Application How to access and test the application once it's running. Replace sections marked with [brackets] with the relevant information for your specific tool and environment.","title":"[Tool Name]"},{"location":"tools/TOOLNAME_template/examples/#tool-name","text":"","title":"[Tool Name]"},{"location":"tools/TOOLNAME_template/examples/#integrating-tool-name-in-environment","text":"","title":"Integrating [Tool Name] in [Environment]"},{"location":"tools/TOOLNAME_template/examples/#introduction","text":"Description: Provide an overview of the tool's purpose and significance in development. Objective: Explain how this tool enhances the application's functionality or security.","title":"Introduction"},{"location":"tools/TOOLNAME_template/examples/#prerequisites","text":"List the prerequisites needed to use the tool effectively.","title":"Prerequisites"},{"location":"tools/TOOLNAME_template/examples/#project-structure","text":"Source: Link to the example project. Directory Layout: Describe the structure of the example project.","title":"Project Structure"},{"location":"tools/TOOLNAME_template/examples/#step-by-step-setup-and-configuration","text":"Outline the steps to set up the tool in the chosen environment.","title":"Step-by-Step Setup and Configuration"},{"location":"tools/TOOLNAME_template/examples/#detailed-configuration-steps","text":"Provide detailed instructions for each configuration file and setting.","title":"Detailed Configuration Steps"},{"location":"tools/TOOLNAME_template/examples/#running-the-services","text":"Instructions for starting the services on different operating systems.","title":"Running the Services"},{"location":"tools/TOOLNAME_template/examples/#accessing-the-application","text":"Guide on how to access the application once it is running.","title":"Accessing the Application"},{"location":"tools/TOOLNAME_template/examples/#additional-configuration-optional","text":"Discuss any optional configuration steps for advanced users.","title":"Additional Configuration (Optional)"},{"location":"tools/TOOLNAME_template/examples/#troubleshooting","text":"Provide common troubleshooting steps or issues and their solutions.","title":"Troubleshooting"},{"location":"tools/TOOLNAME_template/examples/#support-and-additional-resources","text":"Support Channels: List available support channels. External Documentation: Link to the official documentation or other helpful resources.","title":"Support and Additional Resources"},{"location":"tools/TOOLNAME_template/examples/#languageframework-integration-eg-python-nodejs","text":"","title":"[Language/Framework Integration] (e.g., Python, Node.js)"},{"location":"tools/TOOLNAME_template/examples/#introduction_1","text":"Brief introduction to integrating the tool with the specific language or framework.","title":"Introduction"},{"location":"tools/TOOLNAME_template/examples/#prerequisites_1","text":"List any prerequisites specific to this language/framework.","title":"Prerequisites"},{"location":"tools/TOOLNAME_template/examples/#project-structure_1","text":"Describe the structure of the example integration project.","title":"Project Structure"},{"location":"tools/TOOLNAME_template/examples/#steps-for-integration","text":"Step-by-step guide for integrating the tool into projects using this language/framework.","title":"Steps for Integration"},{"location":"tools/TOOLNAME_template/examples/#running-the-example","text":"Instructions on how to run the example project.","title":"Running the Example"},{"location":"tools/TOOLNAME_template/examples/#accessing-the-application_1","text":"How to access and test the application once it's running. Replace sections marked with [brackets] with the relevant information for your specific tool and environment.","title":"Accessing the Application"},{"location":"tools/collaborativeEnvironment/","text":"Collaborative Environment Introduction Description The Collaborative Environment, developed as part of the INTERLINK project, serves as a foundational tool for Citizen Observatories. It leverages cutting-edge frontend technologies to enable effective collaboration and intuitive user interaction. Its modular design, based on functional React components, ensures easy expansion and maintenance, adapting to the evolving needs of Citizen Observatories. Objective Integrating the Collaborative Environment into development projects can significantly enhance team communication, document collaboration, and stakeholder engagement. Its capabilities in process planning and promoting loyalty through incentives and rewards make it an invaluable asset for co-production in diverse fields. Features of Collaborative Environment List of Features: Internationalization Support : Utilizes \"react-i18next\" for advanced translation features, catering to a global user base. Flexible Development : Offers well-documented examples for easy addition of new elements and routes, supporting agile development practices. State Management : Employs Redux for effective global state management and React's Context API for smooth data flow across components. Inclusivity and Diversity : Focus on internationalization promotes access and usability across diverse user groups. Modular Structure : Facilitates easy expansion and maintenance, adapting to changing requirements. Efficient Data Handling : Uses a microservices architecture with RESTful interfaces, ensuring scalability and performance. Security Features : Includes authentication through OpenID Connect (OIDC) and secure data transfer protocols. Gamification Elements : Incorporates a gamification engine to incentivize user participation and engagement. Use Case Scenario Description A typical use case for the Collaborative Environment involves setting up and managing a co-production process within a Citizen Observatory. Users can create organizations, form teams, and develop processes, each with clearly defined phases, objectives, and tasks. The platform allows for resource management, tracking progress, and engaging communities effectively. It also supports integration with external tools, enhancing its utility in diverse scenarios. References User manual of Collaborative Environment Documentation in GREENGAGE catalogue","title":"Collaborative Environment"},{"location":"tools/collaborativeEnvironment/#collaborative-environment","text":"","title":"Collaborative Environment"},{"location":"tools/collaborativeEnvironment/#introduction","text":"","title":"Introduction"},{"location":"tools/collaborativeEnvironment/#description","text":"The Collaborative Environment, developed as part of the INTERLINK project, serves as a foundational tool for Citizen Observatories. It leverages cutting-edge frontend technologies to enable effective collaboration and intuitive user interaction. Its modular design, based on functional React components, ensures easy expansion and maintenance, adapting to the evolving needs of Citizen Observatories.","title":"Description"},{"location":"tools/collaborativeEnvironment/#objective","text":"Integrating the Collaborative Environment into development projects can significantly enhance team communication, document collaboration, and stakeholder engagement. Its capabilities in process planning and promoting loyalty through incentives and rewards make it an invaluable asset for co-production in diverse fields.","title":"Objective"},{"location":"tools/collaborativeEnvironment/#features-of-collaborative-environment","text":"","title":"Features of Collaborative Environment"},{"location":"tools/collaborativeEnvironment/#list-of-features","text":"Internationalization Support : Utilizes \"react-i18next\" for advanced translation features, catering to a global user base. Flexible Development : Offers well-documented examples for easy addition of new elements and routes, supporting agile development practices. State Management : Employs Redux for effective global state management and React's Context API for smooth data flow across components. Inclusivity and Diversity : Focus on internationalization promotes access and usability across diverse user groups. Modular Structure : Facilitates easy expansion and maintenance, adapting to changing requirements. Efficient Data Handling : Uses a microservices architecture with RESTful interfaces, ensuring scalability and performance. Security Features : Includes authentication through OpenID Connect (OIDC) and secure data transfer protocols. Gamification Elements : Incorporates a gamification engine to incentivize user participation and engagement.","title":"List of Features:"},{"location":"tools/collaborativeEnvironment/#use-case-scenario","text":"","title":"Use Case Scenario"},{"location":"tools/collaborativeEnvironment/#description_1","text":"A typical use case for the Collaborative Environment involves setting up and managing a co-production process within a Citizen Observatory. Users can create organizations, form teams, and develop processes, each with clearly defined phases, objectives, and tasks. The platform allows for resource management, tracking progress, and engaging communities effectively. It also supports integration with external tools, enhancing its utility in diverse scenarios.","title":"Description"},{"location":"tools/collaborativeEnvironment/#references","text":"User manual of Collaborative Environment Documentation in GREENGAGE catalogue","title":"References"},{"location":"tools/collaborativeEnvironment/usage/","text":"Collaborative Environment Usage Guide for Third-Party Services This guide explains the process for integrating third-party services into the Collaborative Environment, a platform centralizing organizational management and enhancing collaborative workflows. Initial Login and registration Create an account either by registering with an email or using a Google account at Collaborative Environment > Login. Organization and Team Setup After logging in, navigate to Organizations tab > Create new organization. Fill in the necessary details to create your organization. To create a team, ensure you have an existing organization. In the Organizations tab, select your organization and click on Create new team. Define your team, stakeholders, and invite members. Process Creation and Configuration From the dashboard, click on Go to process list then on Create new process and fill in the required fields. Configure your process by selecting it from the process list and accessing the Overview section. This includes setting up various sections like Front Page, Overview, Resources, Guide, Leaderboard, Workplan, Team, and Settings. Define specific elements like Type, Schema, Organizations, Resources, Permissions, Rewards, and Finish in the roadmap for the process. Development and Completion of the Co-Production Process Begin the co-production process by selecting tasks from the Guide section. Manage phases, objectives, and tasks, including descriptions, statuses, planning times, and permissions. Once the process is complete, change the \"State of the project\" to \"Finished\" in Settings. Support and Contact For additional support or assistance with the integration process, please refer to the Collaborative Environment\u2019s online resources, or contact via https://github.com/Greengage-project/interlink-project/issues . (Technical Issues) Important Notes: Adhere to security best practices throughout the integration process. Regularly update and maintain your service for compatibility with the Collaborative Environment. For further assistance or troubleshooting, refer to the Collaborative Environment\u2019s resources or community forums. By following these steps, third-party services can be effectively integrated into the Collaborative Environment, leveraging its capabilities to improve organizational efficiency and collaboration. References Further details about the usage of this tool are available in the following references: - Collaborative Environment's Usage Manual","title":"Collaborative Environment Usage Guide for Third-Party Services"},{"location":"tools/collaborativeEnvironment/usage/#collaborative-environment-usage-guide-for-third-party-services","text":"This guide explains the process for integrating third-party services into the Collaborative Environment, a platform centralizing organizational management and enhancing collaborative workflows. Initial Login and registration Create an account either by registering with an email or using a Google account at Collaborative Environment > Login. Organization and Team Setup After logging in, navigate to Organizations tab > Create new organization. Fill in the necessary details to create your organization. To create a team, ensure you have an existing organization. In the Organizations tab, select your organization and click on Create new team. Define your team, stakeholders, and invite members. Process Creation and Configuration From the dashboard, click on Go to process list then on Create new process and fill in the required fields. Configure your process by selecting it from the process list and accessing the Overview section. This includes setting up various sections like Front Page, Overview, Resources, Guide, Leaderboard, Workplan, Team, and Settings. Define specific elements like Type, Schema, Organizations, Resources, Permissions, Rewards, and Finish in the roadmap for the process. Development and Completion of the Co-Production Process Begin the co-production process by selecting tasks from the Guide section. Manage phases, objectives, and tasks, including descriptions, statuses, planning times, and permissions. Once the process is complete, change the \"State of the project\" to \"Finished\" in Settings. Support and Contact For additional support or assistance with the integration process, please refer to the Collaborative Environment\u2019s online resources, or contact via https://github.com/Greengage-project/interlink-project/issues . (Technical Issues) Important Notes: Adhere to security best practices throughout the integration process. Regularly update and maintain your service for compatibility with the Collaborative Environment. For further assistance or troubleshooting, refer to the Collaborative Environment\u2019s resources or community forums. By following these steps, third-party services can be effectively integrated into the Collaborative Environment, leveraging its capabilities to improve organizational efficiency and collaboration.","title":"Collaborative Environment Usage Guide for Third-Party Services"},{"location":"tools/collaborativeEnvironment/usage/#references","text":"Further details about the usage of this tool are available in the following references: - Collaborative Environment's Usage Manual","title":"References"},{"location":"tools/collaborativeEnvironment/examples/","text":"Collaborative Environment Installation Guide Deploying Collaborative Environment This guide provides detailed instructions for setting up the Collaborative Environment using Docker and Docker Compose. Prerequisites Docker and Docker Compose installed. Node.js and npm installed for Node.js integration. Python and pip installed for Python integration. Basic understanding of the Collaborative Environment, Docker, and the chosen programming language (Node.js or Python). Project Structure The project structure includes various components like Docker configurations, backend services, interlinkers, and frontend applications. The main components are located in different repositories, which can be cloned for development purposes. Collaborative Environment/ \u251c\u2500\u2500 docker/ # Docker configurations for various services \u251c\u2500\u2500 docs/ # Documentation files \u251c\u2500\u2500 envs/ # Environment configurations \u251c\u2500\u2500 Makefile # Makefile for automating setup and deployment ... Step 1: Cloning Repositories and Setting Up the Environment Use the Makefile to clone all necessary components: make setup This will clone repositories such as: Frontend application Backend services (auth, catalogue, coproduction, logging) Interlinkers (survey, googledrive, ceditor) Gamification service Step 2: Building and Starting Services Build the containers for all services: make build Start the services, set up the environment, and seed the database: make up Step 3: Accessing the Application Once all services are up and running, access the Collaborative Environment's frontend through the configured local or development URL. Additional Commands Stopping Services : To stop all containers and remove volumes, use make down . Pulling Latest Changes : To update all components, use make pull . Restarting Services : To restart containers, apply migrations and seed data, use make restart . Environment Variables Setup : To configure environment variables, use make envVariables . Troubleshooting Common issues can arise during the setup, such as Docker network conflicts or missing dependencies. Refer to the README.md files in each repository for specific troubleshooting steps. Support and Additional Resources For further assistance, refer to the detailed documentation in the docs/ directory or visit the official Collaborative Environment Documentation . Additional support can be obtained through community forums or by contacting the Collaborative Environment support team.","title":"Collaborative Environment Installation Guide"},{"location":"tools/collaborativeEnvironment/examples/#collaborative-environment-installation-guide","text":"","title":"Collaborative Environment Installation Guide"},{"location":"tools/collaborativeEnvironment/examples/#deploying-collaborative-environment","text":"This guide provides detailed instructions for setting up the Collaborative Environment using Docker and Docker Compose.","title":"Deploying Collaborative Environment"},{"location":"tools/collaborativeEnvironment/examples/#prerequisites","text":"Docker and Docker Compose installed. Node.js and npm installed for Node.js integration. Python and pip installed for Python integration. Basic understanding of the Collaborative Environment, Docker, and the chosen programming language (Node.js or Python).","title":"Prerequisites"},{"location":"tools/collaborativeEnvironment/examples/#project-structure","text":"The project structure includes various components like Docker configurations, backend services, interlinkers, and frontend applications. The main components are located in different repositories, which can be cloned for development purposes. Collaborative Environment/ \u251c\u2500\u2500 docker/ # Docker configurations for various services \u251c\u2500\u2500 docs/ # Documentation files \u251c\u2500\u2500 envs/ # Environment configurations \u251c\u2500\u2500 Makefile # Makefile for automating setup and deployment ...","title":"Project Structure"},{"location":"tools/collaborativeEnvironment/examples/#step-1-cloning-repositories-and-setting-up-the-environment","text":"Use the Makefile to clone all necessary components: make setup This will clone repositories such as: Frontend application Backend services (auth, catalogue, coproduction, logging) Interlinkers (survey, googledrive, ceditor) Gamification service","title":"Step 1: Cloning Repositories and Setting Up the Environment"},{"location":"tools/collaborativeEnvironment/examples/#step-2-building-and-starting-services","text":"Build the containers for all services: make build Start the services, set up the environment, and seed the database: make up","title":"Step 2: Building and Starting Services"},{"location":"tools/collaborativeEnvironment/examples/#step-3-accessing-the-application","text":"Once all services are up and running, access the Collaborative Environment's frontend through the configured local or development URL.","title":"Step 3: Accessing the Application"},{"location":"tools/collaborativeEnvironment/examples/#additional-commands","text":"Stopping Services : To stop all containers and remove volumes, use make down . Pulling Latest Changes : To update all components, use make pull . Restarting Services : To restart containers, apply migrations and seed data, use make restart . Environment Variables Setup : To configure environment variables, use make envVariables .","title":"Additional Commands"},{"location":"tools/collaborativeEnvironment/examples/#troubleshooting","text":"Common issues can arise during the setup, such as Docker network conflicts or missing dependencies. Refer to the README.md files in each repository for specific troubleshooting steps.","title":"Troubleshooting"},{"location":"tools/collaborativeEnvironment/examples/#support-and-additional-resources","text":"For further assistance, refer to the detailed documentation in the docs/ directory or visit the official Collaborative Environment Documentation . Additional support can be obtained through community forums or by contacting the Collaborative Environment support team.","title":"Support and Additional Resources"},{"location":"tools/datahub/","text":"DataHub DataHub Description Introduction Description DataHub is an open-source metadata platform designed to democratise data discovery. It acts as a centralised system for companies and teams to track, manage, and find data within their organisation. Its primary aim is to streamline the process of accessing and understanding data assets, making it easier for teams to derive insights and make informed decisions. Objective The primary objective of DataHub is to facilitate better data discovery and management. By offering a unified view of an organisation's data assets, it aims to enhance collaboration, data governance, and compliance, while reducing the time and effort spent in locating and understanding data resources. Features of Datahub Metadata Management: DataHub catalogs metadata from various data sources, offering insights into data quality, usage, and lineage. Search and Discovery: It provides powerful search capabilities, allowing users to quickly find datasets and understand their context. Data Lineage: Visual representation of data lineage helps in understanding how data is transformed and where it flows within the organisation. Data Observability: Monitors data health, alerting users to issues related to data quality and freshness. Access Control: Supports granular access control to manage who can view or edit different data assets. Extensibility: Offers APIs and an extensible architecture, allowing for customisation and integration with other tools and systems. Use Case Scenario DataHub is employed to bridge the gap between citisens, scientists, and open data portals. Pilots have a wealth of open data available \u2013 traffic patterns, pollution levels, public space usage, and more. With DataHub, the city creates a centralised, user-friendly platform where all this data is cataloged and made easily searchable. Citisens and local scientists can access this data to understand various aspects of the city's dynamics. For instance, a group of citisens concerned about air quality uses DataHub to find and analyse pollution data. Furthermore, DataHub can be used to track the lineage of data, allowing users to understand how data is transformed and where it flows within the organisation. This helps in ensuring data quality and compliance with regulations such as GDPR. References Documentation GREENGAGE catalogue entry","title":"DataHub"},{"location":"tools/datahub/#datahub","text":"","title":"DataHub"},{"location":"tools/datahub/#datahub-description","text":"","title":"DataHub Description"},{"location":"tools/datahub/#introduction","text":"","title":"Introduction"},{"location":"tools/datahub/#description","text":"DataHub is an open-source metadata platform designed to democratise data discovery. It acts as a centralised system for companies and teams to track, manage, and find data within their organisation. Its primary aim is to streamline the process of accessing and understanding data assets, making it easier for teams to derive insights and make informed decisions.","title":"Description"},{"location":"tools/datahub/#objective","text":"The primary objective of DataHub is to facilitate better data discovery and management. By offering a unified view of an organisation's data assets, it aims to enhance collaboration, data governance, and compliance, while reducing the time and effort spent in locating and understanding data resources.","title":"Objective"},{"location":"tools/datahub/#features-of-datahub","text":"Metadata Management: DataHub catalogs metadata from various data sources, offering insights into data quality, usage, and lineage. Search and Discovery: It provides powerful search capabilities, allowing users to quickly find datasets and understand their context. Data Lineage: Visual representation of data lineage helps in understanding how data is transformed and where it flows within the organisation. Data Observability: Monitors data health, alerting users to issues related to data quality and freshness. Access Control: Supports granular access control to manage who can view or edit different data assets. Extensibility: Offers APIs and an extensible architecture, allowing for customisation and integration with other tools and systems.","title":"Features of Datahub"},{"location":"tools/datahub/#use-case-scenario","text":"DataHub is employed to bridge the gap between citisens, scientists, and open data portals. Pilots have a wealth of open data available \u2013 traffic patterns, pollution levels, public space usage, and more. With DataHub, the city creates a centralised, user-friendly platform where all this data is cataloged and made easily searchable. Citisens and local scientists can access this data to understand various aspects of the city's dynamics. For instance, a group of citisens concerned about air quality uses DataHub to find and analyse pollution data. Furthermore, DataHub can be used to track the lineage of data, allowing users to understand how data is transformed and where it flows within the organisation. This helps in ensuring data quality and compliance with regulations such as GDPR.","title":"Use Case Scenario"},{"location":"tools/datahub/#references","text":"Documentation GREENGAGE catalogue entry","title":"References"},{"location":"tools/datahub/usage/","text":"DataHub Usage Guide To provide a centralised location for all data assets, we are deploying DataHub in a centralised manner. This means that all data assets will be catalogued in a single instance of DataHub. This usage guide provide instructions on how to use DataHub. Step 1: Requesting Credentials Access to DataHub using your Google account or the account created for Greengage project and registered in Keycloak. Send an email to ruben.sanchez@deusto.es with the subject line: \"[GREENGAGE] Request for rights in DataHub\" including the following information: Tool name. Contact name:(for technical propose) Contact email:(for technical propose) Once you have the credentials and the right access rights you can start using DataHub. Data Discovery and Search You can search for any asset located in DataHub using the search bar located on the main page or at the top of any other page within DataHub. The search bar, as shown in the figure below, enables users to quickly find relevant datasets, metrics, and dashboards. Metadata Management DataHub offers comprehensive metadata management capabilities. It aggregates and manages metadata from a variety of data sources, maintaining details about datasets, schemas, data lineage, and ownership. This feature is illustrated in the Metadata Management interface, providing a clear overview of all metadata assets. Data Lineage and Governance DataHub visualises data lineage, showing how data flows through various systems. This feature is crucial for understanding the transformation and lifecycle of data, aiding in governance and compliance. The Data Lineage interface provides a graphical representation of these data flows. Collaboration and Documentation The platform enhances collaboration among data teams by allowing for the documentation and sharing of insights about datasets. The Collaboration and Documentation interface facilitates this exchange, fostering a data-informed culture.","title":"DataHub Usage Guide"},{"location":"tools/datahub/usage/#datahub-usage-guide","text":"To provide a centralised location for all data assets, we are deploying DataHub in a centralised manner. This means that all data assets will be catalogued in a single instance of DataHub. This usage guide provide instructions on how to use DataHub. Step 1: Requesting Credentials Access to DataHub using your Google account or the account created for Greengage project and registered in Keycloak. Send an email to ruben.sanchez@deusto.es with the subject line: \"[GREENGAGE] Request for rights in DataHub\" including the following information: Tool name. Contact name:(for technical propose) Contact email:(for technical propose) Once you have the credentials and the right access rights you can start using DataHub. Data Discovery and Search You can search for any asset located in DataHub using the search bar located on the main page or at the top of any other page within DataHub. The search bar, as shown in the figure below, enables users to quickly find relevant datasets, metrics, and dashboards. Metadata Management DataHub offers comprehensive metadata management capabilities. It aggregates and manages metadata from a variety of data sources, maintaining details about datasets, schemas, data lineage, and ownership. This feature is illustrated in the Metadata Management interface, providing a clear overview of all metadata assets. Data Lineage and Governance DataHub visualises data lineage, showing how data flows through various systems. This feature is crucial for understanding the transformation and lifecycle of data, aiding in governance and compliance. The Data Lineage interface provides a graphical representation of these data flows. Collaboration and Documentation The platform enhances collaboration among data teams by allowing for the documentation and sharing of insights about datasets. The Collaboration and Documentation interface facilitates this exchange, fostering a data-informed culture.","title":"DataHub Usage Guide"},{"location":"tools/datahub/examples/","text":"DataHub example of Integration Creating an Ingestion Source Introduction In this example we will show how to create an ingestion source that pulls data from the desired platform to DataHub. Prerequisites Access to DataHub Access to the platform to be integrated with DataHub Check if the platform to be integrated with DataHub is already supported by Datahub. List of ingestion sources Creating the recipe 1) Access to DataHub 2) Go to the Ingestion view and click in create new source. 3) Select the platform to be integrated with DataHub. 4) Fill the form with the information of the platform to be integrated with DataHub. Some of the platforms have forms to be fulfilled with the information of the platform to be integrated with DataHub. In other cases, the information is provided in a yaml style as shown in the following figure: 5) Schedule the ingestion of data as shown in the following figure: 6) Click in Save & Run source and the ingestion will start. Adding metadata to the datasets 1) Access to DataHub 2) Access to the desired dataset 3) You can edit the metadata of the dataset in the column shown at the right side of the following figure and descriptions , tags and glossary terms can be added to the dataset fields: 4) In the documenation tab, the documentation of the dataset can be added directly in formatted text or via a link to the documentation of it. 5) The lineage tab allows to edit the lineage of the dataset. 6) The queries tab allows to add the most common queries conducted to the dataset. However, this queries cannot be executed from DataHub. Dataset Usage & Query History Altough is not possible to execute queries from DataHub, it is possible to add the usage and query history of the dataset. Some ingestion sources have the option to add the usage and query history. To see if our desired source has this option enabled we should check the documentation for it in the DataHub website . This figure shows the option that should be present to have the Dataset Usage enabled: If the option is enabled, the usage and query history of the dataset will be shown in the dataset view as shown in the following figure:","title":"DataHub example of Integration"},{"location":"tools/datahub/examples/#datahub-example-of-integration","text":"","title":"DataHub example of Integration"},{"location":"tools/datahub/examples/#creating-an-ingestion-source","text":"","title":"Creating an Ingestion Source"},{"location":"tools/datahub/examples/#introduction","text":"In this example we will show how to create an ingestion source that pulls data from the desired platform to DataHub.","title":"Introduction"},{"location":"tools/datahub/examples/#prerequisites","text":"Access to DataHub Access to the platform to be integrated with DataHub Check if the platform to be integrated with DataHub is already supported by Datahub. List of ingestion sources","title":"Prerequisites"},{"location":"tools/datahub/examples/#creating-the-recipe","text":"1) Access to DataHub 2) Go to the Ingestion view and click in create new source. 3) Select the platform to be integrated with DataHub. 4) Fill the form with the information of the platform to be integrated with DataHub. Some of the platforms have forms to be fulfilled with the information of the platform to be integrated with DataHub. In other cases, the information is provided in a yaml style as shown in the following figure: 5) Schedule the ingestion of data as shown in the following figure: 6) Click in Save & Run source and the ingestion will start.","title":"Creating the recipe"},{"location":"tools/datahub/examples/#adding-metadata-to-the-datasets","text":"1) Access to DataHub 2) Access to the desired dataset 3) You can edit the metadata of the dataset in the column shown at the right side of the following figure and descriptions , tags and glossary terms can be added to the dataset fields: 4) In the documenation tab, the documentation of the dataset can be added directly in formatted text or via a link to the documentation of it. 5) The lineage tab allows to edit the lineage of the dataset. 6) The queries tab allows to add the most common queries conducted to the dataset. However, this queries cannot be executed from DataHub.","title":"Adding metadata to the datasets"},{"location":"tools/datahub/examples/#dataset-usage-query-history","text":"Altough is not possible to execute queries from DataHub, it is possible to add the usage and query history of the dataset. Some ingestion sources have the option to add the usage and query history. To see if our desired source has this option enabled we should check the documentation for it in the DataHub website . This figure shows the option that should be present to have the Dataset Usage enabled: If the option is enabled, the usage and query history of the dataset will be shown in the dataset view as shown in the following figure:","title":"Dataset Usage &amp; Query History"},{"location":"tools/downscaling/","text":"Downscaling Introduction Downscaling is a numerical modeling tool that allows quantifying the level of pollution in cities. This tool is capable of integrating different data sources such as geospatial information, traffic data, COPERNICUS data, and regional atmospheric (WRF) and pollution (CHIMERE) models to calculate pollution levels in cities with high street-level resolution using a street canyon model MUNICH. This tool assists administrations in decision-making and plays a crucial role in the era of digital transformation by creating digital twins of cities. Features of Downscaling Prerequisites To compile the tool, some prerequisites relating to the computing resources are necessary. Linux system operating system CPU with 8-16 cores 32 GB RAM To download the MUNICH model, please, see the documentation here , where you can find all the dependencies and libraries necessary. Multisources data integration. The tool is capable of integrating traffic data from various cities, pollution data from COPERNICUS or satellite sources (SENTINEL 5 and 7), terrain elevation, and building geometry, among other information. Figure 1: Different kinds of sources that the tool are available to use These input data are typically provided in netCDF file format for the model to recognize and integrate. Advanced numerical model MUNICH. Based on all received data sources, Downscaling tool utilizes MUNICH (Model of Urban Network of Intersecting Canyons and Highways), an advanced numerical model that calculates the concentration of pollutants such as CO, NO, CO2, O3, NO2, particulate matter and black carbon. Figure 2: General diagram of downscaling superresolution architecture. Figure 3: Example of MUNICH pollutant calculation. Geospatial analysis. The tool enables spatial analysis of pollution, facilitating the identification of critical areas where high concentrations of pollutants are found to develop mitigation measures. Real-time data. Capable of generating and displaying real-time results, including simulations for the future and simulations over an extended period. Outputs and Interactive Visualization. The model provides results in netCDF files, which are subsequently processed to offer an interactive visualization showing the concentration of various pollutants in the city. Concentration values are displayed for each street. Figure 4: Workflow of the tool. Use case This is a case study in the city of Lindau (Germany) that illustrates how the tool operates, the results it produces, and the visualization it provides. In this visualizer, you can see the concentration level of each street, as well as apply different types of restrictions in the city to see the effect they have. Figure 5: Example of real use case in the city of Lindau. Online visualizer can be found in this link Another example in the city of Cartagena is available in this link References MUNICH documentation can be found here","title":"Downscaling"},{"location":"tools/downscaling/#downscaling","text":"","title":"Downscaling"},{"location":"tools/downscaling/#introduction","text":"Downscaling is a numerical modeling tool that allows quantifying the level of pollution in cities. This tool is capable of integrating different data sources such as geospatial information, traffic data, COPERNICUS data, and regional atmospheric (WRF) and pollution (CHIMERE) models to calculate pollution levels in cities with high street-level resolution using a street canyon model MUNICH. This tool assists administrations in decision-making and plays a crucial role in the era of digital transformation by creating digital twins of cities.","title":"Introduction"},{"location":"tools/downscaling/#features-of-downscaling","text":"","title":"Features of Downscaling"},{"location":"tools/downscaling/#prerequisites","text":"To compile the tool, some prerequisites relating to the computing resources are necessary. Linux system operating system CPU with 8-16 cores 32 GB RAM To download the MUNICH model, please, see the documentation here , where you can find all the dependencies and libraries necessary. Multisources data integration. The tool is capable of integrating traffic data from various cities, pollution data from COPERNICUS or satellite sources (SENTINEL 5 and 7), terrain elevation, and building geometry, among other information. Figure 1: Different kinds of sources that the tool are available to use These input data are typically provided in netCDF file format for the model to recognize and integrate. Advanced numerical model MUNICH. Based on all received data sources, Downscaling tool utilizes MUNICH (Model of Urban Network of Intersecting Canyons and Highways), an advanced numerical model that calculates the concentration of pollutants such as CO, NO, CO2, O3, NO2, particulate matter and black carbon. Figure 2: General diagram of downscaling superresolution architecture. Figure 3: Example of MUNICH pollutant calculation. Geospatial analysis. The tool enables spatial analysis of pollution, facilitating the identification of critical areas where high concentrations of pollutants are found to develop mitigation measures. Real-time data. Capable of generating and displaying real-time results, including simulations for the future and simulations over an extended period. Outputs and Interactive Visualization. The model provides results in netCDF files, which are subsequently processed to offer an interactive visualization showing the concentration of various pollutants in the city. Concentration values are displayed for each street. Figure 4: Workflow of the tool.","title":"Prerequisites"},{"location":"tools/downscaling/#use-case","text":"This is a case study in the city of Lindau (Germany) that illustrates how the tool operates, the results it produces, and the visualization it provides. In this visualizer, you can see the concentration level of each street, as well as apply different types of restrictions in the city to see the effect they have. Figure 5: Example of real use case in the city of Lindau. Online visualizer can be found in this link Another example in the city of Cartagena is available in this link","title":"Use case"},{"location":"tools/downscaling/#references","text":"MUNICH documentation can be found here","title":"References"},{"location":"tools/druid/","text":"Apache Druid Introduction Apache Druid is a high-performance, distributed data store designed for real-time analytics on large-scale datasets. It excels in scenarios where fast queries and ingest are crucial, such as in business intelligence applications, operational analytics, and real-time monitoring. Druid is known for its ability to handle high concurrency, its columnar storage format, and its ability to scale horizontally. It supports streaming and batch data ingestion, and its architecture allows for efficient data summarisation, fast aggregation queries, and low-latency data access. Druid is often used in conjunction with big data technologies like Hadoop and Kafka, making it a popular choice for organisations looking to implement real-time analytics solutions. Apache Druid is intended to be used by data engineers or data scientists who are familiar with data warehousing concepts and are looking for a high-performance, scalable solution for real-time analytics. Thus, this tool will be handled by the Greengage team and the data stored in it will be employed by non-tech users through Apache Superset or other tools. Features of Apache Druid Real-time Data Ingestion: Druid can ingest data in real-time, allowing for immediate data querying and analysis. This feature is crucial for applications that require up-to-the-minute data, like monitoring and event-driven systems. Horizontal Scalability: It can scale horizontally across commodity servers, enhancing performance and storage capacity linearly with the addition of more resources. This makes it well-suited for handling large and growing datasets. Columnar Storage Format: Druid stores data in a columnar format, which is optimised for fast querying. This format allows for efficient data compression and rapid aggregation, making it ideal for analytics workloads. Highly Concurrent: The system can handle a high number of concurrent queries, making it suitable for environments with multiple users or applications accessing the data simultaneously. Fast Aggregations and Filters: Druid is designed for quick data aggregations and filtering, enabling rapid, on-the-fly data analysis. This is particularly beneficial for business intelligence and reporting use cases. Approximate Queries: It supports approximate queries, such as top-N and count-distinct, which can be executed faster than exact computations at the cost of some accuracy. This is useful for large-scale data exploration. Data Summarization and Roll-up: Druid can automatically summarise and roll-up data at ingestion time, reducing the data volume and speeding up query times. This is particularly useful for time-series data. Fault Tolerance: The system is designed to be fault-tolerant, with data replication and recovery mechanisms in place. This ensures high availability and data integrity. Flexible Data Model: Druid supports a flexible schema-on-read model, allowing ingestion of semi-structured data and different data types. Use Case Scenario Context: In an urban citizen observatory, members of the community actively participate in monitoring and analysing environmental conditions. One of the critical concerns in many cities is air quality, which directly impacts public health and quality of life. Objective: To monitor, analyse, and visualise air quality data across different parts of the city to identify pollution hotspots, understand temporal trends, and engage the public in environmental awareness and decision-making processes. Implementation Using Apache Druid: 1) Data Collection: - Deploy IoT sensors across the city to measure air quality indicators like PM2.5, PM10, NO2, CO2, and Ozone. 2) Data Ingestion and Storage with Apache Druid: - Stream the sensor data into Apache Druid for real-time ingestion and storage. - Configure Druid to handle the high volume of time-series data from the sensors, optimising for fast access and query performance. 3) Real-time Data Analysis and Aggregation: - Perform real-time data analysis using Druid's fast aggregation and filtering capabilities. - Aggregate data at various time intervals to observe trends and detect anomalies in air quality. 4) Integration with Apache Superset for Visualisation: - Connect Apache Superset to Apache Druid to leverage its advanced visualisation capabilities. - Ensure real-time data is accessible in Superset for immediate analysis and dashboarding. 5) Dashboard Creation and Data-Driven Decision Making (Out of Scope of Apache Druid): - Develop an interactive dashboard in Superset, powered by the real-time data processed in Druid. - Use insights from the Superset dashboard, underpinned by Apache Druid's real-time analytics, to guide community actions and policy recommendations. References Apache Druid documentation","title":"Apache Druid"},{"location":"tools/druid/#apache-druid","text":"","title":"Apache Druid"},{"location":"tools/druid/#introduction","text":"Apache Druid is a high-performance, distributed data store designed for real-time analytics on large-scale datasets. It excels in scenarios where fast queries and ingest are crucial, such as in business intelligence applications, operational analytics, and real-time monitoring. Druid is known for its ability to handle high concurrency, its columnar storage format, and its ability to scale horizontally. It supports streaming and batch data ingestion, and its architecture allows for efficient data summarisation, fast aggregation queries, and low-latency data access. Druid is often used in conjunction with big data technologies like Hadoop and Kafka, making it a popular choice for organisations looking to implement real-time analytics solutions. Apache Druid is intended to be used by data engineers or data scientists who are familiar with data warehousing concepts and are looking for a high-performance, scalable solution for real-time analytics. Thus, this tool will be handled by the Greengage team and the data stored in it will be employed by non-tech users through Apache Superset or other tools.","title":"Introduction"},{"location":"tools/druid/#features-of-apache-druid","text":"Real-time Data Ingestion: Druid can ingest data in real-time, allowing for immediate data querying and analysis. This feature is crucial for applications that require up-to-the-minute data, like monitoring and event-driven systems. Horizontal Scalability: It can scale horizontally across commodity servers, enhancing performance and storage capacity linearly with the addition of more resources. This makes it well-suited for handling large and growing datasets. Columnar Storage Format: Druid stores data in a columnar format, which is optimised for fast querying. This format allows for efficient data compression and rapid aggregation, making it ideal for analytics workloads. Highly Concurrent: The system can handle a high number of concurrent queries, making it suitable for environments with multiple users or applications accessing the data simultaneously. Fast Aggregations and Filters: Druid is designed for quick data aggregations and filtering, enabling rapid, on-the-fly data analysis. This is particularly beneficial for business intelligence and reporting use cases. Approximate Queries: It supports approximate queries, such as top-N and count-distinct, which can be executed faster than exact computations at the cost of some accuracy. This is useful for large-scale data exploration. Data Summarization and Roll-up: Druid can automatically summarise and roll-up data at ingestion time, reducing the data volume and speeding up query times. This is particularly useful for time-series data. Fault Tolerance: The system is designed to be fault-tolerant, with data replication and recovery mechanisms in place. This ensures high availability and data integrity. Flexible Data Model: Druid supports a flexible schema-on-read model, allowing ingestion of semi-structured data and different data types.","title":"Features of Apache Druid"},{"location":"tools/druid/#use-case-scenario","text":"","title":"Use Case Scenario"},{"location":"tools/druid/#context","text":"In an urban citizen observatory, members of the community actively participate in monitoring and analysing environmental conditions. One of the critical concerns in many cities is air quality, which directly impacts public health and quality of life.","title":"Context:"},{"location":"tools/druid/#objective","text":"To monitor, analyse, and visualise air quality data across different parts of the city to identify pollution hotspots, understand temporal trends, and engage the public in environmental awareness and decision-making processes.","title":"Objective:"},{"location":"tools/druid/#implementation-using-apache-druid","text":"1) Data Collection: - Deploy IoT sensors across the city to measure air quality indicators like PM2.5, PM10, NO2, CO2, and Ozone. 2) Data Ingestion and Storage with Apache Druid: - Stream the sensor data into Apache Druid for real-time ingestion and storage. - Configure Druid to handle the high volume of time-series data from the sensors, optimising for fast access and query performance. 3) Real-time Data Analysis and Aggregation: - Perform real-time data analysis using Druid's fast aggregation and filtering capabilities. - Aggregate data at various time intervals to observe trends and detect anomalies in air quality. 4) Integration with Apache Superset for Visualisation: - Connect Apache Superset to Apache Druid to leverage its advanced visualisation capabilities. - Ensure real-time data is accessible in Superset for immediate analysis and dashboarding. 5) Dashboard Creation and Data-Driven Decision Making (Out of Scope of Apache Druid): - Develop an interactive dashboard in Superset, powered by the real-time data processed in Druid. - Use insights from the Superset dashboard, underpinned by Apache Druid's real-time analytics, to guide community actions and policy recommendations.","title":"Implementation Using Apache Druid:"},{"location":"tools/druid/#references","text":"Apache Druid documentation","title":"References"},{"location":"tools/druid/usage/","text":"Apache Druid Usage Guide Apache Druid offers a variety of ways to ingest data. The most common way is to use the native batch ingestion system. This system is designed to handle large amounts of data and is the recommended way to ingest data into Druid. This usage guide provide instructions on how to use Apache Druid. Step 1: Log In - Access to the Druid deploy of Greengage. - First of all, you should login using your Greengage login in the keycloak authentication webpage that will appear. - You will reach to the main view of the tool as shown in the figure below. Step 2: Loading data Click on the Load data button at the top menu. Here we have several options, however, we will use the Batch - SQL option. This will bring us to the view shown in the figure below. Apache Druid offers us several options to load data, from several online providers to local files. In this example we will employ a HTTPS URL to load the data from an Open API. However, you can use any option since the process is the same. Once we introduced the URL, we should click on the NEXT button at the bottom-right part of the view. This will bring us to the view shown in the figure below. In this step we should configure the data ingestion settings (usually Apache Druid detects them automatically). However, there is an important setting that we should configure: the timestamp . This setting is the one that will be used to order the data in the database. In this case, we will use the month field because the rows do not have a timestamp field. Once we have configured the settings, we should click on the NEXT button at the bottom-right part of the view. Subsequently, Apache Druid will identify the type of each column automatically. However, we can change the type of each column or add new ones if we want. Once we have configured the settings, we should click on the Start loading data button at the bottom-right part of the view. The loading of data is done in separated tasks. Once all the tasks are finished, the data will be available in the database. You can check the status of the task in the Tasks button at the top menu. Step 3: Check datasources Click on the Datasources button at the top menu. In this view, you can visualise the different datasources that you have in the database. When you click on a datasource, you will be able to see the different columns that it has. Furthermore, you can see the different values that each column has. In this view you can also conduct different actions to the datasource, such as, edit the several performance settings or delete it. Step 4: Querying datasources Click on the QUERY button at the top menu. This view will allow you to query the different datasources that you have in the database. At the left part of the view, all the available datasources are shown. In the middle part of the view, you can write the query that you want to execute. After that, you should click on the Run button at the middle-left part of the view. The results of the query will be shown at the bottom part of the view. Extra: Explore datasources Click on the THREE DOTS at the rightmost part of the top menu. Then, click on the Explore button. This view will allow you to explore the different datasources that you have in the database. At the left part you should select the datasource that you want to explore and in the right part you may choose the chart type and its configuration. After that, you should select the desired column to explore. Finally, you can add several filters at the top part of the view.","title":"Apache Druid Usage Guide"},{"location":"tools/druid/usage/#apache-druid-usage-guide","text":"Apache Druid offers a variety of ways to ingest data. The most common way is to use the native batch ingestion system. This system is designed to handle large amounts of data and is the recommended way to ingest data into Druid. This usage guide provide instructions on how to use Apache Druid. Step 1: Log In - Access to the Druid deploy of Greengage. - First of all, you should login using your Greengage login in the keycloak authentication webpage that will appear. - You will reach to the main view of the tool as shown in the figure below. Step 2: Loading data Click on the Load data button at the top menu. Here we have several options, however, we will use the Batch - SQL option. This will bring us to the view shown in the figure below. Apache Druid offers us several options to load data, from several online providers to local files. In this example we will employ a HTTPS URL to load the data from an Open API. However, you can use any option since the process is the same. Once we introduced the URL, we should click on the NEXT button at the bottom-right part of the view. This will bring us to the view shown in the figure below. In this step we should configure the data ingestion settings (usually Apache Druid detects them automatically). However, there is an important setting that we should configure: the timestamp . This setting is the one that will be used to order the data in the database. In this case, we will use the month field because the rows do not have a timestamp field. Once we have configured the settings, we should click on the NEXT button at the bottom-right part of the view. Subsequently, Apache Druid will identify the type of each column automatically. However, we can change the type of each column or add new ones if we want. Once we have configured the settings, we should click on the Start loading data button at the bottom-right part of the view. The loading of data is done in separated tasks. Once all the tasks are finished, the data will be available in the database. You can check the status of the task in the Tasks button at the top menu. Step 3: Check datasources Click on the Datasources button at the top menu. In this view, you can visualise the different datasources that you have in the database. When you click on a datasource, you will be able to see the different columns that it has. Furthermore, you can see the different values that each column has. In this view you can also conduct different actions to the datasource, such as, edit the several performance settings or delete it. Step 4: Querying datasources Click on the QUERY button at the top menu. This view will allow you to query the different datasources that you have in the database. At the left part of the view, all the available datasources are shown. In the middle part of the view, you can write the query that you want to execute. After that, you should click on the Run button at the middle-left part of the view. The results of the query will be shown at the bottom part of the view. Extra: Explore datasources Click on the THREE DOTS at the rightmost part of the top menu. Then, click on the Explore button. This view will allow you to explore the different datasources that you have in the database. At the left part you should select the datasource that you want to explore and in the right part you may choose the chart type and its configuration. After that, you should select the desired column to explore. Finally, you can add several filters at the top part of the view.","title":"Apache Druid Usage Guide"},{"location":"tools/druid/examples/","text":"Apache Druid The integration of Apache Druid are done from other tools like Apache Superset or DataHub. Thus, you can access to the intregration guides of Apache Druid from the guides of the other tools: - Apache Superset - DataHub","title":"Apache Druid"},{"location":"tools/druid/examples/#apache-druid","text":"The integration of Apache Druid are done from other tools like Apache Superset or DataHub. Thus, you can access to the intregration guides of Apache Druid from the guides of the other tools: - Apache Superset - DataHub","title":"Apache Druid"},{"location":"tools/greengage-app-api/","text":"Greengage APP API Introduction: The greengage app api is mesh of multiple services which are provided via a single gateway. This underlying docuemntation will show you in a simple way how you can interact with application and how you can adopt your application to fit in the ecosystem. Endpoints Staging: https://api-stage.greengage.dev/graphql Prouction (available January 2024): https://api.greengage.dev/graphql Datastudio The endpoints always a control-panel under /cp which allows you open the complete documenation of the active connected services eg. https://api-stage.greengage.dev/cp Technology Due to the fact that the (micro-)services are made accessable via the Apollo Server the location, storage of each service is masked by the gateway. GraphQL GraphQL is like a superhero of data query languages, swooping in to save the day for developers tired of over-fetching or under-fetching data. Created by Facebook, this query language provides a more efficient and flexible alternative to traditional REST APIs. With GraphQL, you get to specify exactly what data you need, and you'll receive it in a neat JSON package, eliminating the excess baggage of unnecessary information. It's like ordering a customized pizza instead of settling for a pre-made slice\u2014you get exactly what you asked for, and nothing more. GraphQL provides various type of interactions but the most important are - queries - mutations Queries To get data out of the API you make a \"query\". The datastudio can help you write meaningful queries and helps you find out what the services can do, without you knowing anything about the location or the queries. Example: query Cities { cities { id name } } Mutations Mutations on the other hand are the WRITE/UPDATE/DELETE operations. Or in simple terms every time you send data to the service which should taken care of. Example: ```graphql mutation RegisterAccount { register(data:{ email:\"mpi@sushi.ev\" }) { token refresh_token } } `````` For an overview of all the current data sets and mutations please vistit api-stage.greengage.dev where you can find the complete schema and the interactive documentation. Examples for useage or integratoin An example to provide a restful api via graphql endpoint can be seen here . The link includes a demo implementation and can be used for further useages. For a client side approach to use the graphql endpoint please follow this link . Integration If you want to be integrated into the greengage api or need further support please send an email to support@sushi.dev","title":"Greengage APP API"},{"location":"tools/greengage-app-api/#greengage-app-api","text":"","title":"Greengage APP API"},{"location":"tools/greengage-app-api/#introduction","text":"The greengage app api is mesh of multiple services which are provided via a single gateway. This underlying docuemntation will show you in a simple way how you can interact with application and how you can adopt your application to fit in the ecosystem.","title":"Introduction:"},{"location":"tools/greengage-app-api/#endpoints","text":"Staging: https://api-stage.greengage.dev/graphql Prouction (available January 2024): https://api.greengage.dev/graphql","title":"Endpoints"},{"location":"tools/greengage-app-api/#datastudio","text":"The endpoints always a control-panel under /cp which allows you open the complete documenation of the active connected services eg. https://api-stage.greengage.dev/cp","title":"Datastudio"},{"location":"tools/greengage-app-api/#technology","text":"Due to the fact that the (micro-)services are made accessable via the Apollo Server the location, storage of each service is masked by the gateway.","title":"Technology"},{"location":"tools/greengage-app-api/#graphql","text":"GraphQL is like a superhero of data query languages, swooping in to save the day for developers tired of over-fetching or under-fetching data. Created by Facebook, this query language provides a more efficient and flexible alternative to traditional REST APIs. With GraphQL, you get to specify exactly what data you need, and you'll receive it in a neat JSON package, eliminating the excess baggage of unnecessary information. It's like ordering a customized pizza instead of settling for a pre-made slice\u2014you get exactly what you asked for, and nothing more. GraphQL provides various type of interactions but the most important are - queries - mutations","title":"GraphQL"},{"location":"tools/greengage-app-api/#queries","text":"To get data out of the API you make a \"query\". The datastudio can help you write meaningful queries and helps you find out what the services can do, without you knowing anything about the location or the queries. Example: query Cities { cities { id name } }","title":"Queries"},{"location":"tools/greengage-app-api/#mutations","text":"Mutations on the other hand are the WRITE/UPDATE/DELETE operations. Or in simple terms every time you send data to the service which should taken care of. Example: ```graphql mutation RegisterAccount { register(data:{ email:\"mpi@sushi.ev\" }) { token refresh_token } } `````` For an overview of all the current data sets and mutations please vistit api-stage.greengage.dev where you can find the complete schema and the interactive documentation.","title":"Mutations"},{"location":"tools/greengage-app-api/#examples-for-useage-or-integratoin","text":"An example to provide a restful api via graphql endpoint can be seen here . The link includes a demo implementation and can be used for further useages. For a client side approach to use the graphql endpoint please follow this link .","title":"Examples for useage or integratoin"},{"location":"tools/greengage-app-api/#integration","text":"If you want to be integrated into the greengage api or need further support please send an email to support@sushi.dev","title":"Integration"},{"location":"tools/greengage-app-api/examples/client-js/","text":"How to to fetch data from an GRAPHQL endpoint Javascript Open index.html to see a live demo for a gql query in javascript. Please be a Curl curl --location --request POST 'https://api-stage.greengage.dev/graphql' \\ --header 'Content-Type: application/json' \\ --data-raw '{\"query\":\"query {\\n cities {\\n id\\n name\\n }\\n}\",\"variables\":{}}'","title":"Index"},{"location":"tools/greengage-app-api/examples/client-js/#how-to-to-fetch-data-from-an-graphql-endpoint","text":"","title":"How to to fetch data from an GRAPHQL endpoint"},{"location":"tools/greengage-app-api/examples/client-js/#javascript","text":"Open index.html to see a live demo for a gql query in javascript. Please be a","title":"Javascript"},{"location":"tools/greengage-app-api/examples/client-js/#curl","text":"curl --location --request POST 'https://api-stage.greengage.dev/graphql' \\ --header 'Content-Type: application/json' \\ --data-raw '{\"query\":\"query {\\n cities {\\n id\\n name\\n }\\n}\",\"variables\":{}}'","title":"Curl"},{"location":"tools/greengage-app-api/examples/restful-to-graphql/","text":"GraphQL Resolver a RESTful api Installation Step 1 Create an .env file with the following content. Add the base path to the API. PORT=3333 URL='https://example.com/' Please be aware that the URL parameter will not be taken if the request header \"drupal\" is set. Step 2 Run the following command: npm i && npm run production","title":"GraphQL Resolver a RESTful api"},{"location":"tools/greengage-app-api/examples/restful-to-graphql/#graphql-resolver-a-restful-api","text":"","title":"GraphQL Resolver a RESTful api"},{"location":"tools/greengage-app-api/examples/restful-to-graphql/#installation","text":"","title":"Installation"},{"location":"tools/greengage-app-api/examples/restful-to-graphql/#step-1","text":"Create an .env file with the following content. Add the base path to the API. PORT=3333 URL='https://example.com/' Please be aware that the URL parameter will not be taken if the request header \"drupal\" is set.","title":"Step 1"},{"location":"tools/greengage-app-api/examples/restful-to-graphql/#step-2","text":"Run the following command: npm i && npm run production","title":"Step 2"},{"location":"tools/keycloak/","text":"Keycloak Introduction: In today's rapidly evolving tech landscape, ensuring the security of your applications is paramount. As you venture into developing or enhancing your application, integrating a robust authentication and authorization mechanism is crucial. This tutorial aims to guide you through setting up a development environment by integrating Keycloak, an open-source Identity and Access Management solution. Whether you're working on a brand new project or improving an existing one, following this guide will provide a solid foundation for securing your application using Keycloak. Features Keycloak: Single Sign-On (SSO) & Sign-Out: Keycloak facilitates SSO and Single Sign-Out, allowing users to log in once and access various applications without needing to log in again. Standards-Based: Keycloak supports standard protocols for authentication and authorization such as OpenID Connect, OAuth 2.0, and SAML 2.0, ensuring compatibility with various systems. User Federation: It allows user federation with a variety of sources including LDAP and Active Directory, enabling you to manage users and their roles centrally. Customizable: Keycloak is highly customizable, offering a broad range of options to tailor authentication and authorization flows to meet your specific requirements. Scalable: It is designed to be scalable, catering to both small applications and large enterprise solutions with millions of users. Role-Based Access Control (RBAC): Keycloak provides fine-grained authorization and role management, enabling you to control who has access to what in your applications. Multi-factor Authentication (MFA): It supports multi-factor authentication, enhancing security by requiring users to provide multiple forms of identification before gaining access. Social Login: Keycloak supports social logins, allowing users to log in using their social media accounts which can enhance user experience and increase adoption rates. Easy to Integrate: With a variety of client adapters and libraries, integrating Keycloak with your application is straightforward regardless of the technology stack you're using. Support and Community: Being an open-source project, Keycloak has a vibrant community and a wealth of resources available to help you throughout your integration journey. Use case The following diagram show the sequences for an user auythentication procress using Keycloak as an OpenID Connect (OIDC) provider The User initiates the process by requesting a page from the Application. The Application then redirects the User to a browser for authentication. The browser initiates an Authentication Request to Keycloak. Keycloak presents a Login Page to the User. The User enters their credentials into the login page. The credentials are submitted back to Keycloak for validation. Upon successful validation, Keycloak redirects the User back to the Application. The Application makes a Token Request to Keycloak. Keycloak validates the request and, if valid, returns a Token to the Application. With the Token, the User is granted access to the requested page within the Application.","title":"Keycloak"},{"location":"tools/keycloak/#keycloak","text":"","title":"Keycloak"},{"location":"tools/keycloak/#introduction","text":"In today's rapidly evolving tech landscape, ensuring the security of your applications is paramount. As you venture into developing or enhancing your application, integrating a robust authentication and authorization mechanism is crucial. This tutorial aims to guide you through setting up a development environment by integrating Keycloak, an open-source Identity and Access Management solution. Whether you're working on a brand new project or improving an existing one, following this guide will provide a solid foundation for securing your application using Keycloak.","title":"Introduction:"},{"location":"tools/keycloak/#features-keycloak","text":"Single Sign-On (SSO) & Sign-Out: Keycloak facilitates SSO and Single Sign-Out, allowing users to log in once and access various applications without needing to log in again. Standards-Based: Keycloak supports standard protocols for authentication and authorization such as OpenID Connect, OAuth 2.0, and SAML 2.0, ensuring compatibility with various systems. User Federation: It allows user federation with a variety of sources including LDAP and Active Directory, enabling you to manage users and their roles centrally. Customizable: Keycloak is highly customizable, offering a broad range of options to tailor authentication and authorization flows to meet your specific requirements. Scalable: It is designed to be scalable, catering to both small applications and large enterprise solutions with millions of users. Role-Based Access Control (RBAC): Keycloak provides fine-grained authorization and role management, enabling you to control who has access to what in your applications. Multi-factor Authentication (MFA): It supports multi-factor authentication, enhancing security by requiring users to provide multiple forms of identification before gaining access. Social Login: Keycloak supports social logins, allowing users to log in using their social media accounts which can enhance user experience and increase adoption rates. Easy to Integrate: With a variety of client adapters and libraries, integrating Keycloak with your application is straightforward regardless of the technology stack you're using. Support and Community: Being an open-source project, Keycloak has a vibrant community and a wealth of resources available to help you throughout your integration journey.","title":"Features Keycloak:"},{"location":"tools/keycloak/#use-case","text":"The following diagram show the sequences for an user auythentication procress using Keycloak as an OpenID Connect (OIDC) provider The User initiates the process by requesting a page from the Application. The Application then redirects the User to a browser for authentication. The browser initiates an Authentication Request to Keycloak. Keycloak presents a Login Page to the User. The User enters their credentials into the login page. The credentials are submitted back to Keycloak for validation. Upon successful validation, Keycloak redirects the User back to the Application. The Application makes a Token Request to Keycloak. Keycloak validates the request and, if valid, returns a Token to the Application. With the Token, the User is granted access to the requested page within the Application.","title":"Use case"},{"location":"tools/keycloak/integration/","text":"Keycloak Integration Guide for Third-Party Services To facilitate a centralized user management system, we are utilizing Keycloak as our identity and access management solution. To integrate your service with our Keycloak system, please follow the instructions below to request the necessary credentials. Step 1: Requesting Client Credentials Send an email to greengage.admin@deusto.es with the subject line: \"[GREENGAGE] Request for Keycloak Client Credentials\". In the body of the email, please provide the following information: Tool name: Description: (no more than 200 characters): Contact name:(for technical propose) Contact email:(for technical propose) Step 2: Receiving Your Credentials Upon receipt of your request, we will review the information and create a unique Client ID and Secret for your service. We will then send you an email with your Client ID and Secret, which you will use to configure the integration on your side. Step 3: Configuring Your Service Once you have received your Client ID and Secret, incorporate these credentials into your service\u2019s authentication module. Ensure that your service is set to communicate with our Keycloak server using the OpenID Connect protocol. Step 4: Testing the Integration After configuring your service, conduct thorough testing to ensure that the authentication process works correctly. If you encounter any issues during testing, please reach out to us via email for support. If you have any questions about Keycloak integration, please contact us at greengage.admin@deusto.es with the subject \"[Greengage] Keycloak support request\" Important Notes: Keep your Client Secret confidential. Do not share it with unauthorized personnel or services. If you believe your Client Secret has been compromised, contact us immediately to issue a new one. Adhere to all security best practices when implementing the integration to safeguard user data.","title":"Keycloak Integration Guide for Third-Party Services"},{"location":"tools/keycloak/integration/#keycloak-integration-guide-for-third-party-services","text":"To facilitate a centralized user management system, we are utilizing Keycloak as our identity and access management solution. To integrate your service with our Keycloak system, please follow the instructions below to request the necessary credentials. Step 1: Requesting Client Credentials Send an email to greengage.admin@deusto.es with the subject line: \"[GREENGAGE] Request for Keycloak Client Credentials\". In the body of the email, please provide the following information: Tool name: Description: (no more than 200 characters): Contact name:(for technical propose) Contact email:(for technical propose) Step 2: Receiving Your Credentials Upon receipt of your request, we will review the information and create a unique Client ID and Secret for your service. We will then send you an email with your Client ID and Secret, which you will use to configure the integration on your side. Step 3: Configuring Your Service Once you have received your Client ID and Secret, incorporate these credentials into your service\u2019s authentication module. Ensure that your service is set to communicate with our Keycloak server using the OpenID Connect protocol. Step 4: Testing the Integration After configuring your service, conduct thorough testing to ensure that the authentication process works correctly. If you encounter any issues during testing, please reach out to us via email for support. If you have any questions about Keycloak integration, please contact us at greengage.admin@deusto.es with the subject \"[Greengage] Keycloak support request\" Important Notes: Keep your Client Secret confidential. Do not share it with unauthorized personnel or services. If you believe your Client Secret has been compromised, contact us immediately to issue a new one. Adhere to all security best practices when implementing the integration to safeguard user data.","title":"Keycloak Integration Guide for Third-Party Services"},{"location":"tools/keycloak/examples/","text":"Examples This section includes examples in different programming languages on how integration with the provided KeyCloak can be performed. Nodejs Prerequisites Docker and Docker Compose installed. Node.js and npm installed. Basic understanding of Keycloak, Docker, and Node.js. Project Structure Source: example example/ |-- docker-compose.yml |-- realm-export.json |-- keycloak.json |-- package.json |-- index.js |-- run.sh (for Linux/macOS) |-- run.bat (for Windows) Step 1: Setting Up Keycloak docker-compose.yml : This file contains the configuration to run a Keycloak container. Make sure the docker-compose.yml file is set up as provided in your project. realm-export.json : This file should be configured according to your Keycloak realm requirements. It contains realm, client, user, and role configurations. Step 2: Setting Up Node.js Application package.json : This file contains your project metadata and dependencies. Ensure express , express-session , and keycloak-connect dependencies are listed. index.js : This file contains your Express application setup. It sets up routes for login, logout, and the home page which displays the JWT. keycloak.json : This file contains the Keycloak client configuration. Update the realm , resource , and credentials fields with your Keycloak configuration. Installing Dependencies : Run the following command to install the necessary packages as listed in your package.json : npm i Step 3: Running the Services Linux/macOS : Ensure run.sh is executable: chmod +x run.sh . Execute run.sh to start the services: ./run.sh . Windows : Double-click run.bat or run it in the command prompt to start the services. Accessing the Application Navigate to localhost:3000/auth to log in using Keycloak. Once logged in, navigate to localhost:3000 to view the JWT and its decoded payload. To logout, navigate to localhost:3000/logout . Step 1: Install and Setup Keycloak Download and install Keycloak from the official website . Start Keycloak by navigating to the bin directory of your Keycloak installation and executing the standalone.sh (for Linux/macOS) or standalone.bat (for Windows) script. Access the Keycloak Admin Console at http://localhost:8080/auth and complete the initial setup. Create an admin user for managing Keycloak. Step 2: Create a Realm and a Client (OpenID Connect) Create a New Realm: Navigate to the Keycloak Admin Console. Click on \"Add realm\" to create a new realm. Enter the required details for your realm and save. Register a Client: Navigate to Clients and click Create . Provide a Client ID , and select the Client Protocol as openid-connect . Select the Client Access Type as confidential if your client is a web application that can secure the client secret. Otherwise, select public if your client is a native app or a JavaScript app running in the browser. Set Standard Flow Enabled to ON if you want to use the Authorization Code Flow which is recommended for most scenarios. Configure OpenID Connect Protocol: For each client, you can tailor what claims and assertions are stored in the OIDC token by creating and configuring protocol mappers. You may need to set up JSON mapping for certain claim keys in your application to handle roles or other claims passed by Keycloak. Client Adapters: Install a Keycloak Adapter in your application environment to communicate and be secured by Keycloak. Keycloak provides adapters for different platforms, and there are also third-party adapters available. Test Your Setup: At this point, it would be prudent to test your setup by attempting to authenticate using OpenID Connect. There are various grant types supported by Keycloak for authenticating users including authorization code, implicit, and client credentials. Additional Configuration (Optional): Depending on your application's requirements, you might need to configure additional settings in Keycloak or in your application. For instance, you might need to set up user roles, groups, and permissions, or configure multi-factor authentication. Documentations and Tutorials: There are various resources available that provide step-by-step guides or tutorials on integrating OpenID Connect with Keycloak, including the Keycloak official documentation . This extended step should provide a more thorough understanding of how to integrate OpenID Connect with Keycloak. However, the exact steps might vary based on your application's technology stack and your specific requirements. Step 3: Configure your System For a JavaScript application, you could use the Keycloak JavaScript adapter. npm install keycloak-js or a middleware as keycloak-connect npm install keycloak-connect Configure the library with the details of your Keycloak realm and client. // Example configuration for a JavaScript application const keycloak = Keycloak({ url: \"http://localhost:8080/auth\", realm: \"<your-realm>\", clientId: \"<your-client-id>\", }); Step 4: Integrate Authentication Use the library to add authentication to your system. For a web application, this would typically involve redirecting unauthenticated users to the Keycloak login page, and handling the tokens returned by Keycloak upon successful authentication. // Example integration for a JavaScript application keycloak .init({ onLoad: \"login-required\" }) .then((authenticated) => { console.log(authenticated ? \"Authenticated\" : \"Not authenticated\"); }) .catch((error) => { console.error(\"Failed to initialize authentication\", error); }); Step 5: Integrate Authorization Use the tokens obtained during authentication to make authorized requests to your system's backend, and to check the user's roles and permissions. // Example authorization check in a JavaScript application if (keycloak.hasRealmRole(\"admin\")) { console.log(\"User is an admin\"); } This tutorial provides a high-level overview of the steps involved in integrating Keycloak with your system. The exact steps and code may vary depending on the specifics of your system and the programming languages and frameworks you are using. Python Prerequisites Docker and Docker Compose installed. Python and pip installed. Basic understanding of Keycloak, Docker, Flask, and Python. Access the application over HTTPS and accept the self-signed certificate warning: \"The certificate is not trusted because it is self-signed.\" Project Structure Source: example example/ |-- docker-compose.yml |-- realm-export.json |-- client_secrets.json |-- requirements.txt |-- index.py |-- run.sh (for Linux/macOS) |-- run.bat (for Windows) Step 1: Setting Up Keycloak docker-compose.yml : This file contains the configuration to run a Keycloak container. Ensure it's set up as provided in your project. realm-export.json : Configure this file according to your Keycloak realm requirements. It contains realm, client, user, and role configurations. Step 2: Setting Up Python Flask Application requirements.txt : Lists the necessary Python packages, including Flask and Flask-OIDC. client_secrets.json : Contains Keycloak client configuration. Update the client_id , client_secret , and URLs according to your Keycloak setup. index.py : Contains your Flask application. Sets up routes for login, logout, and home page displaying user information. Step 3: Installing Dependencies Run the following command to install necessary Python packages: pip install -r requirements.txt Step 4: Running the Services Linux/macOS : Make run.sh executable: chmod +x run.sh . Execute run.sh to start the services: ./run.sh . Windows : Execute run.bat to start the services. Accessing the Application Navigate to https://localhost:3000/ (accept the self-signed certificate warning). Use the Keycloak authentication process to log in. Once logged in, user information will be displayed on the home page. To logout, navigate to https://localhost:3000/logout . Important Notes Ensure all URLs in client_secrets.json and realm-export.json match your Keycloak configuration. Remember to access the application via HTTPS and accept the browser warning about the self-signed certificate. Modify the index.py file to suit your application's specific Flask and OIDC needs. For more information you can visit the official documentation in https://www.keycloak.org/documentation","title":"Examples"},{"location":"tools/keycloak/examples/#examples","text":"This section includes examples in different programming languages on how integration with the provided KeyCloak can be performed.","title":"Examples"},{"location":"tools/keycloak/examples/#nodejs","text":"","title":"Nodejs"},{"location":"tools/keycloak/examples/#prerequisites","text":"Docker and Docker Compose installed. Node.js and npm installed. Basic understanding of Keycloak, Docker, and Node.js.","title":"Prerequisites"},{"location":"tools/keycloak/examples/#project-structure","text":"Source: example example/ |-- docker-compose.yml |-- realm-export.json |-- keycloak.json |-- package.json |-- index.js |-- run.sh (for Linux/macOS) |-- run.bat (for Windows)","title":"Project Structure"},{"location":"tools/keycloak/examples/#step-1-setting-up-keycloak","text":"docker-compose.yml : This file contains the configuration to run a Keycloak container. Make sure the docker-compose.yml file is set up as provided in your project. realm-export.json : This file should be configured according to your Keycloak realm requirements. It contains realm, client, user, and role configurations.","title":"Step 1: Setting Up Keycloak"},{"location":"tools/keycloak/examples/#step-2-setting-up-nodejs-application","text":"package.json : This file contains your project metadata and dependencies. Ensure express , express-session , and keycloak-connect dependencies are listed. index.js : This file contains your Express application setup. It sets up routes for login, logout, and the home page which displays the JWT. keycloak.json : This file contains the Keycloak client configuration. Update the realm , resource , and credentials fields with your Keycloak configuration. Installing Dependencies : Run the following command to install the necessary packages as listed in your package.json : npm i","title":"Step 2: Setting Up Node.js Application"},{"location":"tools/keycloak/examples/#step-3-running-the-services","text":"Linux/macOS : Ensure run.sh is executable: chmod +x run.sh . Execute run.sh to start the services: ./run.sh . Windows : Double-click run.bat or run it in the command prompt to start the services.","title":"Step 3: Running the Services"},{"location":"tools/keycloak/examples/#accessing-the-application","text":"Navigate to localhost:3000/auth to log in using Keycloak. Once logged in, navigate to localhost:3000 to view the JWT and its decoded payload. To logout, navigate to localhost:3000/logout .","title":"Accessing the Application"},{"location":"tools/keycloak/examples/#step-1-install-and-setup-keycloak","text":"Download and install Keycloak from the official website . Start Keycloak by navigating to the bin directory of your Keycloak installation and executing the standalone.sh (for Linux/macOS) or standalone.bat (for Windows) script. Access the Keycloak Admin Console at http://localhost:8080/auth and complete the initial setup. Create an admin user for managing Keycloak.","title":"Step 1: Install and Setup Keycloak"},{"location":"tools/keycloak/examples/#step-2-create-a-realm-and-a-client-openid-connect","text":"Create a New Realm: Navigate to the Keycloak Admin Console. Click on \"Add realm\" to create a new realm. Enter the required details for your realm and save. Register a Client: Navigate to Clients and click Create . Provide a Client ID , and select the Client Protocol as openid-connect . Select the Client Access Type as confidential if your client is a web application that can secure the client secret. Otherwise, select public if your client is a native app or a JavaScript app running in the browser. Set Standard Flow Enabled to ON if you want to use the Authorization Code Flow which is recommended for most scenarios. Configure OpenID Connect Protocol: For each client, you can tailor what claims and assertions are stored in the OIDC token by creating and configuring protocol mappers. You may need to set up JSON mapping for certain claim keys in your application to handle roles or other claims passed by Keycloak. Client Adapters: Install a Keycloak Adapter in your application environment to communicate and be secured by Keycloak. Keycloak provides adapters for different platforms, and there are also third-party adapters available. Test Your Setup: At this point, it would be prudent to test your setup by attempting to authenticate using OpenID Connect. There are various grant types supported by Keycloak for authenticating users including authorization code, implicit, and client credentials. Additional Configuration (Optional): Depending on your application's requirements, you might need to configure additional settings in Keycloak or in your application. For instance, you might need to set up user roles, groups, and permissions, or configure multi-factor authentication. Documentations and Tutorials: There are various resources available that provide step-by-step guides or tutorials on integrating OpenID Connect with Keycloak, including the Keycloak official documentation . This extended step should provide a more thorough understanding of how to integrate OpenID Connect with Keycloak. However, the exact steps might vary based on your application's technology stack and your specific requirements.","title":"Step 2: Create a Realm and a Client (OpenID Connect)"},{"location":"tools/keycloak/examples/#step-3-configure-your-system","text":"For a JavaScript application, you could use the Keycloak JavaScript adapter. npm install keycloak-js or a middleware as keycloak-connect npm install keycloak-connect Configure the library with the details of your Keycloak realm and client. // Example configuration for a JavaScript application const keycloak = Keycloak({ url: \"http://localhost:8080/auth\", realm: \"<your-realm>\", clientId: \"<your-client-id>\", });","title":"Step 3: Configure your System"},{"location":"tools/keycloak/examples/#step-4-integrate-authentication","text":"Use the library to add authentication to your system. For a web application, this would typically involve redirecting unauthenticated users to the Keycloak login page, and handling the tokens returned by Keycloak upon successful authentication. // Example integration for a JavaScript application keycloak .init({ onLoad: \"login-required\" }) .then((authenticated) => { console.log(authenticated ? \"Authenticated\" : \"Not authenticated\"); }) .catch((error) => { console.error(\"Failed to initialize authentication\", error); });","title":"Step 4: Integrate Authentication"},{"location":"tools/keycloak/examples/#step-5-integrate-authorization","text":"Use the tokens obtained during authentication to make authorized requests to your system's backend, and to check the user's roles and permissions. // Example authorization check in a JavaScript application if (keycloak.hasRealmRole(\"admin\")) { console.log(\"User is an admin\"); } This tutorial provides a high-level overview of the steps involved in integrating Keycloak with your system. The exact steps and code may vary depending on the specifics of your system and the programming languages and frameworks you are using.","title":"Step 5: Integrate Authorization"},{"location":"tools/keycloak/examples/#python","text":"","title":"Python"},{"location":"tools/keycloak/examples/#prerequisites_1","text":"Docker and Docker Compose installed. Python and pip installed. Basic understanding of Keycloak, Docker, Flask, and Python. Access the application over HTTPS and accept the self-signed certificate warning: \"The certificate is not trusted because it is self-signed.\"","title":"Prerequisites"},{"location":"tools/keycloak/examples/#project-structure_1","text":"Source: example example/ |-- docker-compose.yml |-- realm-export.json |-- client_secrets.json |-- requirements.txt |-- index.py |-- run.sh (for Linux/macOS) |-- run.bat (for Windows)","title":"Project Structure"},{"location":"tools/keycloak/examples/#step-1-setting-up-keycloak_1","text":"docker-compose.yml : This file contains the configuration to run a Keycloak container. Ensure it's set up as provided in your project. realm-export.json : Configure this file according to your Keycloak realm requirements. It contains realm, client, user, and role configurations.","title":"Step 1: Setting Up Keycloak"},{"location":"tools/keycloak/examples/#step-2-setting-up-python-flask-application","text":"requirements.txt : Lists the necessary Python packages, including Flask and Flask-OIDC. client_secrets.json : Contains Keycloak client configuration. Update the client_id , client_secret , and URLs according to your Keycloak setup. index.py : Contains your Flask application. Sets up routes for login, logout, and home page displaying user information.","title":"Step 2: Setting Up Python Flask Application"},{"location":"tools/keycloak/examples/#step-3-installing-dependencies","text":"Run the following command to install necessary Python packages: pip install -r requirements.txt","title":"Step 3: Installing Dependencies"},{"location":"tools/keycloak/examples/#step-4-running-the-services","text":"Linux/macOS : Make run.sh executable: chmod +x run.sh . Execute run.sh to start the services: ./run.sh . Windows : Execute run.bat to start the services.","title":"Step 4: Running the Services"},{"location":"tools/keycloak/examples/#accessing-the-application_1","text":"Navigate to https://localhost:3000/ (accept the self-signed certificate warning). Use the Keycloak authentication process to log in. Once logged in, user information will be displayed on the home page. To logout, navigate to https://localhost:3000/logout .","title":"Accessing the Application"},{"location":"tools/keycloak/examples/#important-notes","text":"Ensure all URLs in client_secrets.json and realm-export.json match your Keycloak configuration. Remember to access the application via HTTPS and accept the browser warning about the self-signed certificate. Modify the index.py file to suit your application's specific Flask and OIDC needs. For more information you can visit the official documentation in https://www.keycloak.org/documentation","title":"Important Notes"},{"location":"tools/mindview/","text":"MindView Introduction MindView is a crowd-driven platform for collecting and uploading geo-tagged street-level imagery, utilizing built-in smartphone cameras and compatible devices like 360\u00b0 cameras, car dashboards, and helmet cameras. This innovative system engages users to undertake diverse 'missions', such as traversing predefined routes, capturing specific objects, or remaining at designated locations, rewarding them based on the mission's complexity, duration, and quantity. The collected imagery undergoes processing through advanced deep-learning algorithms, extracting valuable, anonymized insights about urban environments and social dynamics. Key insights include housing and infrastructure data, green space analysis, socio-economic and cultural insights, and mobility and traffic patterns. This approach is particularly useful for maintaining up-to-date and comprehensive maps, a task that can be resource-intensive for official mapping agencies. This is particularly valuable in regions with limited or outdated official mapping sources. By harnessing the collective efforts of the community, especially in less documented or rapidly changing areas, MindView fills critical gaps in mapping data. MindView plays an integral role in the \"Data Crowdsourcing and Curation\" phase of the GREENGAGE project. It addresses a key citizen science challenge in geospatial data collection and mapping, enabling users to actively contribute to the creation and improvement of spatial maps, in a way that requires no pre-existing skills or knowledge. In this way, MindView serves a wide range of beneficiaries including urban planners, disaster risk managers, territorial managers, sociologists, and environmentalists, providing invaluable data for urban development, risk assessment, and sociocultural research. Features of MindView MindView is built upon several key components, each playing a vital role in its functionality: The MindEarth App (GREENGAGE Edition) This is an Android-based mobile application, currently available as a standalone APK file and soon to be launched on the Google Play Store. The MindEarth App provides users with access to a variety of missions, which are commissioned by third parties for purposes such as research, urban planning, or commercial objectives. These missions involve diverse tasks, including: - Moving from one place to another along a predetermined route, typically ranging from 500 meters to 2 kilometers. - Capturing photographs of specific objects at designated times of the day, such as shopfronts, monuments, or building facades. - Staying at a certain location for a fixed period, usually between 15 to 30 minutes. Some missions come with strict deadlines, while others can be completed at the user's convenience. The app allows users to view details of each mission, including the location, duration, required device, deadline, and compensation. Users can select missions based on their interests and availability. For instance, they might be tasked with photographing particular street scenes at specific times or days. The app is developed on a Flutter codebase with customized views and routers, facilitating each phase of the mission from reservation to image upload. Missions are designed modularly, with the possibility of integrating custom procedures to meet specific data collection needs. Mission Control: This dedicated back-end platform is crucial for defining, designing, and monitoring the missions available in the MindEarth App. It handles several key functions: - Generating Mapping Campaigns, each linked to a particular Area of Interest (AOI) that requires surveying. - Designing individual Missions for each Campaign, utilizing GeoJson format to outline the specific path mappers will follow. - Setting specific time constraints for each Campaign and Mission, including the start and end dates, as well as the times and dates for data collection. - Determining the effort (time and distance) and monetary reward associated with each mission. - Gathering statistics on mission completion, image collection, and survey quality for effective monitoring. DataView DataView is an interactive set of RESTAPIs that facilitate seamless interaction with the data generated by MindView. It allows the integration of additional data sources or services into the workflow of the APIs, enhancing their overall utility. DataView provides various data export options, enabling users to download the collected data in multiple formats such as JSON, CSV, or other geo-referenced file types. This feature is particularly useful for integrating with external data processing and visualization systems. DataView is secured by a user management system overseen by a KeyCloak instance. All API and backend connections are encrypted using SSL certificates, ensuring data security. Furthermore, all collected data is stored within the private section of an AWS VPC, emphasizing data privacy and protection. Use Case Scenario The MindEarth App is ideally suited for scenarios requiring immediate, real-time photographic surveys of urban environments. A prime example of its application is in analyzing pedestrian traffic patterns within bustling city centers or public spaces, which aids in refining urban planning and maximizing space utilization. To initiate a survey, the first step involves delineating the Area of Interest (AOI) to be surveyed. This includes setting specific parameters such as the time of day, day of the week, and the number of passovers necessary for enhancing the statistical reliability of the data. These parameters are established within the Mission Control platform, either directly by the MindEarth team according to client specifications or by an authorized external administrator. Once these missions are formulated and released, they are instantly accessible to users via the MindEarth App. Upon initial launch of the app, users are prompted to either create a new account or log in with existing credentials. Subsequently, they can explore available missions nearby through a list or map interface. These missions cover various activities, ranging from navigating specific streets or areas to following designated routes with clear start and end points. Each mission is detailed with a title, description, estimated duration, distance, and a reward for successful completion. Additionally, the app notifies users of new missions in their vicinity. Users have the flexibility to select and reserve one or more missions based on their interest. When they approach the mission's starting point, they can begin the survey by pressing the 'Start' button and then proceed as per the instructions displayed on their smartphone, including route guidance and location tracking. After completing the designated tasks and reaching the end point of the mission, users can finalize the survey. Post-mission, the app presents an upload page where users can review and upload the captured images to MindEarth\u2019s secure servers. The app's reward section displays the user's accumulated credits for completed tasks. In adherence to GDPR regulations, the app employs advanced machine learning algorithms to anonymize any personal data in the images, both on the local device and subsequently in a secure AWS environment managed by MindEarth. These anonymized images contribute to aggregated data layers focusing on specific aspects, such as pedestrian flow. The image processing is performed on MindEarth's cloud backend, remaining transparent to the user. Finally, the aggregated data and the data layers are made available to clients through DataView, a dedicated REST API that presents the database content according to a predefined schema. Users can access and interact with these APIs, which require no installation, through a public swagger interface for interactive development.","title":"MindView"},{"location":"tools/mindview/#mindview","text":"","title":"MindView"},{"location":"tools/mindview/#introduction","text":"MindView is a crowd-driven platform for collecting and uploading geo-tagged street-level imagery, utilizing built-in smartphone cameras and compatible devices like 360\u00b0 cameras, car dashboards, and helmet cameras. This innovative system engages users to undertake diverse 'missions', such as traversing predefined routes, capturing specific objects, or remaining at designated locations, rewarding them based on the mission's complexity, duration, and quantity. The collected imagery undergoes processing through advanced deep-learning algorithms, extracting valuable, anonymized insights about urban environments and social dynamics. Key insights include housing and infrastructure data, green space analysis, socio-economic and cultural insights, and mobility and traffic patterns. This approach is particularly useful for maintaining up-to-date and comprehensive maps, a task that can be resource-intensive for official mapping agencies. This is particularly valuable in regions with limited or outdated official mapping sources. By harnessing the collective efforts of the community, especially in less documented or rapidly changing areas, MindView fills critical gaps in mapping data. MindView plays an integral role in the \"Data Crowdsourcing and Curation\" phase of the GREENGAGE project. It addresses a key citizen science challenge in geospatial data collection and mapping, enabling users to actively contribute to the creation and improvement of spatial maps, in a way that requires no pre-existing skills or knowledge. In this way, MindView serves a wide range of beneficiaries including urban planners, disaster risk managers, territorial managers, sociologists, and environmentalists, providing invaluable data for urban development, risk assessment, and sociocultural research.","title":"Introduction"},{"location":"tools/mindview/#features-of-mindview","text":"MindView is built upon several key components, each playing a vital role in its functionality: The MindEarth App (GREENGAGE Edition) This is an Android-based mobile application, currently available as a standalone APK file and soon to be launched on the Google Play Store. The MindEarth App provides users with access to a variety of missions, which are commissioned by third parties for purposes such as research, urban planning, or commercial objectives. These missions involve diverse tasks, including: - Moving from one place to another along a predetermined route, typically ranging from 500 meters to 2 kilometers. - Capturing photographs of specific objects at designated times of the day, such as shopfronts, monuments, or building facades. - Staying at a certain location for a fixed period, usually between 15 to 30 minutes. Some missions come with strict deadlines, while others can be completed at the user's convenience. The app allows users to view details of each mission, including the location, duration, required device, deadline, and compensation. Users can select missions based on their interests and availability. For instance, they might be tasked with photographing particular street scenes at specific times or days. The app is developed on a Flutter codebase with customized views and routers, facilitating each phase of the mission from reservation to image upload. Missions are designed modularly, with the possibility of integrating custom procedures to meet specific data collection needs. Mission Control: This dedicated back-end platform is crucial for defining, designing, and monitoring the missions available in the MindEarth App. It handles several key functions: - Generating Mapping Campaigns, each linked to a particular Area of Interest (AOI) that requires surveying. - Designing individual Missions for each Campaign, utilizing GeoJson format to outline the specific path mappers will follow. - Setting specific time constraints for each Campaign and Mission, including the start and end dates, as well as the times and dates for data collection. - Determining the effort (time and distance) and monetary reward associated with each mission. - Gathering statistics on mission completion, image collection, and survey quality for effective monitoring. DataView DataView is an interactive set of RESTAPIs that facilitate seamless interaction with the data generated by MindView. It allows the integration of additional data sources or services into the workflow of the APIs, enhancing their overall utility. DataView provides various data export options, enabling users to download the collected data in multiple formats such as JSON, CSV, or other geo-referenced file types. This feature is particularly useful for integrating with external data processing and visualization systems. DataView is secured by a user management system overseen by a KeyCloak instance. All API and backend connections are encrypted using SSL certificates, ensuring data security. Furthermore, all collected data is stored within the private section of an AWS VPC, emphasizing data privacy and protection.","title":"Features of MindView"},{"location":"tools/mindview/#use-case-scenario","text":"The MindEarth App is ideally suited for scenarios requiring immediate, real-time photographic surveys of urban environments. A prime example of its application is in analyzing pedestrian traffic patterns within bustling city centers or public spaces, which aids in refining urban planning and maximizing space utilization. To initiate a survey, the first step involves delineating the Area of Interest (AOI) to be surveyed. This includes setting specific parameters such as the time of day, day of the week, and the number of passovers necessary for enhancing the statistical reliability of the data. These parameters are established within the Mission Control platform, either directly by the MindEarth team according to client specifications or by an authorized external administrator. Once these missions are formulated and released, they are instantly accessible to users via the MindEarth App. Upon initial launch of the app, users are prompted to either create a new account or log in with existing credentials. Subsequently, they can explore available missions nearby through a list or map interface. These missions cover various activities, ranging from navigating specific streets or areas to following designated routes with clear start and end points. Each mission is detailed with a title, description, estimated duration, distance, and a reward for successful completion. Additionally, the app notifies users of new missions in their vicinity. Users have the flexibility to select and reserve one or more missions based on their interest. When they approach the mission's starting point, they can begin the survey by pressing the 'Start' button and then proceed as per the instructions displayed on their smartphone, including route guidance and location tracking. After completing the designated tasks and reaching the end point of the mission, users can finalize the survey. Post-mission, the app presents an upload page where users can review and upload the captured images to MindEarth\u2019s secure servers. The app's reward section displays the user's accumulated credits for completed tasks. In adherence to GDPR regulations, the app employs advanced machine learning algorithms to anonymize any personal data in the images, both on the local device and subsequently in a secure AWS environment managed by MindEarth. These anonymized images contribute to aggregated data layers focusing on specific aspects, such as pedestrian flow. The image processing is performed on MindEarth's cloud backend, remaining transparent to the user. Finally, the aggregated data and the data layers are made available to clients through DataView, a dedicated REST API that presents the database content according to a predefined schema. Users can access and interact with these APIs, which require no installation, through a public swagger interface for interactive development.","title":"Use Case Scenario"},{"location":"tools/mindview/integration/","text":"MindView Usage & Integration Guide for Third-Party Services MindEarth App (GREENGAGE Edition) If you're interested in participating in the MindView project by using the MindEarth App, here's a detailed guide to help you get started: Step 1. Downloading and Installing the App Download the app : Download the MindEarth App on the Google Play Store, or if it\u2019s not yet available there, download the APK file from the link provided in the resources section. [download app][#] Install the App : Install the app on your Android smartphone. If you're installing via an APK file, make sure to allow installation from unknown sources in your phone's settings. Step 2. Setting Up Your Account Launch the App : Open the MindEarth App on your device. Please note that for the app to function correctly, you need to give it permission to track your precise location, access your camera, microphone, and storage. Register or Log In : Create a new account if you're a first-time user, providing necessary information including payment details to receive compensation for completed missions. If you're a returning user, simply log in with your credentials. Step 3. Exploring Available Missions Browse Missions : Once logged in, you can browse through the list of available missions. These missions are typically commissioned by third parties for research, urban planning, or commercial purposes. View Mission Details : For each mission, you can view detailed information such as the location, the task involved, duration, the device required, deadline, and the compensation offered. Step 4. Selecting and Carry out a Mission Choose a Mission : Select a mission that interests you and fits your availability. Carefully read the mission instructions, including any specific routes to follow or objects to photograph. Start the Mission : When you're ready and at the mission's start location, begin the mission by tapping the 'Start' button. This action automatically activates your camera, and it starts taking pictures as per the mission's requirements. Complete the Tasks : Follow the app's instructions closely, ensuring that all necessary photos are taken as per the mission guidelines. Step 5. Uploading Data and Getting Rewards Upload Your Data : After completing the mission, follow the prompts to upload the images collected to the MindEarth servers. Receive Compensation : Based on the mission's guidelines and your performance, you will be compensated. This could be in monetary form or other rewards as specified in the mission details. Compensation for the mission will be processed on monthly bases based on the payment information provided during registration. Step 6. Review and Repeat Track Your Performance and Progress : You can review your completed missions and rewards in the app and track your performance statistics in terms of km walked, missions completed, accuracy and more. Participate in More Missions : Feel free to engage in more missions as they become available. Additional Tips Stay Charged : Ensure your device is sufficiently charged, especially for longer missions. Follow Guidelines : Always adhere to the guidelines for each mission to ensure the quality of data and secure your reward. Privacy Compliance : Be mindful of privacy regulations, especially when photographing in public spaces. Do not deviate from the path associated with your mission. Mission Control Mission Control serves as the back-end component of MindView, designed for the generation and monitoring of mapping campaigns and missions. The platform is typically managed directly by authorized MindEarth team members, or by authorized third parties, each granted access through a secure Keycloak authentication method, functioning as the operational hub of all active campaigns. If you want to use Mission Control, here's a detailed guide to help you get started: Step 1: Accessing Mission Control Receive access credentials : You will need authorized credentials to log in, so ensure you have them beforehand. If you do not have access, please contact [support@mindearth.ch] (support@mindearth.ch) Log In : Access Mission Control through the platform link and using the login credentials provided. Step 2. Setting Up a New Mapping Campaign Create a New Campaign : Start by creating a new mapping campaign. This involves specifying the overarching goals and objectives of the campaign. Define Area of Interest (AOI) : Use the tools available in Mission Control to define the geographic boundaries of your campaign's Area of Interest. Step 3: Designing Missions Creating missions : Within a campaign, you can add individual missions. For each mission, provide specific details such as: A title of the mission A description of the tasks to be performed The path to be followed by the users, defined in a GeoJson format. Time constraints for the mission, including start and end dates and times. Step 4. Setting Rewards and Effort Estimations Effort Estimation : Time and distance effort associated with the mission is automatically calculated and displayed based on the provided path. Reward setting : A monetary reward can be manually assigned to the mission. Step 5. Publishing Missions Publish campaigns and missions : Once you have finalized campaigns and missions with all necessary details, publish them to make them available to MindEarth App users. Step 6. Monitoring and Managing Missions Track Progress : Use Mission Control\u2019s dashboard to monitor the progress of each mission in real-time. Manage Data Collection : Ensure that the data collected meets the quality standards and requirements of the campaign. Step 7. Confirm Mission Data Review and Analysis : Review data collected from completed missions and, if quality is satisfactory to the campaign goals, confirm it to send if for AI processing. Dataview REST API DataView is an integral part of the MindView system, offering a suite of interactive REST APIs that enable interaction with data collected via the MindEarth App and processed by MindEarth. Note: DataView is still under development and no final link to the resource nor a final outline of the data endpoints is available. Authentication KeyCloak Authentication : Secure access control for using DataView APIs. SSL Encryption : All API interactions are SSL encrypted for data security during transmission. Get List of Campaigns Description : Retrieve a list of available campaigns Endpoint : https://greengage-mindview.mindearth.ai/list/campaign Method : GET Header : Bearer Output Example : [ { \"id\": 1, \"campaignName\": \"Test 1\", \"features\": [ { \"id\": 6, \"type\": [1, 2] }, { \"id\": 2, \"type\": [2] } ] }, { \"id\": 2, \"campaignName\": \"Test 2\" } ] Accessing Geospatial Layers Description : Within the selected campaign, define the features of interest and other applicable parameters (e.g., results spatial aggregation methods). Endpoint : https://greengage-mindview.mindearth.ai/results/ / / - Method : GET - Header : Bearer Features ID (Example of typical socio-demographic features extractable through the GREENGAGE project, as discussed with Pilot cities): - 9 \u2192 Footfall - 10 \u2192 Gender - 11 \u2192 Age - \u2026 Geometry Type : Examples of different geometry types: - 1 \u2192 Point - 2 \u2192 Line - 3 \u2192 Raster Grid - 4 \u2192 Vector Polygon Data Export DataView provides options to export data in formats like JSON, CSV, etc., facilitating easy integration with external data processing tools. Single Sign On (SSO) Service To facilitate a centralized user management system, MindEarth App utilizes the Keycloak system provided by Deusto as our identity and access management solution. Please refer to the relevant documentation available at this link . Integrating MindView with the Greengage App To streamline user interactions and mission management between different crowd-based data collection tools, a dedicated integration protocol of the MindEarth App and the GREENGAGE App is implemented. The implementation of a dedicated protocol ensures availability and visibility of MindEarth\u2019s missions across both apps, with seamless redirection for mission execution. Please refer to the relevant documentation available at this link . Testing You can find here some sample output data We can give you early access to the APK and create sample missions around your location if you want to test the app and there are no active missions in your vicinity. Support and Contact If you have any questions or require technical support, do not hesitate to contact our support team at support@mindearth.ch .","title":"MindView Usage & Integration Guide for Third-Party Services"},{"location":"tools/mindview/integration/#mindview-usage-integration-guide-for-third-party-services","text":"","title":"MindView Usage &amp; Integration Guide for Third-Party Services"},{"location":"tools/mindview/integration/#mindearth-app-greengage-edition","text":"If you're interested in participating in the MindView project by using the MindEarth App, here's a detailed guide to help you get started:","title":"MindEarth App (GREENGAGE Edition)"},{"location":"tools/mindview/integration/#step-1-downloading-and-installing-the-app","text":"Download the app : Download the MindEarth App on the Google Play Store, or if it\u2019s not yet available there, download the APK file from the link provided in the resources section. [download app][#] Install the App : Install the app on your Android smartphone. If you're installing via an APK file, make sure to allow installation from unknown sources in your phone's settings.","title":"Step 1. Downloading and Installing the App"},{"location":"tools/mindview/integration/#step-2-setting-up-your-account","text":"Launch the App : Open the MindEarth App on your device. Please note that for the app to function correctly, you need to give it permission to track your precise location, access your camera, microphone, and storage. Register or Log In : Create a new account if you're a first-time user, providing necessary information including payment details to receive compensation for completed missions. If you're a returning user, simply log in with your credentials.","title":"Step 2. Setting Up Your Account"},{"location":"tools/mindview/integration/#step-3-exploring-available-missions","text":"Browse Missions : Once logged in, you can browse through the list of available missions. These missions are typically commissioned by third parties for research, urban planning, or commercial purposes. View Mission Details : For each mission, you can view detailed information such as the location, the task involved, duration, the device required, deadline, and the compensation offered.","title":"Step 3. Exploring Available Missions"},{"location":"tools/mindview/integration/#step-4-selecting-and-carry-out-a-mission","text":"Choose a Mission : Select a mission that interests you and fits your availability. Carefully read the mission instructions, including any specific routes to follow or objects to photograph. Start the Mission : When you're ready and at the mission's start location, begin the mission by tapping the 'Start' button. This action automatically activates your camera, and it starts taking pictures as per the mission's requirements. Complete the Tasks : Follow the app's instructions closely, ensuring that all necessary photos are taken as per the mission guidelines.","title":"Step 4. Selecting and Carry out a Mission"},{"location":"tools/mindview/integration/#step-5-uploading-data-and-getting-rewards","text":"Upload Your Data : After completing the mission, follow the prompts to upload the images collected to the MindEarth servers. Receive Compensation : Based on the mission's guidelines and your performance, you will be compensated. This could be in monetary form or other rewards as specified in the mission details. Compensation for the mission will be processed on monthly bases based on the payment information provided during registration.","title":"Step 5. Uploading Data and Getting Rewards"},{"location":"tools/mindview/integration/#step-6-review-and-repeat","text":"Track Your Performance and Progress : You can review your completed missions and rewards in the app and track your performance statistics in terms of km walked, missions completed, accuracy and more. Participate in More Missions : Feel free to engage in more missions as they become available.","title":"Step 6. Review and Repeat"},{"location":"tools/mindview/integration/#additional-tips","text":"Stay Charged : Ensure your device is sufficiently charged, especially for longer missions. Follow Guidelines : Always adhere to the guidelines for each mission to ensure the quality of data and secure your reward. Privacy Compliance : Be mindful of privacy regulations, especially when photographing in public spaces. Do not deviate from the path associated with your mission.","title":"Additional Tips"},{"location":"tools/mindview/integration/#mission-control","text":"Mission Control serves as the back-end component of MindView, designed for the generation and monitoring of mapping campaigns and missions. The platform is typically managed directly by authorized MindEarth team members, or by authorized third parties, each granted access through a secure Keycloak authentication method, functioning as the operational hub of all active campaigns. If you want to use Mission Control, here's a detailed guide to help you get started:","title":"Mission Control"},{"location":"tools/mindview/integration/#step-1-accessing-mission-control","text":"Receive access credentials : You will need authorized credentials to log in, so ensure you have them beforehand. If you do not have access, please contact [support@mindearth.ch] (support@mindearth.ch) Log In : Access Mission Control through the platform link and using the login credentials provided.","title":"Step 1: Accessing Mission Control"},{"location":"tools/mindview/integration/#step-2-setting-up-a-new-mapping-campaign","text":"Create a New Campaign : Start by creating a new mapping campaign. This involves specifying the overarching goals and objectives of the campaign. Define Area of Interest (AOI) : Use the tools available in Mission Control to define the geographic boundaries of your campaign's Area of Interest.","title":"Step 2. Setting Up a New Mapping Campaign"},{"location":"tools/mindview/integration/#step-3-designing-missions","text":"Creating missions : Within a campaign, you can add individual missions. For each mission, provide specific details such as: A title of the mission A description of the tasks to be performed The path to be followed by the users, defined in a GeoJson format. Time constraints for the mission, including start and end dates and times.","title":"Step 3: Designing Missions"},{"location":"tools/mindview/integration/#step-4-setting-rewards-and-effort-estimations","text":"Effort Estimation : Time and distance effort associated with the mission is automatically calculated and displayed based on the provided path. Reward setting : A monetary reward can be manually assigned to the mission.","title":"Step 4. Setting Rewards and Effort Estimations"},{"location":"tools/mindview/integration/#step-5-publishing-missions","text":"Publish campaigns and missions : Once you have finalized campaigns and missions with all necessary details, publish them to make them available to MindEarth App users.","title":"Step 5. Publishing Missions"},{"location":"tools/mindview/integration/#step-6-monitoring-and-managing-missions","text":"Track Progress : Use Mission Control\u2019s dashboard to monitor the progress of each mission in real-time. Manage Data Collection : Ensure that the data collected meets the quality standards and requirements of the campaign.","title":"Step 6. Monitoring and Managing Missions"},{"location":"tools/mindview/integration/#step-7-confirm-mission-data","text":"Review and Analysis : Review data collected from completed missions and, if quality is satisfactory to the campaign goals, confirm it to send if for AI processing.","title":"Step 7. Confirm Mission Data"},{"location":"tools/mindview/integration/#dataview-rest-api","text":"DataView is an integral part of the MindView system, offering a suite of interactive REST APIs that enable interaction with data collected via the MindEarth App and processed by MindEarth. Note: DataView is still under development and no final link to the resource nor a final outline of the data endpoints is available.","title":"Dataview REST API"},{"location":"tools/mindview/integration/#authentication","text":"KeyCloak Authentication : Secure access control for using DataView APIs. SSL Encryption : All API interactions are SSL encrypted for data security during transmission.","title":"Authentication"},{"location":"tools/mindview/integration/#get-list-of-campaigns","text":"Description : Retrieve a list of available campaigns Endpoint : https://greengage-mindview.mindearth.ai/list/campaign Method : GET Header : Bearer Output Example : [ { \"id\": 1, \"campaignName\": \"Test 1\", \"features\": [ { \"id\": 6, \"type\": [1, 2] }, { \"id\": 2, \"type\": [2] } ] }, { \"id\": 2, \"campaignName\": \"Test 2\" } ]","title":"Get List of Campaigns"},{"location":"tools/mindview/integration/#accessing-geospatial-layers","text":"Description : Within the selected campaign, define the features of interest and other applicable parameters (e.g., results spatial aggregation methods). Endpoint : https://greengage-mindview.mindearth.ai/results/ / / - Method : GET - Header : Bearer Features ID (Example of typical socio-demographic features extractable through the GREENGAGE project, as discussed with Pilot cities): - 9 \u2192 Footfall - 10 \u2192 Gender - 11 \u2192 Age - \u2026 Geometry Type : Examples of different geometry types: - 1 \u2192 Point - 2 \u2192 Line - 3 \u2192 Raster Grid - 4 \u2192 Vector Polygon","title":"Accessing Geospatial Layers"},{"location":"tools/mindview/integration/#data-export","text":"DataView provides options to export data in formats like JSON, CSV, etc., facilitating easy integration with external data processing tools.","title":"Data Export"},{"location":"tools/mindview/integration/#single-sign-on-sso-service","text":"To facilitate a centralized user management system, MindEarth App utilizes the Keycloak system provided by Deusto as our identity and access management solution. Please refer to the relevant documentation available at this link .","title":"Single Sign On (SSO) Service"},{"location":"tools/mindview/integration/#integrating-mindview-with-the-greengage-app","text":"To streamline user interactions and mission management between different crowd-based data collection tools, a dedicated integration protocol of the MindEarth App and the GREENGAGE App is implemented. The implementation of a dedicated protocol ensures availability and visibility of MindEarth\u2019s missions across both apps, with seamless redirection for mission execution. Please refer to the relevant documentation available at this link .","title":"Integrating MindView with the Greengage App"},{"location":"tools/mindview/integration/#testing","text":"You can find here some sample output data We can give you early access to the APK and create sample missions around your location if you want to test the app and there are no active missions in your vicinity.","title":"Testing"},{"location":"tools/mindview/integration/#support-and-contact","text":"If you have any questions or require technical support, do not hesitate to contact our support team at support@mindearth.ch .","title":"Support and Contact"},{"location":"tools/mindview/examples/","text":"MindView <> Greengage App Integration To streamline user interactions and mission management between different crowd-based data collection tools, a dedicated integration protocol of the MindEarth App and the GREENGAGE App is implemented. The implementation of a dedicated protocol ensures availability and visibility of MindEarth\u2019s missions across both apps, with seamless redirection for mission execution. This means that any mission available in the MindEarth App will also be visible in the GREENGAGE App and that users of the GREENGAGE App will be able to select a MindEarth App mission and be redirected to the MindEarth app to complete the booking and mission execution and upload process, as well as to know, in real time the status, duration, reward and deadline of MindEarth App\u2019s missions. This involves using two main systems based on webhooks and deeplinks. Webhook A webhook functionality for actions like booking, canceling, and completing missions. These webhook interactions use the base path https://mindearth.greengage.dev/ (example url). Mission Add Interaction: Action: Adding new Mission on MindEarth App URL: https://mindearth.greengage.dev/mission/add Method: POST Header: Bearer payload: Mission Booking Interaction: Action: Booking a Mission on MindEarth App URL: https://mindearth.greengage.dev/mission/ /book Method: PUT Header: Bearer Mission Cancellation Interaction: Action: Canceling a Mission Reservation on MindEarth App URL: https://mindearth.greengage.dev/mission/ /book/cancel Method: PUT Header: Bearer Mission Completion Interaction: Action: Completing a Mission on MindEarth App URL: https://mindearth.greengage.dev/mission/ /complete Method: PUT Header: Bearer Back-Office Integration: Action: Adding New Missions URL: https://mindearth.greengage.dev/mission Method: POST Header: Bearer Body: Details provided in the relevant JSON structure documentation. Deep links Deeplinking is planned to facilitate user transition between GREENGAGE and MindEarth apps. Deep links in each mission would enable seamless transition from the GREENGAGE App and engagement with the MindEarth App. A query URL will provide a unique webhook link for each mission, eliminating the need for an Auth-Header due to the signed nature of the request. Example of Deep Link: mindearth://mission/23?callback=https%3A%2F%2Fmindearth.greengage.dev%2Fmission%2F23%3Fsignature%3Df834ed8570e05de6c50ad10bd6abcf71e9867fcb14bdf2670b4bf572ce346f3b Testing You can find here some [sample output data] (assets/sample_data) We can give you [early access to the APK] () and create sample missions around your location if you want to test the app and there are no active missions in your vicinity Support and Contact If you have any questions or require technical support, do not hesitate to contact our support team at [support@mindearth.ch] (support@mindearth.ch).","title":"MindView <> Greengage App Integration"},{"location":"tools/mindview/examples/#mindview-greengage-app-integration","text":"To streamline user interactions and mission management between different crowd-based data collection tools, a dedicated integration protocol of the MindEarth App and the GREENGAGE App is implemented. The implementation of a dedicated protocol ensures availability and visibility of MindEarth\u2019s missions across both apps, with seamless redirection for mission execution. This means that any mission available in the MindEarth App will also be visible in the GREENGAGE App and that users of the GREENGAGE App will be able to select a MindEarth App mission and be redirected to the MindEarth app to complete the booking and mission execution and upload process, as well as to know, in real time the status, duration, reward and deadline of MindEarth App\u2019s missions. This involves using two main systems based on webhooks and deeplinks.","title":"MindView &lt;&gt; Greengage App Integration"},{"location":"tools/mindview/examples/#webhook","text":"A webhook functionality for actions like booking, canceling, and completing missions. These webhook interactions use the base path https://mindearth.greengage.dev/ (example url). Mission Add Interaction: Action: Adding new Mission on MindEarth App URL: https://mindearth.greengage.dev/mission/add Method: POST Header: Bearer payload: Mission Booking Interaction: Action: Booking a Mission on MindEarth App URL: https://mindearth.greengage.dev/mission/ /book Method: PUT Header: Bearer Mission Cancellation Interaction: Action: Canceling a Mission Reservation on MindEarth App URL: https://mindearth.greengage.dev/mission/ /book/cancel Method: PUT Header: Bearer Mission Completion Interaction: Action: Completing a Mission on MindEarth App URL: https://mindearth.greengage.dev/mission/ /complete Method: PUT Header: Bearer Back-Office Integration: Action: Adding New Missions URL: https://mindearth.greengage.dev/mission Method: POST Header: Bearer Body: Details provided in the relevant JSON structure documentation.","title":"Webhook"},{"location":"tools/mindview/examples/#deep-links","text":"Deeplinking is planned to facilitate user transition between GREENGAGE and MindEarth apps. Deep links in each mission would enable seamless transition from the GREENGAGE App and engagement with the MindEarth App. A query URL will provide a unique webhook link for each mission, eliminating the need for an Auth-Header due to the signed nature of the request. Example of Deep Link: mindearth://mission/23?callback=https%3A%2F%2Fmindearth.greengage.dev%2Fmission%2F23%3Fsignature%3Df834ed8570e05de6c50ad10bd6abcf71e9867fcb14bdf2670b4bf572ce346f3b","title":"Deep links"},{"location":"tools/mindview/examples/#testing","text":"You can find here some [sample output data] (assets/sample_data) We can give you [early access to the APK] () and create sample missions around your location if you want to test the app and there are no active missions in your vicinity","title":"Testing"},{"location":"tools/mindview/examples/#support-and-contact","text":"If you have any questions or require technical support, do not hesitate to contact our support team at [support@mindearth.ch] (support@mindearth.ch).","title":"Support and Contact"},{"location":"tools/mode/","text":"MODE Introduction MODE software technology uses smartphones to automatically track the distances travelled and transport modes used, thus enabling software developers, system integrators and transport operators to design innovative mobility services. MODE provides the missing link for the development of innovative mobility services: automatic, reliable recognition of the means of transport used, and travelled routes using smartphones. Features of MODE MODE is a trip recording and mode detection framework. It consists of two main software components: 1. A data recording module for iOS and Android Apps 2. A backend data analysis module The above figure shows a high-level overview and demonstrates how to embed the MODE components into a system (AIT components are highlighted in blue, other components must be provided by a partner). Client-side tracking and data recording functionality can be added to an existing App by including the libmode library. Once the library is configured and activated (see Frontend ), it tracks user activity and records relevant movement data. The library transmits completed trips to a backend server using REST calls. On the server side, the transmitted trip data is processed using the AIT JMDCA library. The JMDCA library is written in Java and provides core data structures, processing routines and analysis algorithms which allow application programmers to easily analyse trips and perform mode detection on the recorded data (JMDCA API, see section Backend ). To deliver high-quality results, the JMDCA mode detection algorithm requires access to a third-party public transit routing engine (such as HERE or Google routing). The result of using MODE are trips. These trips can include multiple segments (\"activities\") which make up a trip. E.g. if a user walks to his or her car, then drives to a transit station and then takes the train to reach the destination it would yield three activities, each with: Start and end times The travel mode (e.g. bike, car, public transport, walking, etc.) The probability of the \"correctness\" (e.g. it could be \"LOW; Reason: Bad GPS signal\") Use Case Scenario The main usage of MODE is to be integrated in smartphone apps to track user travel behavior. E.g. it was used in apps which incentivize \"green\" transport modes where you start the app and if it detects that you did the trip via bicycle or public transport you could earn \"tokens\" which could be spent on culture events.","title":"MODE"},{"location":"tools/mode/#mode","text":"","title":"MODE"},{"location":"tools/mode/#introduction","text":"MODE software technology uses smartphones to automatically track the distances travelled and transport modes used, thus enabling software developers, system integrators and transport operators to design innovative mobility services. MODE provides the missing link for the development of innovative mobility services: automatic, reliable recognition of the means of transport used, and travelled routes using smartphones.","title":"Introduction"},{"location":"tools/mode/#features-of-mode","text":"MODE is a trip recording and mode detection framework. It consists of two main software components: 1. A data recording module for iOS and Android Apps 2. A backend data analysis module The above figure shows a high-level overview and demonstrates how to embed the MODE components into a system (AIT components are highlighted in blue, other components must be provided by a partner). Client-side tracking and data recording functionality can be added to an existing App by including the libmode library. Once the library is configured and activated (see Frontend ), it tracks user activity and records relevant movement data. The library transmits completed trips to a backend server using REST calls. On the server side, the transmitted trip data is processed using the AIT JMDCA library. The JMDCA library is written in Java and provides core data structures, processing routines and analysis algorithms which allow application programmers to easily analyse trips and perform mode detection on the recorded data (JMDCA API, see section Backend ). To deliver high-quality results, the JMDCA mode detection algorithm requires access to a third-party public transit routing engine (such as HERE or Google routing). The result of using MODE are trips. These trips can include multiple segments (\"activities\") which make up a trip. E.g. if a user walks to his or her car, then drives to a transit station and then takes the train to reach the destination it would yield three activities, each with: Start and end times The travel mode (e.g. bike, car, public transport, walking, etc.) The probability of the \"correctness\" (e.g. it could be \"LOW; Reason: Bad GPS signal\")","title":"Features of MODE"},{"location":"tools/mode/#use-case-scenario","text":"The main usage of MODE is to be integrated in smartphone apps to track user travel behavior. E.g. it was used in apps which incentivize \"green\" transport modes where you start the app and if it detects that you did the trip via bicycle or public transport you could earn \"tokens\" which could be spent on culture events.","title":"Use Case Scenario"},{"location":"tools/mode/integration/","text":"MODE Integration Guide for Third-Party Services Since MODE is a distributed system with multiple components, it can (or must) be implemented at various levels, mostly in the frontend (Android, iOS apps) and the backend (server) which processes the trips. Frontend The MODE smartphone libraries ( libmode for Android and iOS ) must be integrated in a host app which takes care about user authentication, and starting/stopping the MODE service. While MODE can run continuously in the background, it can adversely affect battery consumption (especially on iOS devices). It should therefore only be started when the user agrees to track a trip. When MODE is running, it will detect when activities start or end (e.g. due to GPS signals, or when it detects, that the phone is not moving anymore). Once an activity end, the binary data of the trip is sent to a REST server in the background, which does the processing. Backend The backend is provided as a Docker image which opens the rest service on port 8080. Its sole purpose is to listen to the binary data from the frontend, process the data, and return JSON output of trips. If advanced logic is needed, like storing the returned trips in a database, this must be done by some kind of middleware which details are yet to be determined, depending on the concrete use cases of MODE within the project. Testing Testing the setup with real data can be cumbersome, as someone would need to walk/drive around to see the data pushed from the frontend to the backend. Therefore we provide some sample data which can be sent to the backend to be processed using a simple test script . Note that the backend server requires an API key which must be changed in the test script and is provided on demand . Output If the front end and backend works correctly, the backend will spit out the resulting trips as a JSON file : { \"message\": \"success\", \"activities\": [ { \"begin\": \"2020-07-15T13:20:41.059Z\", \"end\": \"2020-07-15T13:45:37.059Z\", \"mode\": \"WALK\", \"modeDetails\": \" | Sensor mode Walk | GPSQuality(AVERAGE : time gap)\", \"distanceMeters\": 1253.2470899042082, \"route\": \"ue`eHkddcB??????????rCrAj@Vj@Vh@Th@Vp@Xj@Xp@Xh@Tl@Vl@XJDp@Xn@Xp@Vp@XJDl@Tn@Rp@RRB@@D@?@?@???@???@@????????@???A?????????????????????????????????@????????????????????????Nh@??@E??@??@@B@B@B@@ACAAAC?AM]EICGCE?A???@?@BFBH@DP`A@F?D???B????A@A???EA??A??A????A@??????A????AA????CAI@??A@????A????????@?????B@B?D?B?F@D?B?H@H?F?D@B?@?@????C?CAC?CAGAGCG?CAA?AAA@??ABABCBABAB?@?@?BBBBD@D@F?DCDCBCBEBEBC@EBEBC@EBEBC@EBC@CBCBCBCBCBEBC@EBE@C@C@ABCBCBEBCBE@E@G@G@EBEBEBGBEBG@GBG@G?G?E?ECCEAEAEAECEECECEEECEECECCCCECCECCACACAEACAECECEEECEECCEEECECECECECEEECECECEAECEAECGCGCEEECE?C@CBE@C?CCECC?E?CBCFADCFCDABC@A@A?@A????A?A??@A???@A??@A??????@??A?@@?@??????A?A?AA???????????A?@???????@??@???@A??@?A???A@A?A??????@A??A??@A@???@???@?B????@??A?AA????A?????????A???@?????@?@?@?@?@A??@??????A???@@?A?A???????????A???A?A??????????@??@???A??@A???A@?A??@?@A@?@?@????@????A?????A?A?A?????A??????A????DA??????A??@????A???@??@?????@??A?????????@??????A?????CAAA?AAAA@???@A?????????@@@@?@????@@???????????????A?????????A???AA????????A?????A???@@@??@A?????A?@?????@??A?A@A@A@????@I@????B?@?A@??@?@ABCBA@A???\" }, { \"begin\": \"2020-07-15T13:45:38.059Z\", \"end\": \"2020-07-15T13:51:20.059Z\", \"mode\": \"BUS\", \"modeDetails\": \"Line 69A (Hauptbahnhof - Simmering) (BUS) from Am Kanal to Arsenalsteg | Sensor mode Bus routed to Bus | GPSQuality(GOOD)\", \"distanceMeters\": 2143.843162687046, \"route\": \"ch_eHk_dcB????yA`ByAxA??gBbB??}AlA??{BrA??cBp@uBn@??cBZ??s@`C????????????????????AnD??ElC??EjCGlC??EtB??EtBGtB??IxD??KpC????????????????????A`D??IdE????c@xC??_AlB??CzC??m@lC??_AbB????????????qA`C????wBxD??_BfC??wAzB????}A~B????????????????iAbB??gAdB??gB`C??q@~@s@~@s@~@KW\" }, { \"begin\": \"2020-07-15T13:51:29.059Z\", \"end\": \"2020-07-15T14:32:45.059Z\", \"mode\": \"WALK\", \"modeDetails\": \" | Sensor mode Walk | GPSQuality(BAD : large time gap)\", \"distanceMeters\": 1435.4269300101091, \"route\": \"mhaeHq{_cBE^@?AAACEAEAE?E?C?CACCCAC?E?CACCAG?CBEBABA@CBCBAB?@D?@?BCFA@C@CBAB?B@B@@@A@C@C@A@@@B?DABAB?B?@@@??@?BABA@?@?@B?D?D@D@F@D@D@F@F@D@FBD@F@FBH@FBHBFBFBFBHBF@HBF@DBF@FBDBDBDBFBD@FBD@D?B?@???????A??????????????@@?B@DBDBF@DBFBDBFBD@FBDBFBFBDBDBDBFBDBFBFBDBFBD@F@FBFBDB@DAD?DAB@D@B@@@A@CBC@CB?BDBFBF@FBD@BDBB@F@F@H?H@H@H@HBHBF@DBF@DBFBBDDDBDBDBDDBDBFBDDFBD@FBFBD@FBFBDBFBDDDDFDFBDDDBDBDBBBBBDBBBBD@B@DABABEBCBCBE@G@G@G@GBE@EBG@E?G@G?E@G?E@GBE@IBGBGDGBGBEBG@E@E@G@E@G@G@G@E@G@G?E@G?G@EBEBEBEBAD?D@FBDBBDDD@F@B@D?D@B?B@B@B??DJ?@@@?@B@LLDBFB@@@?@@@???@@@?@@@???@@@???@@@?@?@@??@@@?@?@@??@?@@@@DBB@NZ?@?@@@?@@??@?@?@@??@??@B??????????????AA???????A????EM??AC????AC?A???A????CG?AAC????????CI?AAA?A???????A??A????A??AE?????AA????A?????AAA?AAC?????A??????AAAE???A????M_@AACEAA??A??AAA??AAA?AA??MKAACCCACCECCCCACCEECCEAE?E@EDEDEFCDAHCHAHAHAH?FAF?DADCBADADAF?FAHAFAFAFAF?H?FAF?FAFCFCDCFCDCDCDEDCBEDCBCDAFCF?FAF@F?FAD?DADAFADCBADCDCDCDAFCDAFADAFCDCDCDCBADADADCDADCDADCFAH?HAF?HAF?F?H?F@FAD?FCFADAHAFAFAFAHAFADCFAF?D?F?D?D?FADADAFAFCFCHAFCFCHCDCFCDEBEDEBEDC@E?EACCEECACCCACCCCCCACCCCCCECCACCCACACA@A@A@?@?@?@?@?????????A??A?A?A?A?@@@?????????@?????@A@ABU??EH?@A@?@ABEJ?@ABA@CHW|@K^ADAH?@?????C?????A???@?????A?E?A?C?????J?@????A?@????????@???A????????????\" } ] } The format uses the encoded polyline algorithm for encoding the polylines of the trips ( sample code ). We will, however, for the project also try to output GeoJSON files directly, which might be easier to consume. Support and Contact For any questions, contact Martin Stubenschrott directly at martin.stubenschrott@ait.ac.at Important Notes You might get an API key for third-party routing services (e.g. Google Router). Make sure to keep that confident and only use it for testing and sparsingly. Overuse of the API key may result in high fees, as only about 20.000 requests/per month are free! For production, we must use a different key being used for billing!","title":"MODE Integration Guide for Third-Party Services"},{"location":"tools/mode/integration/#mode-integration-guide-for-third-party-services","text":"Since MODE is a distributed system with multiple components, it can (or must) be implemented at various levels, mostly in the frontend (Android, iOS apps) and the backend (server) which processes the trips.","title":"MODE Integration Guide for Third-Party Services"},{"location":"tools/mode/integration/#frontend","text":"The MODE smartphone libraries ( libmode for Android and iOS ) must be integrated in a host app which takes care about user authentication, and starting/stopping the MODE service. While MODE can run continuously in the background, it can adversely affect battery consumption (especially on iOS devices). It should therefore only be started when the user agrees to track a trip. When MODE is running, it will detect when activities start or end (e.g. due to GPS signals, or when it detects, that the phone is not moving anymore). Once an activity end, the binary data of the trip is sent to a REST server in the background, which does the processing.","title":"Frontend"},{"location":"tools/mode/integration/#backend","text":"The backend is provided as a Docker image which opens the rest service on port 8080. Its sole purpose is to listen to the binary data from the frontend, process the data, and return JSON output of trips. If advanced logic is needed, like storing the returned trips in a database, this must be done by some kind of middleware which details are yet to be determined, depending on the concrete use cases of MODE within the project.","title":"Backend"},{"location":"tools/mode/integration/#testing","text":"Testing the setup with real data can be cumbersome, as someone would need to walk/drive around to see the data pushed from the frontend to the backend. Therefore we provide some sample data which can be sent to the backend to be processed using a simple test script . Note that the backend server requires an API key which must be changed in the test script and is provided on demand .","title":"Testing"},{"location":"tools/mode/integration/#output","text":"If the front end and backend works correctly, the backend will spit out the resulting trips as a JSON file : { \"message\": \"success\", \"activities\": [ { \"begin\": \"2020-07-15T13:20:41.059Z\", \"end\": \"2020-07-15T13:45:37.059Z\", \"mode\": \"WALK\", \"modeDetails\": \" | Sensor mode Walk | GPSQuality(AVERAGE : time gap)\", \"distanceMeters\": 1253.2470899042082, \"route\": \"ue`eHkddcB??????????rCrAj@Vj@Vh@Th@Vp@Xj@Xp@Xh@Tl@Vl@XJDp@Xn@Xp@Vp@XJDl@Tn@Rp@RRB@@D@?@?@???@???@@????????@???A?????????????????????????????????@????????????????????????Nh@??@E??@??@@B@B@B@@ACAAAC?AM]EICGCE?A???@?@BFBH@DP`A@F?D???B????A@A???EA??A??A????A@??????A????AA????CAI@??A@????A????????@?????B@B?D?B?F@D?B?H@H?F?D@B?@?@????C?CAC?CAGAGCG?CAA?AAA@??ABABCBABAB?@?@?BBBBD@D@F?DCDCBCBEBEBC@EBEBC@EBEBC@EBC@CBCBCBCBCBEBC@EBE@C@C@ABCBCBEBCBE@E@G@G@EBEBEBGBEBG@GBG@G?G?E?ECCEAEAEAECEECECEEECEECECCCCECCECCACACAEACAECECEEECEECCEEECECECECECEEECECECEAECEAECGCGCEEECE?C@CBE@C?CCECC?E?CBCFADCFCDABC@A@A?@A????A?A??@A???@A??@A??????@??A?@@?@??????A?A?AA???????????A?@???????@??@???@A??@?A???A@A?A??????@A??A??@A@???@???@?B????@??A?AA????A?????????A???@?????@?@?@?@?@A??@??????A???@@?A?A???????????A???A?A??????????@??@???A??@A???A@?A??@?@A@?@?@????@????A?????A?A?A?????A??????A????DA??????A??@????A???@??@?????@??A?????????@??????A?????CAAA?AAAA@???@A?????????@@@@?@????@@???????????????A?????????A???AA????????A?????A???@@@??@A?????A?@?????@??A?A@A@A@????@I@????B?@?A@??@?@ABCBA@A???\" }, { \"begin\": \"2020-07-15T13:45:38.059Z\", \"end\": \"2020-07-15T13:51:20.059Z\", \"mode\": \"BUS\", \"modeDetails\": \"Line 69A (Hauptbahnhof - Simmering) (BUS) from Am Kanal to Arsenalsteg | Sensor mode Bus routed to Bus | GPSQuality(GOOD)\", \"distanceMeters\": 2143.843162687046, \"route\": \"ch_eHk_dcB????yA`ByAxA??gBbB??}AlA??{BrA??cBp@uBn@??cBZ??s@`C????????????????????AnD??ElC??EjCGlC??EtB??EtBGtB??IxD??KpC????????????????????A`D??IdE????c@xC??_AlB??CzC??m@lC??_AbB????????????qA`C????wBxD??_BfC??wAzB????}A~B????????????????iAbB??gAdB??gB`C??q@~@s@~@s@~@KW\" }, { \"begin\": \"2020-07-15T13:51:29.059Z\", \"end\": \"2020-07-15T14:32:45.059Z\", \"mode\": \"WALK\", \"modeDetails\": \" | Sensor mode Walk | GPSQuality(BAD : large time gap)\", \"distanceMeters\": 1435.4269300101091, \"route\": \"mhaeHq{_cBE^@?AAACEAEAE?E?C?CACCCAC?E?CACCAG?CBEBABA@CBCBAB?@D?@?BCFA@C@CBAB?B@B@@@A@C@C@A@@@B?DABAB?B?@@@??@?BABA@?@?@B?D?D@D@F@D@D@F@F@D@FBD@F@FBH@FBHBFBFBFBHBF@HBF@DBF@FBDBDBDBFBD@FBD@D?B?@???????A??????????????@@?B@DBDBF@DBFBDBFBD@FBDBFBFBDBDBDBFBDBFBFBDBFBD@F@FBFBDB@DAD?DAB@D@B@@@A@CBC@CB?BDBFBF@FBD@BDBB@F@F@H?H@H@H@HBHBF@DBF@DBFBBDDDBDBDBDDBDBFBDDFBD@FBFBD@FBFBDBFBDDDDFDFBDDDBDBDBBBBBDBBBBD@B@DABABEBCBCBE@G@G@G@GBE@EBG@E?G@G?E@G?E@GBE@IBGBGDGBGBEBG@E@E@G@E@G@G@G@E@G@G?E@G?G@EBEBEBEBAD?D@FBDBBDDD@F@B@D?D@B?B@B@B??DJ?@@@?@B@LLDBFB@@@?@@@???@@@?@@@???@@@???@@@?@?@@??@@@?@?@@??@?@@@@DBB@NZ?@?@@@?@@??@?@?@@??@??@B??????????????AA???????A????EM??AC????AC?A???A????CG?AAC????????CI?AAA?A???????A??A????A??AE?????AA????A?????AAA?AAC?????A??????AAAE???A????M_@AACEAA??A??AAA??AAA?AA??MKAACCCACCECCCCACCEECCEAE?E@EDEDEFCDAHCHAHAHAH?FAF?DADCBADADAF?FAHAFAFAFAF?H?FAF?FAFCFCDCFCDCDCDEDCBEDCBCDAFCF?FAF@F?FAD?DADAFADCBADCDCDCDAFCDAFADAFCDCDCDCBADADADCDADCDADCFAH?HAF?HAF?F?H?F@FAD?FCFADAHAFAFAFAHAFADCFAF?D?F?D?D?FADADAFAFCFCHAFCFCHCDCFCDEBEDEBEDC@E?EACCEECACCCACCCCCCACCCCCCECCACCCACACA@A@A@?@?@?@?@?????????A??A?A?A?A?@@@?????????@?????@A@ABU??EH?@A@?@ABEJ?@ABA@CHW|@K^ADAH?@?????C?????A???@?????A?E?A?C?????J?@????A?@????????@???A????????????\" } ] } The format uses the encoded polyline algorithm for encoding the polylines of the trips ( sample code ). We will, however, for the project also try to output GeoJSON files directly, which might be easier to consume.","title":"Output"},{"location":"tools/mode/integration/#support-and-contact","text":"For any questions, contact Martin Stubenschrott directly at martin.stubenschrott@ait.ac.at","title":"Support and Contact"},{"location":"tools/mode/integration/#important-notes","text":"You might get an API key for third-party routing services (e.g. Google Router). Make sure to keep that confident and only use it for testing and sparsingly. Overuse of the API key may result in high fees, as only about 20.000 requests/per month are free! For production, we must use a different key being used for billing!","title":"Important Notes"},{"location":"tools/mode/examples/android/","text":"MODE Integration Guide for data collection on Android devices The MODE smartphone library ( libmode ) provides a complete, out-of-the-box solution for mobility data collection via smartphones. It is built as an autonomous component that can easily be integrated into third party apps. For the standard use-case (trip data collection) the library can be integrated into existing Apps with little effort: The most recent version of the MODE library will be provided by AIT upon request . You will also be granted Git access to https://gitlab.ait.ac.at/energy/commons/smartsurvey/android/smartsurvey_demo_app/ which hosts a very basic demo app, showing how to use the MODE library. Step 1: Add the library to the App as a build dependency The library can be added to the App by adding a corresponding dependency to the build. The following example shows how to add libmode to the build.gradle of an Android App. We will provide the library with enough meta-data according to the Maven standard. When putting the library into your ~/.m2/repository/ folder, you can add the dependency like this: dependencies { implementation 'at.ac.ait.mode:libmode:2.0.6' } Of course, you can also just copy the libmode.jar file to a libs folder in your project, and add the dependency like this: dependencies { implementation fileTree(dir: 'libs', include: ['*.jar']) } Step 2: Initialize the library and start data collection The library needs to be initialized before it can start recording data: private void initLibMode() { Context ctx = getApplicationContext(); Mode.initialize(ctx); // Optional: allow data uploading via cellular network (otherwise data only gets uploaded when on WiFi) Mode.getConfigurationEntry(Configuration.CONFIG_METERED_UPLOADING_ALLOWED).setBooleanValue(ctx, true); // set Backend Server Mode.setBaseServerURL(\"http://localhost:8080\"); // Set OAUTH2 token manually // This will be sent to the backend service on each request Mode.setAuthorizationToken(\"MY_OAUTH2_TOKEN\"); // Start Mode.startSurvey(); } Note that we need to ask first for permission to access the location. ( ANDROID_PERMISSION_ACCESS_FINE_LOCATION ). Only when this is granted, we should initialize the library as in the example above. The provided demo app shows how to handle this. Support and Contact For any questions, contact Martin Stubenschrott directly at martin.stubenschrott@ait.ac.at","title":"MODE Integration Guide for data collection on Android devices"},{"location":"tools/mode/examples/android/#mode-integration-guide-for-data-collection-on-android-devices","text":"The MODE smartphone library ( libmode ) provides a complete, out-of-the-box solution for mobility data collection via smartphones. It is built as an autonomous component that can easily be integrated into third party apps. For the standard use-case (trip data collection) the library can be integrated into existing Apps with little effort: The most recent version of the MODE library will be provided by AIT upon request . You will also be granted Git access to https://gitlab.ait.ac.at/energy/commons/smartsurvey/android/smartsurvey_demo_app/ which hosts a very basic demo app, showing how to use the MODE library. Step 1: Add the library to the App as a build dependency The library can be added to the App by adding a corresponding dependency to the build. The following example shows how to add libmode to the build.gradle of an Android App. We will provide the library with enough meta-data according to the Maven standard. When putting the library into your ~/.m2/repository/ folder, you can add the dependency like this: dependencies { implementation 'at.ac.ait.mode:libmode:2.0.6' } Of course, you can also just copy the libmode.jar file to a libs folder in your project, and add the dependency like this: dependencies { implementation fileTree(dir: 'libs', include: ['*.jar']) } Step 2: Initialize the library and start data collection The library needs to be initialized before it can start recording data: private void initLibMode() { Context ctx = getApplicationContext(); Mode.initialize(ctx); // Optional: allow data uploading via cellular network (otherwise data only gets uploaded when on WiFi) Mode.getConfigurationEntry(Configuration.CONFIG_METERED_UPLOADING_ALLOWED).setBooleanValue(ctx, true); // set Backend Server Mode.setBaseServerURL(\"http://localhost:8080\"); // Set OAUTH2 token manually // This will be sent to the backend service on each request Mode.setAuthorizationToken(\"MY_OAUTH2_TOKEN\"); // Start Mode.startSurvey(); } Note that we need to ask first for permission to access the location. ( ANDROID_PERMISSION_ACCESS_FINE_LOCATION ). Only when this is granted, we should initialize the library as in the example above. The provided demo app shows how to handle this.","title":"MODE Integration Guide for data collection on Android devices"},{"location":"tools/mode/examples/android/#support-and-contact","text":"For any questions, contact Martin Stubenschrott directly at martin.stubenschrott@ait.ac.at","title":"Support and Contact"},{"location":"tools/mode/examples/backend/","text":"MODE Integration Guide for backend data analysis We provide a basic REST entpoint as a Docker image which listens on an endpoint /analyze . The input is binary movement data gathered from the smartphone apps (see here and here ). Output is a JSON string of the taken activities. Using the image Step 1: Copy exported image to server Use your tool of choice. Step 2: Import image on server docker load -i modesrv-1.0.0.tar.gz Step 3: Create Docker container # docker stop modesrv # docker rm modesrv docker create --name modesrv -p 8080:8080 --restart=unless-stopped ait/modesrv:1.0.0 Step 4: Run Docker image and follow its logs docker start modesrv docker logs -f modesrv Step 5: Test the image Follow these instructions to send example data to localhost:8080 and see some JSON output. Important Notes The MODE backend service uses third-party routing services (e.g. Google Router). At this stage, a sample API key is hardcoded in the project. Make sure to keep that key confident and only use it for testing and sparsingly. Overuse of the API key may result in high fees, as only about 20.000 requests/per month are free! For production, we must use a different key being used for billing. This will be set via an ENV variable to the Docker image.","title":"MODE Integration Guide for backend data analysis"},{"location":"tools/mode/examples/backend/#mode-integration-guide-for-backend-data-analysis","text":"We provide a basic REST entpoint as a Docker image which listens on an endpoint /analyze . The input is binary movement data gathered from the smartphone apps (see here and here ). Output is a JSON string of the taken activities.","title":"MODE Integration Guide for backend data analysis"},{"location":"tools/mode/examples/backend/#using-the-image","text":"Step 1: Copy exported image to server Use your tool of choice. Step 2: Import image on server docker load -i modesrv-1.0.0.tar.gz Step 3: Create Docker container # docker stop modesrv # docker rm modesrv docker create --name modesrv -p 8080:8080 --restart=unless-stopped ait/modesrv:1.0.0 Step 4: Run Docker image and follow its logs docker start modesrv docker logs -f modesrv Step 5: Test the image Follow these instructions to send example data to localhost:8080 and see some JSON output.","title":"Using the image"},{"location":"tools/mode/examples/backend/#important-notes","text":"The MODE backend service uses third-party routing services (e.g. Google Router). At this stage, a sample API key is hardcoded in the project. Make sure to keep that key confident and only use it for testing and sparsingly. Overuse of the API key may result in high fees, as only about 20.000 requests/per month are free! For production, we must use a different key being used for billing. This will be set via an ENV variable to the Docker image.","title":"Important Notes"},{"location":"tools/mode/examples/ios/","text":"MODE Integration Guide for data collection on iOS devices The MODE smartphone library ( libmode ) provides a complete, out-of-the-box solution for mobility data collection via smartphones. It is built as an autonomous component that can easily be integrated into third party apps. For the standard use-case (trip data collection) the library can be integrated into existing Apps with little effort: The most recent version of the MODE library will be provided by AIT upon request . You will also be granted Git access to https://gitlab.ait.ac.at/energy/commons/smartsurvey/ios/SmartSurveyDemoApp/ which hosts a very basic demo app, showing how to use the MODE library. Step 1: Add the library to the App as a build dependency To make it easy to use libmode, we have published the library on our Gitlab in https://gitlab.ait.ac.at/energy/commons/smartsurvey/ios/Libmode.git You can then add the swift package as a dependency in XCode. Step 2: Initialize the library and start data collection The library needs to be initialized before it can start recording data: func initLibMode() { // Allow uploading of trips also on cellular connection (default is Wi-Fi only) UserDefaultsStore.cellularUploadAllowed = true // Set the OAUTH2 token and the server address where trips are uploaded and analyzed. Mode.setAuthorizationToken(token: \"myuser:abcdef123456\") // Define where the backend server is located, waiting for the trips to be // uploaded // // By default, access to non-https is disabled by iOS. For development purposes one can change // Info.plist like described here, but that might not allow the app to be // accepted in the app store, so only use during development or install a local // SSL certificate: // https://stackoverflow.com/questions/63597760/not-allowed-to-make-request-to-local-host-swift Mode.setBaseServerURL(surveyUrl: \"http://localhost:8080\") // This call starts recording trip data. Once it has detected a trip, it is uploaded and analyzed Mode.startSurvey() } Also make sure that you request background location access in the manifest of the project. Support and Contact For any questions, contact Martin Stubenschrott directly at martin.stubenschrott@ait.ac.at","title":"MODE Integration Guide for data collection on iOS devices"},{"location":"tools/mode/examples/ios/#mode-integration-guide-for-data-collection-on-ios-devices","text":"The MODE smartphone library ( libmode ) provides a complete, out-of-the-box solution for mobility data collection via smartphones. It is built as an autonomous component that can easily be integrated into third party apps. For the standard use-case (trip data collection) the library can be integrated into existing Apps with little effort: The most recent version of the MODE library will be provided by AIT upon request . You will also be granted Git access to https://gitlab.ait.ac.at/energy/commons/smartsurvey/ios/SmartSurveyDemoApp/ which hosts a very basic demo app, showing how to use the MODE library. Step 1: Add the library to the App as a build dependency To make it easy to use libmode, we have published the library on our Gitlab in https://gitlab.ait.ac.at/energy/commons/smartsurvey/ios/Libmode.git You can then add the swift package as a dependency in XCode. Step 2: Initialize the library and start data collection The library needs to be initialized before it can start recording data: func initLibMode() { // Allow uploading of trips also on cellular connection (default is Wi-Fi only) UserDefaultsStore.cellularUploadAllowed = true // Set the OAUTH2 token and the server address where trips are uploaded and analyzed. Mode.setAuthorizationToken(token: \"myuser:abcdef123456\") // Define where the backend server is located, waiting for the trips to be // uploaded // // By default, access to non-https is disabled by iOS. For development purposes one can change // Info.plist like described here, but that might not allow the app to be // accepted in the app store, so only use during development or install a local // SSL certificate: // https://stackoverflow.com/questions/63597760/not-allowed-to-make-request-to-local-host-swift Mode.setBaseServerURL(surveyUrl: \"http://localhost:8080\") // This call starts recording trip data. Once it has detected a trip, it is uploaded and analyzed Mode.startSurvey() } Also make sure that you request background location access in the manifest of the project.","title":"MODE Integration Guide for data collection on iOS devices"},{"location":"tools/mode/examples/ios/#support-and-contact","text":"For any questions, contact Martin Stubenschrott directly at martin.stubenschrott@ait.ac.at","title":"Support and Contact"},{"location":"tools/nifi/","text":"Apache NiFi Introduction Apache NiFi is a robust, open-source software solution designed for efficient and reliable data ingestion, processing, and distribution. It is particularly noted for its user-friendly graphical interface that simplifies the design, management, and monitoring of data flows. NiFi supports highly configurable and flexible data routing, system mediation, and transformation capabilities, enabling the handling of diverse data formats and systems. Key features include its scalability to accommodate large volumes of data, built-in data security measures, and extensive audit trails for data provenance. Primarily used in scenarios requiring complex data pipelines, such as real-time data processing and integration across varied systems, Apache NiFi stands out for its ease of use and comprehensive data management capabilities. Features of Apache NiFi Data Provenance: It tracks data provenance, recording a detailed history of data flowing through the system, which is key for auditing and understanding the lifecycle of the data. Flow Management: NiFi enables the management of data flows in real-time, allowing users to route, transform, and process data as it moves through the system. Extensible Architecture: NiFi can be extended with custom processors, services, and other components, making it adaptable to various data processing needs. Scalability: Designed to scale out across a cluster of servers to accommodate large volumes of data and high throughput requirements. Flexible Scheduling: Users can schedule when and how often data processors run, providing control over resource utilisation and timing of data flows. Back Pressure and Prioritisation: NiFi maintains system stability by applying back pressure and data prioritisation, ensuring smooth handling of data under load. Visual Command and Control: The visual command and control feature allows users to see exactly how data is flowing through their system in real-time. Data Buffering: NiFi buffers data between each processing step, ensuring that data is not lost in transit and can handle varying processing loads efficiently. Record-Oriented Data Handling: It supports record-oriented data handling, allowing for fine-grained data processing, which is particularly useful for complex and structured data types. Integration with Diverse Systems: NiFi integrates with a wide range of data sources and sinks, enabling interaction with numerous types of data systems and formats. Use Case Scenario Context: In an urban citizen observatory, members of the community actively participate in monitoring and analysing environmental conditions. One of the critical concerns in many cities is air quality, which directly impacts public health and quality of life. Objective: To monitor, analyse, and visualise air quality data across different parts of the city to identify pollution hotspots, understand temporal trends, and engage the public in environmental awareness and decision-making processes. Implementation Using Apache Druid: 1) Data Collection: - Deploy IoT sensors across the city to measure air quality indicators like PM2.5, PM10, NO2, CO2, and Ozone. 2) Data Ingestion and Storage with Apache NiFi: - If the data cant be collected directly by Apache Druid we should use Apache NiFi to ingest the data from the IoT sensors. - Configure NiFi to gather, preprocess and clean the data, ensuring it's in the correct format for analysis and storage. - Set up NiFi to handle the real-time flow of data, managing any necessary transformations or routing. 3) Data Storage and Real-time aggregation: - Stream the processed sensor data from Apache NiFi to Apache Druid for real-time ingestion and storage. - Configure Druid to handle the high volume of time-series data from the sensors, optimizing for fast access and query performance. Next steps in the process (Data integation, visualisation and data-driven decision making) are not handled by Apache NiFi, thus, we will not cover them in this guide. References Apache NiFi documentation GREENGAGE catalogue entry","title":"Apache NiFi"},{"location":"tools/nifi/#apache-nifi","text":"","title":"Apache NiFi"},{"location":"tools/nifi/#introduction","text":"Apache NiFi is a robust, open-source software solution designed for efficient and reliable data ingestion, processing, and distribution. It is particularly noted for its user-friendly graphical interface that simplifies the design, management, and monitoring of data flows. NiFi supports highly configurable and flexible data routing, system mediation, and transformation capabilities, enabling the handling of diverse data formats and systems. Key features include its scalability to accommodate large volumes of data, built-in data security measures, and extensive audit trails for data provenance. Primarily used in scenarios requiring complex data pipelines, such as real-time data processing and integration across varied systems, Apache NiFi stands out for its ease of use and comprehensive data management capabilities.","title":"Introduction"},{"location":"tools/nifi/#features-of-apache-nifi","text":"Data Provenance: It tracks data provenance, recording a detailed history of data flowing through the system, which is key for auditing and understanding the lifecycle of the data. Flow Management: NiFi enables the management of data flows in real-time, allowing users to route, transform, and process data as it moves through the system. Extensible Architecture: NiFi can be extended with custom processors, services, and other components, making it adaptable to various data processing needs. Scalability: Designed to scale out across a cluster of servers to accommodate large volumes of data and high throughput requirements. Flexible Scheduling: Users can schedule when and how often data processors run, providing control over resource utilisation and timing of data flows. Back Pressure and Prioritisation: NiFi maintains system stability by applying back pressure and data prioritisation, ensuring smooth handling of data under load. Visual Command and Control: The visual command and control feature allows users to see exactly how data is flowing through their system in real-time. Data Buffering: NiFi buffers data between each processing step, ensuring that data is not lost in transit and can handle varying processing loads efficiently. Record-Oriented Data Handling: It supports record-oriented data handling, allowing for fine-grained data processing, which is particularly useful for complex and structured data types. Integration with Diverse Systems: NiFi integrates with a wide range of data sources and sinks, enabling interaction with numerous types of data systems and formats.","title":"Features of Apache NiFi"},{"location":"tools/nifi/#use-case-scenario","text":"","title":"Use Case Scenario"},{"location":"tools/nifi/#context","text":"In an urban citizen observatory, members of the community actively participate in monitoring and analysing environmental conditions. One of the critical concerns in many cities is air quality, which directly impacts public health and quality of life.","title":"Context:"},{"location":"tools/nifi/#objective","text":"To monitor, analyse, and visualise air quality data across different parts of the city to identify pollution hotspots, understand temporal trends, and engage the public in environmental awareness and decision-making processes.","title":"Objective:"},{"location":"tools/nifi/#implementation-using-apache-druid","text":"1) Data Collection: - Deploy IoT sensors across the city to measure air quality indicators like PM2.5, PM10, NO2, CO2, and Ozone. 2) Data Ingestion and Storage with Apache NiFi: - If the data cant be collected directly by Apache Druid we should use Apache NiFi to ingest the data from the IoT sensors. - Configure NiFi to gather, preprocess and clean the data, ensuring it's in the correct format for analysis and storage. - Set up NiFi to handle the real-time flow of data, managing any necessary transformations or routing. 3) Data Storage and Real-time aggregation: - Stream the processed sensor data from Apache NiFi to Apache Druid for real-time ingestion and storage. - Configure Druid to handle the high volume of time-series data from the sensors, optimizing for fast access and query performance. Next steps in the process (Data integation, visualisation and data-driven decision making) are not handled by Apache NiFi, thus, we will not cover them in this guide.","title":"Implementation Using Apache Druid:"},{"location":"tools/nifi/#references","text":"Apache NiFi documentation GREENGAGE catalogue entry","title":"References"},{"location":"tools/nifi/usage/","text":"Apache NiFi Usage Guide These instructions will guide you through the basic process of setting up a simple data flow. Step 1: Log In - Access to the NiFi deploy of Greengage. - First of all, you should login using your Greengage login in the keycloak authentication webpage that will appear. - You will reach to the main view of the tool as shown in the figure below. Step 2: Creating a New Process Group: Drag and drop a Process Group onto the canvas by selecting the fourth icon from the top menu. Process groups are containers for data flows. They allow you to group together related processors and other components. Give your process group a meaningful name, this helps in organizing different data flows. When you create the group it will show in the canvas like this. Now you can double click on it to access it and start adding new processors. Step 3: Adding Processors to Your Flow Drag and drop processors from the components toolbar onto the canvas within your process group. Use processors like 'GetFile' to read data from a file system or 'ListenHTTP' for receiving data over HTTP. Double-click a processor to configure it. Set properties like file path for 'GetFile' or listening port for 'ListenHTTP'. Apply and save the configuration. Step 4: Connecting Processors Draw connections between processors by dragging the arrow from one processor to another. This defines the flow of data between processors. Step 5: Setting Up Processor Scheduling Configure how often each processor runs (e.g., every 5 seconds, on an event basis). This is crucial for managing the flow and processing of data. Step 6: Starting and Stopping Processors Start individual processors or the entire process group to initiate data flow. Stop processors to make configuration changes or for maintenance. Step 7: Securing Your Data Flow If dealing with sensitive data, explore NiFi\u2019s security options, like configuring users, groups, and policies for access control. Remember, Apache NiFi has a steep learning curve, but its flexibility and power are unmatched for data flow management. Experiment with different processors and configurations to understand how they impact your specific use case.","title":"Apache NiFi Usage Guide"},{"location":"tools/nifi/usage/#apache-nifi-usage-guide","text":"These instructions will guide you through the basic process of setting up a simple data flow. Step 1: Log In - Access to the NiFi deploy of Greengage. - First of all, you should login using your Greengage login in the keycloak authentication webpage that will appear. - You will reach to the main view of the tool as shown in the figure below. Step 2: Creating a New Process Group: Drag and drop a Process Group onto the canvas by selecting the fourth icon from the top menu. Process groups are containers for data flows. They allow you to group together related processors and other components. Give your process group a meaningful name, this helps in organizing different data flows. When you create the group it will show in the canvas like this. Now you can double click on it to access it and start adding new processors. Step 3: Adding Processors to Your Flow Drag and drop processors from the components toolbar onto the canvas within your process group. Use processors like 'GetFile' to read data from a file system or 'ListenHTTP' for receiving data over HTTP. Double-click a processor to configure it. Set properties like file path for 'GetFile' or listening port for 'ListenHTTP'. Apply and save the configuration. Step 4: Connecting Processors Draw connections between processors by dragging the arrow from one processor to another. This defines the flow of data between processors. Step 5: Setting Up Processor Scheduling Configure how often each processor runs (e.g., every 5 seconds, on an event basis). This is crucial for managing the flow and processing of data. Step 6: Starting and Stopping Processors Start individual processors or the entire process group to initiate data flow. Stop processors to make configuration changes or for maintenance. Step 7: Securing Your Data Flow If dealing with sensitive data, explore NiFi\u2019s security options, like configuring users, groups, and policies for access control. Remember, Apache NiFi has a steep learning curve, but its flexibility and power are unmatched for data flow management. Experiment with different processors and configurations to understand how they impact your specific use case.","title":"Apache NiFi Usage Guide"},{"location":"tools/nifi/examples/","text":"Apache NiFi Apache NiFi it's not directly integrated into other software tools in the traditional sense of built-in connectors or plugins, NiFi excels in interfacing with a wide range of systems and services through its extensive library of processors. These processors are adept at 'getting' data from various sources and 'putting' processed data to numerous destinations. For example, NiFi can extract data from web services, databases, file systems, and even message queues, process this data according to defined rules and workflows, and then deliver the processed data to different systems, locations, or storage solutions. This flexibility in interacting with external systems is achieved not through direct integration, but by leveraging NiFi's ability to handle diverse data formats and communication protocols, making it a powerful intermediary in any data-centric architecture.","title":"Apache NiFi"},{"location":"tools/nifi/examples/#apache-nifi","text":"Apache NiFi it's not directly integrated into other software tools in the traditional sense of built-in connectors or plugins, NiFi excels in interfacing with a wide range of systems and services through its extensive library of processors. These processors are adept at 'getting' data from various sources and 'putting' processed data to numerous destinations. For example, NiFi can extract data from web services, databases, file systems, and even message queues, process this data according to defined rules and workflows, and then deliver the processed data to different systems, locations, or storage solutions. This flexibility in interacting with external systems is achieved not through direct integration, but by leveraging NiFi's ability to handle diverse data formats and communication protocols, making it a powerful intermediary in any data-centric architecture.","title":"Apache NiFi"},{"location":"tools/sensorsIntegration/","text":"MQTT Introduction MQTT is one of the most wide IoT protocols used both due to its scalable capabilities and the number of platforms for Smart Cities accepting this protocol. For that reason, MQTT has been introduced in the Smart Spot to be a protocol to report values to 3rd party platforms. Nowadays, it can be used only to report values, while managing it will be carried out using the original Smart Spot protocol, which is LwM2M. The management can be used both from the platform provided by Libelium so called Homard. This version of MQTT also includes significant security and reliability improvements compared to earlier versions. For example, authentication and authorization mechanisms were added to enable safer and more controlled communication between devices. Session management was also improved, and a \"last will and testament\" mechanism was added to ensure that devices disconnect properly in case of a network failure. Overall, MQTT v3.1.1 is considered a mature and stable version of the MQTT protocol and is widely used in a variety of IoT applications, such as environmental monitoring, vehicle telemetry, asset tracking, and more. Features of MQTT Supported Security MQTT v3.1.1 supports the two main security mechanisms to ensure secure communication between devices. These include: User and password-based authentication: MQTT devices can be authenticated using user and password credentials stored on the MQTT server. 1.2 Digital certificates: MQTT devices can be authenticated using digital certificates, which provides greater security and mutual authentication between devices. Both of them can be configured using the remote device management platform Homard provided by Libelium. MQTT Ports MQTT uses by default two different ports for communication: TCP port 1883: This is the default port used for unencrypted connection. MQTT clients connect to the MQTT server using this port. The communication between the client and the server is performed without encryption, meaning that the exchanged data is not protected. TCP port 8883: This is the port used for encrypted connection. MQTT clients can connect to the MQTT server using this port and establish a secure connection over TLS (Transport Layer Security). TLS provides an additional layer of security to MQTT communication, protecting the exchanged data between the client and the server It is important to note that default ports may vary depending on the MQTT provider or the MQTT server configuration. Therefore, the MQTT server documentation should be consulted to confirm the ports used in MQTT communication and ensure that clients are properly configured to connect to the server. Configuration The vertical data concept involves grouping and organising the data reported by the device into different verticals depending on the transmitted data set. The grouping of this data aligns with the definition of the official Data Models of the FIWARE platform described on the website https://smartdatamodels.org/. Given the versatility of the Smart Spot to be configurable at the hardware level during the order with different types of sensors, not all data verticals have to be active on the device. The device transmits data using the necessary verticals depending on the hardware configuration of the device. Additionally, it is possible to enable or disable specific verticals separately through the Homard device remote platform as indicated in the next section. The definition of a vertical at MQTT integration level is mainly composed of two elements: Topic: The topic for publishing each data set of the vertical will be composed based on a default prefix or a topic prefix specified by the client through the Homard device remote platform. The topic prefix must have the following format /{base_apikey}/{device_id} <br> where 'base_apikey' is usually an identifying key for a set of devices and 'device_id' is a specific name for the device. If a topic prefix is not configured, the device will use a default one. This topic prefix will be internally modified by the SmartSpot depending on the vertical. Specifically, the SmartSpot will add both the \u2018/attrs\u2019 postfix corresponding to the one required by the FIWARE IoT Agents to receive data and the 'lower_vertical_suffix' and 'upper_vertical_suffix' fields within the topic prefix to separate the data into different topics, one for each vertical, resulting in the final topic being composed in the following way: /{base_apikey}{lower_vertical_suffix}/{device_id}_{upper_vertical_suffix}/attrs <br> The following table illustrates an example of the prefix formation according to the publication vertical. Message: The data published through JSON-encoded text messages on each of the specified topics contain at least the data from the different sensors included in your Smart Spot device for each vertical, and may also include extra information regarding how the device is configured. This extra information is provided as it may be useful to the user and may enable device configuration via MQTT protocol in the future, although this feature is not currently implemented. The following table illustrate the information that can be included in each vertical depending on your device's configuration. The example message will be: ```{ \"TimeInstant\": \"2023-03-20T13:50:00Z\", \"period\": 5, \"status\": \"connected\", \"no2-a4\": 0.640869140625, \"ox-a431\": 28.586013793945312, \"co-a4\": 91.760452270507812, \"so2-a4\": 0.927800178527832, \"pm10\": 49.334125518798828, \"pm2\": 15.33404541015625, \"pm1\": 2.1444158554077148 } ``` It is important to note that if a sensor is not connected or not working properly, the field is still included in the transmitted message, but its value is set to 'null'. The reasons for carrying out this procedure are mainly to: Help identify device malfunctions for maintenance tasks. Prevent the introduction of '0' or repetitive non-real values that can cause confusion when data persists in databases or is represented in graphs.","title":"MQTT"},{"location":"tools/sensorsIntegration/#mqtt","text":"","title":"MQTT"},{"location":"tools/sensorsIntegration/#introduction","text":"MQTT is one of the most wide IoT protocols used both due to its scalable capabilities and the number of platforms for Smart Cities accepting this protocol. For that reason, MQTT has been introduced in the Smart Spot to be a protocol to report values to 3rd party platforms. Nowadays, it can be used only to report values, while managing it will be carried out using the original Smart Spot protocol, which is LwM2M. The management can be used both from the platform provided by Libelium so called Homard. This version of MQTT also includes significant security and reliability improvements compared to earlier versions. For example, authentication and authorization mechanisms were added to enable safer and more controlled communication between devices. Session management was also improved, and a \"last will and testament\" mechanism was added to ensure that devices disconnect properly in case of a network failure. Overall, MQTT v3.1.1 is considered a mature and stable version of the MQTT protocol and is widely used in a variety of IoT applications, such as environmental monitoring, vehicle telemetry, asset tracking, and more.","title":"Introduction"},{"location":"tools/sensorsIntegration/#features-of-mqtt","text":"Supported Security MQTT v3.1.1 supports the two main security mechanisms to ensure secure communication between devices. These include: User and password-based authentication: MQTT devices can be authenticated using user and password credentials stored on the MQTT server. 1.2 Digital certificates: MQTT devices can be authenticated using digital certificates, which provides greater security and mutual authentication between devices. Both of them can be configured using the remote device management platform Homard provided by Libelium. MQTT Ports MQTT uses by default two different ports for communication: TCP port 1883: This is the default port used for unencrypted connection. MQTT clients connect to the MQTT server using this port. The communication between the client and the server is performed without encryption, meaning that the exchanged data is not protected. TCP port 8883: This is the port used for encrypted connection. MQTT clients can connect to the MQTT server using this port and establish a secure connection over TLS (Transport Layer Security). TLS provides an additional layer of security to MQTT communication, protecting the exchanged data between the client and the server It is important to note that default ports may vary depending on the MQTT provider or the MQTT server configuration. Therefore, the MQTT server documentation should be consulted to confirm the ports used in MQTT communication and ensure that clients are properly configured to connect to the server. Configuration The vertical data concept involves grouping and organising the data reported by the device into different verticals depending on the transmitted data set. The grouping of this data aligns with the definition of the official Data Models of the FIWARE platform described on the website https://smartdatamodels.org/. Given the versatility of the Smart Spot to be configurable at the hardware level during the order with different types of sensors, not all data verticals have to be active on the device. The device transmits data using the necessary verticals depending on the hardware configuration of the device. Additionally, it is possible to enable or disable specific verticals separately through the Homard device remote platform as indicated in the next section. The definition of a vertical at MQTT integration level is mainly composed of two elements: Topic: The topic for publishing each data set of the vertical will be composed based on a default prefix or a topic prefix specified by the client through the Homard device remote platform. The topic prefix must have the following format /{base_apikey}/{device_id} <br> where 'base_apikey' is usually an identifying key for a set of devices and 'device_id' is a specific name for the device. If a topic prefix is not configured, the device will use a default one. This topic prefix will be internally modified by the SmartSpot depending on the vertical. Specifically, the SmartSpot will add both the \u2018/attrs\u2019 postfix corresponding to the one required by the FIWARE IoT Agents to receive data and the 'lower_vertical_suffix' and 'upper_vertical_suffix' fields within the topic prefix to separate the data into different topics, one for each vertical, resulting in the final topic being composed in the following way: /{base_apikey}{lower_vertical_suffix}/{device_id}_{upper_vertical_suffix}/attrs <br> The following table illustrates an example of the prefix formation according to the publication vertical. Message: The data published through JSON-encoded text messages on each of the specified topics contain at least the data from the different sensors included in your Smart Spot device for each vertical, and may also include extra information regarding how the device is configured. This extra information is provided as it may be useful to the user and may enable device configuration via MQTT protocol in the future, although this feature is not currently implemented. The following table illustrate the information that can be included in each vertical depending on your device's configuration. The example message will be: ```{ \"TimeInstant\": \"2023-03-20T13:50:00Z\", \"period\": 5, \"status\": \"connected\", \"no2-a4\": 0.640869140625, \"ox-a431\": 28.586013793945312, \"co-a4\": 91.760452270507812, \"so2-a4\": 0.927800178527832, \"pm10\": 49.334125518798828, \"pm2\": 15.33404541015625, \"pm1\": 2.1444158554077148 } ``` It is important to note that if a sensor is not connected or not working properly, the field is still included in the transmitted message, but its value is set to 'null'. The reasons for carrying out this procedure are mainly to: Help identify device malfunctions for maintenance tasks. Prevent the introduction of '0' or repetitive non-real values that can cause confusion when data persists in databases or is represented in graphs.","title":"Features of MQTT"},{"location":"tools/sensorsIntegration/integration/","text":"Libelium IoT sensors Usage & Integration Guide for Third-Party Services Generic usage instructions for SmartSpot and other kind of sensors Instructions on how to use the tool. The user's manual is available at this link The importance of using MQTT connectors in the project is highlighted by the need to establish efficient and standardized communication in an environment where various technologies converge. MQTT (Message Queuing Telemetry Transport) stands out as a lightweight and robust protocol that enables real-time data transmission between devices, ensuring seamless connectivity in heterogeneous environments. In the context of air quality monitoring, data standardization through MQTT facilitates the integration of devices from different manufacturers and technologies, promoting interoperability and simplifying the development of IoT architecture. The efficiency of MQTT is particularly evident in the effective management of messages, minimizing bandwidth and optimizing the transmission of critical information about sensors. Thus, the choice of MQTT connectors not only improves the coherence and speed of communication but also establishes a solid foundation for scalability and future system expansion, ensuring robustness and reliability in projects that require efficient real-time data management. Step 1: Requesting Access Credentials Instructions on how to request access credentials. To get started with your MQTT broker, please contact a.pujante@libelium.com Step 2: Receiving Credentials Once you have your MQTT connector, the next step is to integrate it into your platform or architecture in order to be able to receive the data on it Step 4: Testing the Integration To check the integration, it will be necessary to see if the data is arriving correctly to the platform and can be displayed correctly.. Support and Contact Contact for support, issues and information: a.pujante@libelium.com, e.illueca@libelium.com and aj.jara@libelium.com","title":"Libelium IoT sensors Usage & Integration Guide for Third-Party Services"},{"location":"tools/sensorsIntegration/integration/#libelium-iot-sensors-usage-integration-guide-for-third-party-services","text":"Generic usage instructions for SmartSpot and other kind of sensors Instructions on how to use the tool. The user's manual is available at this link The importance of using MQTT connectors in the project is highlighted by the need to establish efficient and standardized communication in an environment where various technologies converge. MQTT (Message Queuing Telemetry Transport) stands out as a lightweight and robust protocol that enables real-time data transmission between devices, ensuring seamless connectivity in heterogeneous environments. In the context of air quality monitoring, data standardization through MQTT facilitates the integration of devices from different manufacturers and technologies, promoting interoperability and simplifying the development of IoT architecture. The efficiency of MQTT is particularly evident in the effective management of messages, minimizing bandwidth and optimizing the transmission of critical information about sensors. Thus, the choice of MQTT connectors not only improves the coherence and speed of communication but also establishes a solid foundation for scalability and future system expansion, ensuring robustness and reliability in projects that require efficient real-time data management. Step 1: Requesting Access Credentials Instructions on how to request access credentials. To get started with your MQTT broker, please contact a.pujante@libelium.com Step 2: Receiving Credentials Once you have your MQTT connector, the next step is to integrate it into your platform or architecture in order to be able to receive the data on it Step 4: Testing the Integration To check the integration, it will be necessary to see if the data is arriving correctly to the platform and can be displayed correctly.. Support and Contact Contact for support, issues and information: a.pujante@libelium.com, e.illueca@libelium.com and aj.jara@libelium.com","title":"Libelium IoT sensors Usage &amp; Integration Guide for Third-Party Services"},{"location":"tools/superset/","text":"Apache Superset Introduction Apache Superset is a sophisticated, open-source data exploration and visualisation platform designed to transform the way analysts and businesses interact with their data. As a versatile web application, it empowers users with tools for deep data exploration and the creation of beautifully rendered visualisations and dashboards. Superset is renowned for its user-friendly interface, democratising data analysis by enabling users of all skill levels to engage with complex datasets, design compelling visual narratives, and derive actionable insights. Offering a rich array of features, including a variety of chart types, interactive dashboards, and an integrated SQL editor, Apache Superset stands out as a comprehensive solution for modern data analytics and business intelligence needs. Features of Superset Data Exploration: Superset allows users to explore datasets through simple, intuitive, and interactive visualisations. It offers a variety of chart types and the ability to dive deep into the details of the data. Easy Visualisation Creation: Users can easily create and share visualisations without the need for programming. It includes a rich set of visualisations like line charts, bar charts, and scatter plots, among others. Interactive Dashboards: Superset enables the creation of dynamic dashboards that are interactive and customisable. These dashboards can be shared with others and include various visualisations for comprehensive data analysis. SQL IDE: It comes with an integrated SQL IDE, which allows users to write, run, and visualise the results of SQL queries in a user-friendly environment. Security and Integration: Apache Superset offers robust security features, including role-based permission control. It can be integrated with most SQL-based data sources, including traditional databases and newer SQL-speaking data engines. Scalability and Performance: It is designed to be highly scalable, easily handling large-scale data exploration jobs. It also provides features for optimising query performance. Open Source and Extensible: Being open source allows for customisation and extension, enabling developers to contribute to its features or adapt it to specific needs. Community-Driven: Apache Superset is backed by a strong community, which contributes to its continuous development and improvement. Use Case Scenario Context: In an urban citizen observatory, members of the community actively participate in monitoring and analysing environmental conditions. One of the critical concerns in many cities is air quality, which directly impacts public health and quality of life. Objective: To monitor, analyse, and visualise air quality data across different parts of the city to identify pollution hotspots, understand temporal trends, and engage the public in environmental awareness and decision-making processes. Implementation Using Apache Superset: 1) Data Collection: - Deploy IoT sensors across the city to measure air quality indicators like PM2.5, PM10, NO2, CO2, and Ozone. - Sensors transmit data to a centralised database, storing time-stamped readings and location data. 2) Data Integration with Apache Superset: - Connect Superset to the database where sensor data is stored. - Ensure data is updated in real-time or at regular intervals for accurate analysis. 3) Exploratory Data Analysis: - Use Superset\u2019s SQL IDE to query specific aspects of the data, like identifying times of day with peak pollution levels. - Create visualisations to explore correlations, such as between traffic intensity and NO2 levels. 4) Dashboard Creation: - Develop an interactive dashboard in Superset, displaying various visualisations: - Maps showing real-time pollution levels across different city areas. - Line charts depicting temporal trends in air quality. - Bar charts comparing air quality on weekdays vs weekends. - Include filters to allow users to select specific dates, times, or areas for detailed analysis. 5) Data-Driven Decisions: - Use insights from the Superset dashboard to guide community actions and policy recommendations. - For example, advocate for traffic reduction measures in areas with consistently high pollution levels. References Apache Superset documentation GREENGAGE catalogue entry","title":"Apache Superset"},{"location":"tools/superset/#apache-superset","text":"","title":"Apache Superset"},{"location":"tools/superset/#introduction","text":"Apache Superset is a sophisticated, open-source data exploration and visualisation platform designed to transform the way analysts and businesses interact with their data. As a versatile web application, it empowers users with tools for deep data exploration and the creation of beautifully rendered visualisations and dashboards. Superset is renowned for its user-friendly interface, democratising data analysis by enabling users of all skill levels to engage with complex datasets, design compelling visual narratives, and derive actionable insights. Offering a rich array of features, including a variety of chart types, interactive dashboards, and an integrated SQL editor, Apache Superset stands out as a comprehensive solution for modern data analytics and business intelligence needs.","title":"Introduction"},{"location":"tools/superset/#features-of-superset","text":"Data Exploration: Superset allows users to explore datasets through simple, intuitive, and interactive visualisations. It offers a variety of chart types and the ability to dive deep into the details of the data. Easy Visualisation Creation: Users can easily create and share visualisations without the need for programming. It includes a rich set of visualisations like line charts, bar charts, and scatter plots, among others. Interactive Dashboards: Superset enables the creation of dynamic dashboards that are interactive and customisable. These dashboards can be shared with others and include various visualisations for comprehensive data analysis. SQL IDE: It comes with an integrated SQL IDE, which allows users to write, run, and visualise the results of SQL queries in a user-friendly environment. Security and Integration: Apache Superset offers robust security features, including role-based permission control. It can be integrated with most SQL-based data sources, including traditional databases and newer SQL-speaking data engines. Scalability and Performance: It is designed to be highly scalable, easily handling large-scale data exploration jobs. It also provides features for optimising query performance. Open Source and Extensible: Being open source allows for customisation and extension, enabling developers to contribute to its features or adapt it to specific needs. Community-Driven: Apache Superset is backed by a strong community, which contributes to its continuous development and improvement.","title":"Features of Superset"},{"location":"tools/superset/#use-case-scenario","text":"","title":"Use Case Scenario"},{"location":"tools/superset/#context","text":"In an urban citizen observatory, members of the community actively participate in monitoring and analysing environmental conditions. One of the critical concerns in many cities is air quality, which directly impacts public health and quality of life.","title":"Context:"},{"location":"tools/superset/#objective","text":"To monitor, analyse, and visualise air quality data across different parts of the city to identify pollution hotspots, understand temporal trends, and engage the public in environmental awareness and decision-making processes.","title":"Objective:"},{"location":"tools/superset/#implementation-using-apache-superset","text":"1) Data Collection: - Deploy IoT sensors across the city to measure air quality indicators like PM2.5, PM10, NO2, CO2, and Ozone. - Sensors transmit data to a centralised database, storing time-stamped readings and location data. 2) Data Integration with Apache Superset: - Connect Superset to the database where sensor data is stored. - Ensure data is updated in real-time or at regular intervals for accurate analysis. 3) Exploratory Data Analysis: - Use Superset\u2019s SQL IDE to query specific aspects of the data, like identifying times of day with peak pollution levels. - Create visualisations to explore correlations, such as between traffic intensity and NO2 levels. 4) Dashboard Creation: - Develop an interactive dashboard in Superset, displaying various visualisations: - Maps showing real-time pollution levels across different city areas. - Line charts depicting temporal trends in air quality. - Bar charts comparing air quality on weekdays vs weekends. - Include filters to allow users to select specific dates, times, or areas for detailed analysis. 5) Data-Driven Decisions: - Use insights from the Superset dashboard to guide community actions and policy recommendations. - For example, advocate for traffic reduction measures in areas with consistently high pollution levels.","title":"Implementation Using Apache Superset:"},{"location":"tools/superset/#references","text":"Apache Superset documentation GREENGAGE catalogue entry","title":"References"},{"location":"tools/superset/usage/","text":"Apache Superset Usage Guide Superset allows you to create charts and dashboards from a wide variety of data sources. In this guide we will show you how to create charts and dashboards from the data sources that we have available in Greengage: Step 1: Log In - Access to the Superset deploy of Greengage. - First of all, you should login using your Greengage login in the keycloak authentication webpage that will appear. - You will reach to the main view of the tool as shown in the figure below. Step 2: Creating datasets Click on the DATASETS button at the top-left menu and click on the +DATASET button at the top-right part of the view that you have arrived. Introduce the database, schema and table that you want to import from the possible sources and click CREATE DATASET AND CREATE CHART button. NOTE: In case that you want to connect Apache Superset to a new database or source contact with ruben.sanchez@deusto.es Step 3: Creating charts Click on the CHARTS button at the top-left menu and click on the +CHART button at the top-right part of the view that you have arrived. For the chart creation you should select a dataset and a chart type to show the data. Once in the chart editing view, you should add a name to the chart. In this view you have two columns: the left one has all the rows from the dataset yout selected and the right one has the settings for the chart. Once you have selected the right settings, click on the CREATE CHART button at the bottom of the second column. Note that to upload the chart you will have to click again in a button that updates it in the same position as the previous one. Step 4: Creating dashboards Click on the DASHBOARDS button at the top-left menu and click on the +DASHBOARD button at the top-right part of the view that you have arrived. Once in the dashboard creation view, you should add a name to the dashboard. In this view you can add already created charts that appear in the right side or create new charts. Furthermore, you can add new Layout elements such as headers, text or dividers. Extra: Executing SQL queries to imported datasets Accessing to the SQL view using the button at the top left menu you can access a SQL statement executor against the datasets imported in Superset. Creating a chart from geo sources First of all, create a chart with the desired data source. For geographical sources, the best is to put the \"map\" word into the search bar to find all the map-based charts. Once in the chart editing view, you should identify the fields regarding latitude and longitude. Note that if both fields are in one field they may be inverted. Once the settings are set, create or update the chart to visualise it.","title":"Apache Superset Usage Guide"},{"location":"tools/superset/usage/#apache-superset-usage-guide","text":"Superset allows you to create charts and dashboards from a wide variety of data sources. In this guide we will show you how to create charts and dashboards from the data sources that we have available in Greengage: Step 1: Log In - Access to the Superset deploy of Greengage. - First of all, you should login using your Greengage login in the keycloak authentication webpage that will appear. - You will reach to the main view of the tool as shown in the figure below. Step 2: Creating datasets Click on the DATASETS button at the top-left menu and click on the +DATASET button at the top-right part of the view that you have arrived. Introduce the database, schema and table that you want to import from the possible sources and click CREATE DATASET AND CREATE CHART button. NOTE: In case that you want to connect Apache Superset to a new database or source contact with ruben.sanchez@deusto.es Step 3: Creating charts Click on the CHARTS button at the top-left menu and click on the +CHART button at the top-right part of the view that you have arrived. For the chart creation you should select a dataset and a chart type to show the data. Once in the chart editing view, you should add a name to the chart. In this view you have two columns: the left one has all the rows from the dataset yout selected and the right one has the settings for the chart. Once you have selected the right settings, click on the CREATE CHART button at the bottom of the second column. Note that to upload the chart you will have to click again in a button that updates it in the same position as the previous one. Step 4: Creating dashboards Click on the DASHBOARDS button at the top-left menu and click on the +DASHBOARD button at the top-right part of the view that you have arrived. Once in the dashboard creation view, you should add a name to the dashboard. In this view you can add already created charts that appear in the right side or create new charts. Furthermore, you can add new Layout elements such as headers, text or dividers. Extra: Executing SQL queries to imported datasets Accessing to the SQL view using the button at the top left menu you can access a SQL statement executor against the datasets imported in Superset. Creating a chart from geo sources First of all, create a chart with the desired data source. For geographical sources, the best is to put the \"map\" word into the search bar to find all the map-based charts. Once in the chart editing view, you should identify the fields regarding latitude and longitude. Note that if both fields are in one field they may be inverted. Once the settings are set, create or update the chart to visualise it.","title":"Apache Superset Usage Guide"},{"location":"tools/superset/examples/","text":"Apache Superset Apache Superset cannot be integrated with third-party tools, however, new data sources can be integrated into Superset. Integrating new data sources to Apache Superset Apache Superset can ingest data from more than 40 sources. Some of the integrations have built-in configuration wizards, as seen in the image below. However, others, only request the connection URL as stated in Superset documentation Once you have added the details for the connection, Apache Superset will create a connection test and provide you with feedback on it. If the connection is successful, the new datasets are ready to be used.","title":"Apache Superset"},{"location":"tools/superset/examples/#apache-superset","text":"Apache Superset cannot be integrated with third-party tools, however, new data sources can be integrated into Superset.","title":"Apache Superset"},{"location":"tools/superset/examples/#integrating-new-data-sources-to-apache-superset","text":"Apache Superset can ingest data from more than 40 sources. Some of the integrations have built-in configuration wizards, as seen in the image below. However, others, only request the connection URL as stated in Superset documentation Once you have added the details for the connection, Apache Superset will create a connection test and provide you with feedback on it. If the connection is successful, the new datasets are ready to be used.","title":"Integrating new data sources to Apache Superset"},{"location":"tools/wordpress/","text":"WordPress Introduction Description WordPress is a versatile Content Management System (CMS). It initially gained popularity for blog creation but has since evolved into a key tool for developing a wide range of websites, including commercial ones. Its flexibility and user-friendly interface make it a go-to solution for website creation. Objective The primary objective of WordPress is to simplify the process of website creation and management. Its user-friendly design, combined with powerful features, makes it an ideal choice for users ranging from novices to seasoned web developers. WordPress aims to enhance website functionality, improve user experience, and provide robust website management tools. Features of WordPress List of Features Easy Installation, Update, and Customization : Streamlines the setup and maintenance process. Automatic System Updates : Ensures the CMS stays current and secure. Multiple Authors and User Roles : Allows for collaborative work with various permission levels. Support for Multiple Blogs : Facilitates the management of several blogs within a single site. Static Page Creation Capability : Enhances the flexibility of content management. Additional Features : Includes categorization of articles and pages, password protection, WYSIWYG editor, email publishing, import options from various platforms, auto-save drafts, comment and communication tools, permalink support, content and comment distribution via RSS and Atom, link management, multimedia file management, plugin and widget support, integrated search functionality, and theme creation system. Use Case Scenario Description A typical use case for WordPress involves creating and managing a professional blog or a commercial website. In this scenario, WordPress serves as the backbone, offering tools for content creation, theme customization, and audience engagement through comments and social media integrations. The scenario demonstrates WordPress' versatility in handling diverse web development needs. References Description For comprehensive guides and technical details about WordPress, the following resources are available: Support guides : https://wordpress.org/documentation/support-guides/ - Provides step-by-step instructions for using WordPress effectively.","title":"WordPress"},{"location":"tools/wordpress/#wordpress","text":"","title":"WordPress"},{"location":"tools/wordpress/#introduction","text":"","title":"Introduction"},{"location":"tools/wordpress/#description","text":"WordPress is a versatile Content Management System (CMS). It initially gained popularity for blog creation but has since evolved into a key tool for developing a wide range of websites, including commercial ones. Its flexibility and user-friendly interface make it a go-to solution for website creation.","title":"Description"},{"location":"tools/wordpress/#objective","text":"The primary objective of WordPress is to simplify the process of website creation and management. Its user-friendly design, combined with powerful features, makes it an ideal choice for users ranging from novices to seasoned web developers. WordPress aims to enhance website functionality, improve user experience, and provide robust website management tools.","title":"Objective"},{"location":"tools/wordpress/#features-of-wordpress","text":"","title":"Features of WordPress"},{"location":"tools/wordpress/#list-of-features","text":"Easy Installation, Update, and Customization : Streamlines the setup and maintenance process. Automatic System Updates : Ensures the CMS stays current and secure. Multiple Authors and User Roles : Allows for collaborative work with various permission levels. Support for Multiple Blogs : Facilitates the management of several blogs within a single site. Static Page Creation Capability : Enhances the flexibility of content management. Additional Features : Includes categorization of articles and pages, password protection, WYSIWYG editor, email publishing, import options from various platforms, auto-save drafts, comment and communication tools, permalink support, content and comment distribution via RSS and Atom, link management, multimedia file management, plugin and widget support, integrated search functionality, and theme creation system.","title":"List of Features"},{"location":"tools/wordpress/#use-case-scenario","text":"","title":"Use Case Scenario"},{"location":"tools/wordpress/#description_1","text":"A typical use case for WordPress involves creating and managing a professional blog or a commercial website. In this scenario, WordPress serves as the backbone, offering tools for content creation, theme customization, and audience engagement through comments and social media integrations. The scenario demonstrates WordPress' versatility in handling diverse web development needs.","title":"Description"},{"location":"tools/wordpress/#references","text":"","title":"References"},{"location":"tools/wordpress/#description_2","text":"For comprehensive guides and technical details about WordPress, the following resources are available: Support guides : https://wordpress.org/documentation/support-guides/ - Provides step-by-step instructions for using WordPress effectively.","title":"Description"},{"location":"tools/wordpress/integration/","text":"Integration WordPress + Keycloak Integration Introduction Integrating Keycloak with WordPress enhances security by implementing Single Sign-On (SSO). This integration allows users to access WordPress using Keycloak credentials, streamlining the login process and improving security. Step 1: Integrating Keycloak in WordPress Access WordPress Admin : Log into wp-admin in WordPress using credentials defined in docker-compose ( WORDPRESS_EMAIL , WORDPRESS_PASSWORD ). Install OAuth SSO Plugin : Go to 'Plugins' > 'Add New'. Search for \"OAuth Single Sign On \u2013 SSO (OAuth Client)\" and click on \"Install Now\". Activate and Configure : After installation, click \"Activate\". You\u2019ll be redirected to the configuration page. Follow the steps in the \"Setup Wizard\". For more details on Keycloak integration, visit the Keycloak WordPress Integration Documentation . Step 2: Configuring Keycloak with WordPress Ensure you have a clientid and secret from Keycloak. Follow the instructions provided in the setup wizard to integrate these credentials into WordPress. WordPress + Discourse Integration Introduction Integrating Discourse with WordPress allows seamless connectivity between WordPress posts and Discourse discussions. This integration enhances community engagement by linking WordPress content with Discourse forums. Step 1: Setting Up API Key in Discourse Admin Access : Log into Discourse as an admin. Create API Key : Navigate to /admin/api/keys . Create an API Key (User Level: \"All Users\", Scope: \"Global\") and save it for later use in WordPress. Step 2: Integrating Discourse in WordPress Access WordPress Admin : Log into wp-admin using docker-compose credentials ( WORDPRESS_EMAIL , WORDPRESS_PASSWORD ). Install WP Discourse Plugin : Go to 'Plugins' > 'Add New'. Search for \"WP Discourse\" and click \"Install Now\". Activate and Configure : After installation, click \"Activate\". A \"Discourse\" option will appear in the sidebar. Configure Plugin Settings : In the \"Connection\" section - Set the 'Discourse URL'. - Use the API Key created in Discourse. - Define the \"Publishing Username\" In the \"Publishing\" section, enable the following options - \"Default Discourse Category\" - \"Use Full Post Content\" - \"Auto Publish\" - \"Send Email Notification on Publish Failure\" - (Optional) Set \"Email Address for Failure Notification\" - \"Auto Track Published Topics\" In the \"Commenting\" tab, enable the following - [IMPORTANT] \"Enable Discourse Comments\" - \"Show Existing WP Comments\" In the \"Webhook\" tab, enable \"Sync Comment Data\". For more information on configuring the WP Discourse plugin, visit the Discourse WordPress Plugin Guide . Important Notes: Ensure to securely store and manage your API keys and credentials. Regularly update your integration plugins to maintain security and functionality. In case of any security concerns or compromised credentials, contact the respective support teams immediately.","title":"Integration"},{"location":"tools/wordpress/integration/#integration","text":"","title":"Integration"},{"location":"tools/wordpress/integration/#wordpress-keycloak-integration","text":"","title":"WordPress + Keycloak Integration"},{"location":"tools/wordpress/integration/#introduction","text":"Integrating Keycloak with WordPress enhances security by implementing Single Sign-On (SSO). This integration allows users to access WordPress using Keycloak credentials, streamlining the login process and improving security.","title":"Introduction"},{"location":"tools/wordpress/integration/#step-1-integrating-keycloak-in-wordpress","text":"Access WordPress Admin : Log into wp-admin in WordPress using credentials defined in docker-compose ( WORDPRESS_EMAIL , WORDPRESS_PASSWORD ). Install OAuth SSO Plugin : Go to 'Plugins' > 'Add New'. Search for \"OAuth Single Sign On \u2013 SSO (OAuth Client)\" and click on \"Install Now\". Activate and Configure : After installation, click \"Activate\". You\u2019ll be redirected to the configuration page. Follow the steps in the \"Setup Wizard\". For more details on Keycloak integration, visit the Keycloak WordPress Integration Documentation .","title":"Step 1: Integrating Keycloak in WordPress"},{"location":"tools/wordpress/integration/#step-2-configuring-keycloak-with-wordpress","text":"Ensure you have a clientid and secret from Keycloak. Follow the instructions provided in the setup wizard to integrate these credentials into WordPress.","title":"Step 2: Configuring Keycloak with WordPress"},{"location":"tools/wordpress/integration/#wordpress-discourse-integration","text":"","title":"WordPress + Discourse Integration"},{"location":"tools/wordpress/integration/#introduction_1","text":"Integrating Discourse with WordPress allows seamless connectivity between WordPress posts and Discourse discussions. This integration enhances community engagement by linking WordPress content with Discourse forums.","title":"Introduction"},{"location":"tools/wordpress/integration/#step-1-setting-up-api-key-in-discourse","text":"Admin Access : Log into Discourse as an admin. Create API Key : Navigate to /admin/api/keys . Create an API Key (User Level: \"All Users\", Scope: \"Global\") and save it for later use in WordPress.","title":"Step 1: Setting Up API Key in Discourse"},{"location":"tools/wordpress/integration/#step-2-integrating-discourse-in-wordpress","text":"Access WordPress Admin : Log into wp-admin using docker-compose credentials ( WORDPRESS_EMAIL , WORDPRESS_PASSWORD ). Install WP Discourse Plugin : Go to 'Plugins' > 'Add New'. Search for \"WP Discourse\" and click \"Install Now\". Activate and Configure : After installation, click \"Activate\". A \"Discourse\" option will appear in the sidebar. Configure Plugin Settings : In the \"Connection\" section - Set the 'Discourse URL'. - Use the API Key created in Discourse. - Define the \"Publishing Username\" In the \"Publishing\" section, enable the following options - \"Default Discourse Category\" - \"Use Full Post Content\" - \"Auto Publish\" - \"Send Email Notification on Publish Failure\" - (Optional) Set \"Email Address for Failure Notification\" - \"Auto Track Published Topics\" In the \"Commenting\" tab, enable the following - [IMPORTANT] \"Enable Discourse Comments\" - \"Show Existing WP Comments\" In the \"Webhook\" tab, enable \"Sync Comment Data\". For more information on configuring the WP Discourse plugin, visit the Discourse WordPress Plugin Guide . Important Notes: Ensure to securely store and manage your API keys and credentials. Regularly update your integration plugins to maintain security and functionality. In case of any security concerns or compromised credentials, contact the respective support teams immediately.","title":"Step 2: Integrating Discourse in WordPress"},{"location":"tools/wordpress/examples/","text":"WordPress, Keycloak, and Discourse Integration Guide Introduction This guide details the process of deploying WordPress, Keycloak, and Discourse using Docker, and integrating them for a seamless user experience. This setup utilizes Keycloak for Single Sign-On (SSO) across WordPress and Discourse, enhancing security and streamlining user management. Prerequisites Docker and Docker Compose installed. Familiarity with Docker, YAML syntax, and basic network configurations. Access to a SMTP service for email functionalities. Docker Compose Configuration The provided docker-compose.yml file outlines the configuration for deploying WordPress, Keycloak, and Discourse. The structure includes: WordPress Service : Utilizes the bitnami/wordpress image, configured for web content management. MariaDB Service : A database for WordPress. Keycloak Database : PostgreSQL database dedicated to Keycloak. Keycloak Service : Manages SSO and user authentication. Discourse Services : Includes postgresqldiscourse , redis , discourse , and sidekiq for the forum platform. Deploying the Services Download or Clone the Docker Compose File : Obtain the provided docker-compose.yml . Configure Environment Variables : Replace placeholders in the file with actual values for SMTP, database credentials, and other settings. Run the Services : Execute docker-compose up in the directory containing the Docker Compose file. Access Services : WordPress: Accessible at http://localhost:9000 . Discourse: Accessible at http://localhost:3000 . Keycloak: Accessible at http://localhost:8080 . Integrating Keycloak with WordPress and Discourse WordPress + Keycloak Install OAuth SSO Plugin in WordPress : Access WordPress admin panel. Navigate to 'Plugins' > 'Add New'. Search and install \"OAuth Single Sign On \u2013 SSO (OAuth Client)\". Configure Keycloak Integration : Use Keycloak's clientid and secret . Follow the setup wizard in WordPress for Keycloak integration. Discourse + Keycloak Enable Keycloak SSO in Discourse : Access the Discourse container's command line. Install the OpenID Connect plugin and configure it to use Keycloak as the SSO provider. Configure API Key in Discourse : Generate an API key in Discourse admin panel for integration purposes. WordPress + Discourse Install WP Discourse Plugin in WordPress : Access WordPress admin panel. Navigate to 'Plugins' > 'Add New'. Search and install \"WP Discourse\". Configure Discourse Integration : Use the Discourse API key. Set up publishing and commenting settings in WordPress to synchronize with Discourse. Example of Post Integration from WordPress to Discourse This example demonstrates the seamless integration of posting on WordPress and how it reflects on Discourse. First, we create a post in WordPress. As soon as the post is published, it automatically generates a corresponding thread in Discourse. Now, the post made in WordPress can be viewed as a thread in Discourse. Additionally, users can engage with the post by commenting directly in the Discourse thread. Additional Configuration and Troubleshooting Email Setup : Ensure SMTP settings are correctly configured in both WordPress and Discourse for email functionalities. Security and Data Backup : Regularly update services and back up data. Troubleshooting : Consult the official documentation of WordPress, Keycloak, and Discourse for specific issues. Support and Additional Resources Community Forums : Engage with community forums for WordPress, Keycloak, and Discourse for support. External Documentation : Refer to the official documentation of each tool for detailed guides and updates. Note : Always ensure that you are working with the latest versions of the software and following the best practices for security and maintenance.","title":"WordPress, Keycloak, and Discourse Integration Guide"},{"location":"tools/wordpress/examples/#wordpress-keycloak-and-discourse-integration-guide","text":"","title":"WordPress, Keycloak, and Discourse Integration Guide"},{"location":"tools/wordpress/examples/#introduction","text":"This guide details the process of deploying WordPress, Keycloak, and Discourse using Docker, and integrating them for a seamless user experience. This setup utilizes Keycloak for Single Sign-On (SSO) across WordPress and Discourse, enhancing security and streamlining user management.","title":"Introduction"},{"location":"tools/wordpress/examples/#prerequisites","text":"Docker and Docker Compose installed. Familiarity with Docker, YAML syntax, and basic network configurations. Access to a SMTP service for email functionalities.","title":"Prerequisites"},{"location":"tools/wordpress/examples/#docker-compose-configuration","text":"The provided docker-compose.yml file outlines the configuration for deploying WordPress, Keycloak, and Discourse. The structure includes: WordPress Service : Utilizes the bitnami/wordpress image, configured for web content management. MariaDB Service : A database for WordPress. Keycloak Database : PostgreSQL database dedicated to Keycloak. Keycloak Service : Manages SSO and user authentication. Discourse Services : Includes postgresqldiscourse , redis , discourse , and sidekiq for the forum platform.","title":"Docker Compose Configuration"},{"location":"tools/wordpress/examples/#deploying-the-services","text":"Download or Clone the Docker Compose File : Obtain the provided docker-compose.yml . Configure Environment Variables : Replace placeholders in the file with actual values for SMTP, database credentials, and other settings. Run the Services : Execute docker-compose up in the directory containing the Docker Compose file. Access Services : WordPress: Accessible at http://localhost:9000 . Discourse: Accessible at http://localhost:3000 . Keycloak: Accessible at http://localhost:8080 .","title":"Deploying the Services"},{"location":"tools/wordpress/examples/#integrating-keycloak-with-wordpress-and-discourse","text":"","title":"Integrating Keycloak with WordPress and Discourse"},{"location":"tools/wordpress/examples/#wordpress-keycloak","text":"Install OAuth SSO Plugin in WordPress : Access WordPress admin panel. Navigate to 'Plugins' > 'Add New'. Search and install \"OAuth Single Sign On \u2013 SSO (OAuth Client)\". Configure Keycloak Integration : Use Keycloak's clientid and secret . Follow the setup wizard in WordPress for Keycloak integration.","title":"WordPress + Keycloak"},{"location":"tools/wordpress/examples/#discourse-keycloak","text":"Enable Keycloak SSO in Discourse : Access the Discourse container's command line. Install the OpenID Connect plugin and configure it to use Keycloak as the SSO provider. Configure API Key in Discourse : Generate an API key in Discourse admin panel for integration purposes.","title":"Discourse + Keycloak"},{"location":"tools/wordpress/examples/#wordpress-discourse","text":"Install WP Discourse Plugin in WordPress : Access WordPress admin panel. Navigate to 'Plugins' > 'Add New'. Search and install \"WP Discourse\". Configure Discourse Integration : Use the Discourse API key. Set up publishing and commenting settings in WordPress to synchronize with Discourse.","title":"WordPress + Discourse"},{"location":"tools/wordpress/examples/#example-of-post-integration-from-wordpress-to-discourse","text":"This example demonstrates the seamless integration of posting on WordPress and how it reflects on Discourse. First, we create a post in WordPress. As soon as the post is published, it automatically generates a corresponding thread in Discourse. Now, the post made in WordPress can be viewed as a thread in Discourse. Additionally, users can engage with the post by commenting directly in the Discourse thread.","title":"Example of Post Integration from WordPress to Discourse"},{"location":"tools/wordpress/examples/#additional-configuration-and-troubleshooting","text":"Email Setup : Ensure SMTP settings are correctly configured in both WordPress and Discourse for email functionalities. Security and Data Backup : Regularly update services and back up data. Troubleshooting : Consult the official documentation of WordPress, Keycloak, and Discourse for specific issues.","title":"Additional Configuration and Troubleshooting"},{"location":"tools/wordpress/examples/#support-and-additional-resources","text":"Community Forums : Engage with community forums for WordPress, Keycloak, and Discourse for support. External Documentation : Refer to the official documentation of each tool for detailed guides and updates. Note : Always ensure that you are working with the latest versions of the software and following the best practices for security and maintenance.","title":"Support and Additional Resources"}]}