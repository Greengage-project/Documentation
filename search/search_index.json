{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Technical Guide for GREENGAGE Project Integration Project The GREENGAGE project is a three-year-long pan-European initiative aimed at strengthening urban governance through citizen-based co-creation and crowd-sourcing. Funded under the Horizon Europe Framework Programme and co-funded by the UK government and Switzerland's State Secretariat for Education, Research and Innovation, this project is led by the Austrian Institute of Technology and comprises 17 research and industry partners from the EU and the UK. The project seeks to encourage innovative governance processes and assist public authorities in formulating climate mitigation and adaptation policies. By leveraging citizen participation and providing innovative digital solutions, GREENGAGE lays the groundwork for co-creating and co-designing innovative solutions to monitor environmental issues at the grassroots level. The developed Citizen Observatories (CO) are intended to be further integrated into urban planning decision-making processes. The project encompasses campaigns in four pilot cities across Europe, focusing on mobility, air quality, and healthy living, with citizens encouraged to observe and co-create data solutions for urban environments through Citizen Observatories. Tools The tools directory contains subdirectories for each tool utilized in the project. Each subdirectory includes a detailed description of the tool, its functionality, and its role within the project. Keycloak Directory : /tools Link : tools/keycloak Deployed in : https://auth1.demo.greengage-project.eu Description : Keycloak is an open-source Identity and Access Management solution aimed at modern applications and services. It facilitates the secure transmission of identity information between applications and services, even across different domains. Examples of programmatic integration/usage : Node Python Collaborative Environment Directory : /tools Link : tools/collaborativeEnvironment Deployed in : https://demo.greengage-project.eu Description : The Collaborative Environment is a comprehensive platform designed to facilitate co-production, asset management, and enhanced collaboration among users. It incorporates various interlinkers and modules to streamline workflows and improve organizational processes, supporting integration with different technologies and frameworks. Examples of programmatic integration/usage : Deploy example MODE Directory : /tools Link : tools/mode Description : This module allows to determine the transport mode of a user based on GPS data provided. Examples of programmatic integration/usage : iOS example Android example Backend example Discourse Directory : /tools Link : tools/Discourse Deployed in : https://discourse.16.171.94.204.nip.io Description : Discourse is a modern, open-source discussion platform designed for building and managing community forums. Known for its user-friendly interface and robust features, it supports real-time discussions, multimedia integration, and comprehensive moderation tools. Ideal for a wide range of communities, from small groups to large enterprises, Discourse fosters engagement, knowledge sharing, and collaboration. Examples of programmatic integration/usage : Deploy example DataHub Directory : /tools Link : tools/DataHub Deployed in : https://datahub.16.171.94.204.nip.io Description : DataHub is an open-source metadata platform designed to democratise data discovery. It acts as a centralised system for companies and teams to track, manage, and find data within their organisation. Its primary aim is to streamline the process of accessing and understanding data assets, making it easier for teams to derive insights and make informed decisions. Examples of programmatic integration : Integration example Apache Superset Directory : /tools Link : tools/Superset Deployed in : https://superset.16.171.94.204.nip.io Description : Apache Superset is a sophisticated, open-source data exploration and visualisation platform designed to transform the way analysts and businesses interact with their data. As a versatile web application, it empowers users with tools for deep data exploration and the creation of beautifully rendered visualisations and dashboards. Superset is renowned for its user-friendly interface, democratising data analysis by enabling users of all skill levels to engage with complex datasets, design compelling visual narratives, and derive actionable insights. Offering a rich array of features, including a variety of chart types, interactive dashboards, and an integrated SQL editor, Apache Superset stands out as a comprehensive solution for modern data analytics and business intelligence needs. Examples of integration of sources with Superset : Integration example Libelium MQTT sensor integration Directory : /tools Link : tools/sensors_integration Description : This documentation contains all the information necessary to connect and configure the IoT sensors provided by Libelium to MQTT broker and integrate it in other platforms or architectures. The above README provides a structured outline to document the technical aspects of the GREENGAGE project, offering a solid starting point to further elaborate on the tools and processes involved in the project. Source code in: https://github.com/Greengage-project/Documentation","title":"Home"},{"location":"#technical-guide-for-greengage-project-integration","text":"","title":"Technical Guide for GREENGAGE Project Integration"},{"location":"#project","text":"The GREENGAGE project is a three-year-long pan-European initiative aimed at strengthening urban governance through citizen-based co-creation and crowd-sourcing. Funded under the Horizon Europe Framework Programme and co-funded by the UK government and Switzerland's State Secretariat for Education, Research and Innovation, this project is led by the Austrian Institute of Technology and comprises 17 research and industry partners from the EU and the UK. The project seeks to encourage innovative governance processes and assist public authorities in formulating climate mitigation and adaptation policies. By leveraging citizen participation and providing innovative digital solutions, GREENGAGE lays the groundwork for co-creating and co-designing innovative solutions to monitor environmental issues at the grassroots level. The developed Citizen Observatories (CO) are intended to be further integrated into urban planning decision-making processes. The project encompasses campaigns in four pilot cities across Europe, focusing on mobility, air quality, and healthy living, with citizens encouraged to observe and co-create data solutions for urban environments through Citizen Observatories.","title":"Project"},{"location":"#tools","text":"The tools directory contains subdirectories for each tool utilized in the project. Each subdirectory includes a detailed description of the tool, its functionality, and its role within the project.","title":"Tools"},{"location":"#keycloak","text":"Directory : /tools Link : tools/keycloak Deployed in : https://auth1.demo.greengage-project.eu Description : Keycloak is an open-source Identity and Access Management solution aimed at modern applications and services. It facilitates the secure transmission of identity information between applications and services, even across different domains. Examples of programmatic integration/usage : Node Python","title":"Keycloak"},{"location":"#collaborative-environment","text":"Directory : /tools Link : tools/collaborativeEnvironment Deployed in : https://demo.greengage-project.eu Description : The Collaborative Environment is a comprehensive platform designed to facilitate co-production, asset management, and enhanced collaboration among users. It incorporates various interlinkers and modules to streamline workflows and improve organizational processes, supporting integration with different technologies and frameworks. Examples of programmatic integration/usage : Deploy example","title":"Collaborative Environment"},{"location":"#mode","text":"Directory : /tools Link : tools/mode Description : This module allows to determine the transport mode of a user based on GPS data provided. Examples of programmatic integration/usage : iOS example Android example Backend example","title":"MODE"},{"location":"#discourse","text":"Directory : /tools Link : tools/Discourse Deployed in : https://discourse.16.171.94.204.nip.io Description : Discourse is a modern, open-source discussion platform designed for building and managing community forums. Known for its user-friendly interface and robust features, it supports real-time discussions, multimedia integration, and comprehensive moderation tools. Ideal for a wide range of communities, from small groups to large enterprises, Discourse fosters engagement, knowledge sharing, and collaboration. Examples of programmatic integration/usage : Deploy example","title":"Discourse"},{"location":"#datahub","text":"Directory : /tools Link : tools/DataHub Deployed in : https://datahub.16.171.94.204.nip.io Description : DataHub is an open-source metadata platform designed to democratise data discovery. It acts as a centralised system for companies and teams to track, manage, and find data within their organisation. Its primary aim is to streamline the process of accessing and understanding data assets, making it easier for teams to derive insights and make informed decisions. Examples of programmatic integration : Integration example","title":"DataHub"},{"location":"#apache-superset","text":"Directory : /tools Link : tools/Superset Deployed in : https://superset.16.171.94.204.nip.io Description : Apache Superset is a sophisticated, open-source data exploration and visualisation platform designed to transform the way analysts and businesses interact with their data. As a versatile web application, it empowers users with tools for deep data exploration and the creation of beautifully rendered visualisations and dashboards. Superset is renowned for its user-friendly interface, democratising data analysis by enabling users of all skill levels to engage with complex datasets, design compelling visual narratives, and derive actionable insights. Offering a rich array of features, including a variety of chart types, interactive dashboards, and an integrated SQL editor, Apache Superset stands out as a comprehensive solution for modern data analytics and business intelligence needs. Examples of integration of sources with Superset : Integration example","title":"Apache Superset"},{"location":"#libelium-mqtt-sensor-integration","text":"Directory : /tools Link : tools/sensors_integration Description : This documentation contains all the information necessary to connect and configure the IoT sensors provided by Libelium to MQTT broker and integrate it in other platforms or architectures. The above README provides a structured outline to document the technical aspects of the GREENGAGE project, offering a solid starting point to further elaborate on the tools and processes involved in the project.","title":"Libelium MQTT sensor integration"},{"location":"#source-code-in-httpsgithubcomgreengage-projectdocumentation","text":"","title":"Source code in: https://github.com/Greengage-project/Documentation"},{"location":"guidelines/","text":"Comprehensive Guidelines for Documenting and Integrating Tools 1. Duplicating the Documentation Template Action : Begin by duplicating the folder docs/tools/TOOLNAME_template . Purpose : This template serves as a foundational structure for documenting new tools, ensuring consistency across your documentation. 2. Describing the Tool in index.md Content Requirements : In index.md , offer a comprehensive overview of the tool. This should include its purpose, key features, and any unique aspects that make it beneficial for your project. Incorporating External Documentation : If there are external resources or official documentation for the tool, link them. This helps users understand the broader context and capabilities of the tool. External resources should be listed in the References section at the end of index.md, check the template provided. Notice that resources like images or PDFs should be stored in subfolder assets . Example Structure : ```markdown # Tool Name ## Introduction to Tool Name [Detailed introduction, including the tool's purpose and general functionality.] ## Features [Detailed list of features, explaining how each aspect adds value to the project.] ## Use Case Illustration [Detailed description of a use case, demonstrating how the tool integrates into workflows or solves specific problems.] ## References [Detailed list of references, include as one reference PDF version of catalogue entry for your tool.] ``` 3. Usage and Integration Guide in integration.md Content : Provide some brief description on how the tools is supposed to be used. If your tool is going to be offered as a SaaS extend such usage highlights, always referring to further documentation. In case your app is designed to be integrated with third party tools, then provide a clear, step-by-step guide for integrating the tool with third-party services. Remember provide a channel to ask for support about the tool. Structure : Usage instructions : Brief usage instructions of the tool. Add references to external resources where further details about usage are given if available. Otherwise, try to make these descriptions self-contained, including snaphshots, URLs and other usage tips. Requesting Credentials : Outline the process for obtaining necessary access credentials. Configuration Steps : Detail the steps for integrating the tool within their systems, including any specific settings or parameters. Testing and Validation : Emphasize the importance of testing the integration and provide guidance on troubleshooting common issues. 4. Proof of Concept Code in examples Purpose : Supply ready-to-use code examples in various programming languages to demonstrate practical integration or programmatic consumption/usage of the tool documented. Details : Each example should include comprehensive comments explaining each step and its relevance to the integration/usage process. 5. Documenting Examples in examples/index.md Structure : For each example, include objective of example, prerequisites, a detailed walkthrough of the project structure, and step-by-step instructions for setup and execution. Clarity : Ensure that the instructions are clear, concise, and suitable for users with varying levels of expertise. 6. Summarizing Tool in docs/index.md Overview : Provide a brief yet informative summary of the tool, highlighting its significance and utility within the project. Example Entry : ```markdown ### Tool Name Directory : /tools/toolname Documentation Link : Tool Documentation Description : A concise summary highlighting the key features and benefits of the tool. Insight into how it integrates with and enhances the project. Integration Example: Node.js Integration Example: Python ``` These guidelines aim to provide a thorough and user-friendly approach to documenting and integrating tools, thereby enhancing the accessibility and usability of your project's documentation.","title":"Guidelines"},{"location":"guidelines/#comprehensive-guidelines-for-documenting-and-integrating-tools","text":"","title":"Comprehensive Guidelines for Documenting and Integrating Tools"},{"location":"guidelines/#1-duplicating-the-documentation-template","text":"Action : Begin by duplicating the folder docs/tools/TOOLNAME_template . Purpose : This template serves as a foundational structure for documenting new tools, ensuring consistency across your documentation.","title":"1. Duplicating the Documentation Template"},{"location":"guidelines/#2-describing-the-tool-in-indexmd","text":"Content Requirements : In index.md , offer a comprehensive overview of the tool. This should include its purpose, key features, and any unique aspects that make it beneficial for your project. Incorporating External Documentation : If there are external resources or official documentation for the tool, link them. This helps users understand the broader context and capabilities of the tool. External resources should be listed in the References section at the end of index.md, check the template provided. Notice that resources like images or PDFs should be stored in subfolder assets . Example Structure : ```markdown # Tool Name ## Introduction to Tool Name [Detailed introduction, including the tool's purpose and general functionality.] ## Features [Detailed list of features, explaining how each aspect adds value to the project.] ## Use Case Illustration [Detailed description of a use case, demonstrating how the tool integrates into workflows or solves specific problems.] ## References [Detailed list of references, include as one reference PDF version of catalogue entry for your tool.] ```","title":"2. Describing the Tool in index.md"},{"location":"guidelines/#3-usage-and-integration-guide-in-integrationmd","text":"Content : Provide some brief description on how the tools is supposed to be used. If your tool is going to be offered as a SaaS extend such usage highlights, always referring to further documentation. In case your app is designed to be integrated with third party tools, then provide a clear, step-by-step guide for integrating the tool with third-party services. Remember provide a channel to ask for support about the tool. Structure : Usage instructions : Brief usage instructions of the tool. Add references to external resources where further details about usage are given if available. Otherwise, try to make these descriptions self-contained, including snaphshots, URLs and other usage tips. Requesting Credentials : Outline the process for obtaining necessary access credentials. Configuration Steps : Detail the steps for integrating the tool within their systems, including any specific settings or parameters. Testing and Validation : Emphasize the importance of testing the integration and provide guidance on troubleshooting common issues.","title":"3. Usage and Integration Guide in integration.md"},{"location":"guidelines/#4-proof-of-concept-code-in-examples","text":"Purpose : Supply ready-to-use code examples in various programming languages to demonstrate practical integration or programmatic consumption/usage of the tool documented. Details : Each example should include comprehensive comments explaining each step and its relevance to the integration/usage process.","title":"4. Proof of Concept Code in examples"},{"location":"guidelines/#5-documenting-examples-in-examplesindexmd","text":"Structure : For each example, include objective of example, prerequisites, a detailed walkthrough of the project structure, and step-by-step instructions for setup and execution. Clarity : Ensure that the instructions are clear, concise, and suitable for users with varying levels of expertise.","title":"5. Documenting Examples in examples/index.md"},{"location":"guidelines/#6-summarizing-tool-in-docsindexmd","text":"Overview : Provide a brief yet informative summary of the tool, highlighting its significance and utility within the project. Example Entry : ```markdown ### Tool Name Directory : /tools/toolname Documentation Link : Tool Documentation Description : A concise summary highlighting the key features and benefits of the tool. Insight into how it integrates with and enhances the project. Integration Example: Node.js Integration Example: Python ``` These guidelines aim to provide a thorough and user-friendly approach to documenting and integrating tools, thereby enhancing the accessibility and usability of your project's documentation.","title":"6. Summarizing Tool in docs/index.md"},{"location":"tools/Discourse/","text":"Discourse Introduction Description : Discourse is a free, open-source forum software that plays a crucial role in modern tech environments by enabling thoughtful discussion and meaningful connections. It integrates seamlessly with various platforms, enhancing both application functionality and community engagement. Objective : The integration of Discourse can significantly benefit development projects by offering robust collaboration and communication tools. It enhances application security with features like spam blocking and two-factor authentication, ensuring a safe and productive environment for users. Features of Discourse List of Features : Conversations, Not Pages : Utilizes just-in-time loading for seamless conversation flow. Real-Time Chat : Enables informal chats alongside more structured discussions. AI Integration : Enhances user experience and moderation with AI capabilities. Customizable User Experience : Offers a personalized interface with adjustable preferences. Mobile Compatibility : Fully functional on high-resolution touch devices. Link Expansion : Automatically enhances shared links with additional context. Single Sign-On : Facilitates easy integration with existing login systems. Trust System : Rewards regular members with additional abilities. Community Moderation : Allows community-driven content moderation. Spam Blocking : Integrates Akismet spam protection. Social Login : Supports common social media logins. Topic Summarization : Condenses long discussions to highlights. Badges : Encourages positive behaviors with a badge system. Emoji Support : Offers a searchable list of emojis. Email Interaction : Allows replying to notifications via email. Two-Factor Authentication : Enhances account security. Admin Dashboard : Provides essential community health metrics. Official Plugins : Supports various additional features. Comprehensive API : Facilitates broad functionality access. Open Source : Ensures transparency and adaptability. One-Click Upgrades : Simplifies version updates. Additional Features : Includes support for multiple languages, SEO optimization, various content formatting options, automatic backups, blog integration, and more","title":"Discourse"},{"location":"tools/Discourse/#discourse","text":"","title":"Discourse"},{"location":"tools/Discourse/#introduction","text":"Description : Discourse is a free, open-source forum software that plays a crucial role in modern tech environments by enabling thoughtful discussion and meaningful connections. It integrates seamlessly with various platforms, enhancing both application functionality and community engagement. Objective : The integration of Discourse can significantly benefit development projects by offering robust collaboration and communication tools. It enhances application security with features like spam blocking and two-factor authentication, ensuring a safe and productive environment for users.","title":"Introduction"},{"location":"tools/Discourse/#features-of-discourse","text":"List of Features : Conversations, Not Pages : Utilizes just-in-time loading for seamless conversation flow. Real-Time Chat : Enables informal chats alongside more structured discussions. AI Integration : Enhances user experience and moderation with AI capabilities. Customizable User Experience : Offers a personalized interface with adjustable preferences. Mobile Compatibility : Fully functional on high-resolution touch devices. Link Expansion : Automatically enhances shared links with additional context. Single Sign-On : Facilitates easy integration with existing login systems. Trust System : Rewards regular members with additional abilities. Community Moderation : Allows community-driven content moderation. Spam Blocking : Integrates Akismet spam protection. Social Login : Supports common social media logins. Topic Summarization : Condenses long discussions to highlights. Badges : Encourages positive behaviors with a badge system. Emoji Support : Offers a searchable list of emojis. Email Interaction : Allows replying to notifications via email. Two-Factor Authentication : Enhances account security. Admin Dashboard : Provides essential community health metrics. Official Plugins : Supports various additional features. Comprehensive API : Facilitates broad functionality access. Open Source : Ensures transparency and adaptability. One-Click Upgrades : Simplifies version updates. Additional Features : Includes support for multiple languages, SEO optimization, various content formatting options, automatic backups, blog integration, and more","title":"Features of Discourse"},{"location":"tools/Discourse/integration/","text":"Discourse Plugin Integration Guide with Keycloak SSO on Bitnami Discourse is a popular open-source discussion platform designed for modern community engagement. It is highly customizable, supporting numerous plugins for extended functionality. For detailed usage instructions and deployment options, refer to the Discourse User Manual . Integrating Discourse into the GREENGAGE engine amplifies its capabilities by enabling seamless Single Sign-On (SSO) authentication through Keycloak. This guide provides steps to integrate Discourse with Keycloak SSO, ensuring a smooth authentication process for users across various GREENGAGE services. Requesting Access Credentials To integrate Discourse with Keycloak SSO, you'll need to request access credentials. ( Follow this guide ) Receiving Credentials Following your request, you'll receive the necessary credentials via email. These credentials will enable the setup of SSO with Keycloak in your Discourse installation. Configuring Your Service Installing OpenID Connect Plugin for Keycloak Integration : Navigate to the Discourse installation directory: cd /opt/bitnami/discourse Install the OpenID Connect plugin: RAILS_ENV=production bundle exec rake plugin:install repo=https://github.com/discourse/discourse-openid-connect Precompile the assets for the plugin to take effect: RAILS_ENV=production bundle exec rake assets:precompile Detailed guide for installing plugins on Discourse over Bitnami can be found here . Disabling Email Verification : To disable the email verification typically sent upon first-time registration in Discourse, use the following plugin: cd /opt/bitnami/discourse/plugins git clone https://github.com/codergautam/disable-email-verification-discourse.git cd .. RAILS_ENV=production bundle install RAILS_ENV=production bundle exec rake assets:precompile These steps will set up Discourse for Keycloak SSO and disable the default email verification, aligning the authentication process with the Keycloak-managed user accounts. Step 4: Testing the Integration Test the SSO integration by attempting to log in to Discourse using credentials managed by Keycloak. For testing API integrations, verify if you can call the Discourse API using the token obtained from Keycloak. Support and Contact For additional support, please reach out to greengage.admin@deusto.es. Important Notes: Always keep your credentials confidential and secure. In case of any security concerns or compromised credentials, immediately contact the support team. Regularly update your integration setup to comply with the latest security standards and feature enhancements.","title":"Discourse Plugin Integration Guide with Keycloak SSO on Bitnami"},{"location":"tools/Discourse/integration/#discourse-plugin-integration-guide-with-keycloak-sso-on-bitnami","text":"Discourse is a popular open-source discussion platform designed for modern community engagement. It is highly customizable, supporting numerous plugins for extended functionality. For detailed usage instructions and deployment options, refer to the Discourse User Manual . Integrating Discourse into the GREENGAGE engine amplifies its capabilities by enabling seamless Single Sign-On (SSO) authentication through Keycloak. This guide provides steps to integrate Discourse with Keycloak SSO, ensuring a smooth authentication process for users across various GREENGAGE services. Requesting Access Credentials To integrate Discourse with Keycloak SSO, you'll need to request access credentials. ( Follow this guide ) Receiving Credentials Following your request, you'll receive the necessary credentials via email. These credentials will enable the setup of SSO with Keycloak in your Discourse installation. Configuring Your Service Installing OpenID Connect Plugin for Keycloak Integration : Navigate to the Discourse installation directory: cd /opt/bitnami/discourse Install the OpenID Connect plugin: RAILS_ENV=production bundle exec rake plugin:install repo=https://github.com/discourse/discourse-openid-connect Precompile the assets for the plugin to take effect: RAILS_ENV=production bundle exec rake assets:precompile Detailed guide for installing plugins on Discourse over Bitnami can be found here . Disabling Email Verification : To disable the email verification typically sent upon first-time registration in Discourse, use the following plugin: cd /opt/bitnami/discourse/plugins git clone https://github.com/codergautam/disable-email-verification-discourse.git cd .. RAILS_ENV=production bundle install RAILS_ENV=production bundle exec rake assets:precompile These steps will set up Discourse for Keycloak SSO and disable the default email verification, aligning the authentication process with the Keycloak-managed user accounts. Step 4: Testing the Integration Test the SSO integration by attempting to log in to Discourse using credentials managed by Keycloak. For testing API integrations, verify if you can call the Discourse API using the token obtained from Keycloak. Support and Contact For additional support, please reach out to greengage.admin@deusto.es. Important Notes: Always keep your credentials confidential and secure. In case of any security concerns or compromised credentials, immediately contact the support team. Regularly update your integration setup to comply with the latest security standards and feature enhancements.","title":"Discourse Plugin Integration Guide with Keycloak SSO on Bitnami"},{"location":"tools/Discourse/examples/","text":"Discourse Deployment Guide Integrating Discourse in Docker Environment Introduction Description : Discourse is a versatile forum software ideal for creating and managing online communities. Its significance in development lies in its ability to foster engagement and provide a platform for discussions. Objective : Deploying Discourse using Docker enhances application scalability, security, and ease of maintenance. Prerequisites Docker and Docker Compose installed. Basic knowledge of Docker and YAML syntax. Project Structure Source : Discourse Docker Compose Example Directory Layout : postgresql : PostgreSQL container configuration. redis : Redis container configuration. discourse : Main Discourse application container setup. sidekiq : Sidekiq container for background jobs. Step-by-Step Setup and Configuration Download or Clone the Docker Compose YAML File : Obtain the Docker Compose file for Discourse deployment. Replace Placeholders with Actual Values : [HERE_YOUR_DOMAIN] : Enter the domain where your Discourse instance will be accessible. [HERE_ADMIN_EMAIL] : Provide the email address for the Discourse administrator account. [HERE_ADMIN_USERNAME] : Set the username for the Discourse administrator. [HERE_ADMIN_PASSWORD] : Choose a secure password for the Discourse administrator account. [HERE_SMTP_HOST] : Specify the SMTP server host for sending emails. [HERE_SMTP_PORT] : Enter the port number used by your SMTP server. [HERE_SMTP_USER] : Provide the SMTP username for email authentication. [HERE_SMTP_PASSWORD] : Set the SMTP password for the email account. [HERE_SMTP_PROTOCOL] : Indicate the email protocol used by your SMTP server (e.g., TLS). Each of these placeholders corresponds to essential configuration settings for your Discourse deployment, ensuring proper functionality and access to the application. Running the Services Run docker-compose up to start all services. Accessing the Application Access Discourse at the specified domain after successful deployment. Additional Configuration (Optional) Integrating Keycloak for Single Sign-On (SSO) : To enable SSO with Keycloak in Discourse: cd /opt/bitnami/discourse RAILS_ENV=production bundle exec rake plugin:install repo=https://github.com/discourse/discourse-openid-connect RAILS_ENV=production bundle exec rake assets:precompile This configures Discourse to use Keycloak as the authentication provider, allowing seamless login across connected services. Disabling Email Verification : Given that Keycloak is used for user management, you might want to disable the default email verification sent by Discourse: cd /opt/bitnami/discourse/plugins git clone https://github.com/codergautam/disable-email-verification-discourse.git cd .. RAILS_ENV=production bundle install RAILS_ENV=production bundle exec rake assets:precompile This step ensures that the email verification step is bypassed, streamlining the registration process in a Keycloak-managed environment. Troubleshooting Issues with User and Site Configuration : If you encounter problems related to configuring users or site settings in your Docker-deployed Discourse instance, refer to the Bitnami Discourse Container Configuration Guide . This resource provides detailed instructions and examples for configuring various aspects of the Discourse application within a Docker environment. General Docker Deployment Issues : For common Docker-related problems, such as container networking, volume management, or image retrieval issues, consult Docker's official documentation or community forums for guidance and solutions. Support and Additional Resources Support Channels : Community forums, email support. External Documentation : Docker Documentation , Discourse Official Documentation . Docker Integration for Discourse Introduction This section focuses on integrating the Discourse forum software within a Docker containerized environment, leveraging Docker's capabilities for efficient deployment and management. Prerequisites Knowledge of Docker and Docker Compose. Familiarity with YAML syntax. Project Structure The Docker Compose file defines multiple services: postgresql for the database, redis for caching, discourse for the main application, and sidekiq for background jobs. The volumes and networks sections manage data persistence and inter-container communication. Steps for Integration Clone or create a Docker Compose file similar to the provided example. Customize the environment variables and service settings as per your requirements. Running the Example Execute docker-compose up in the directory containing your Docker Compose file to start the Discourse environment. Accessing the Application Once the containers are running, access Discourse through the specified domain configured in the DISCOURSE_HOST environment variable of the discourse service. For specific details, always refer to the latest official documentation of the tools involved.","title":"Discourse Deployment Guide"},{"location":"tools/Discourse/examples/#discourse-deployment-guide","text":"","title":"Discourse Deployment Guide"},{"location":"tools/Discourse/examples/#integrating-discourse-in-docker-environment","text":"","title":"Integrating Discourse in Docker Environment"},{"location":"tools/Discourse/examples/#introduction","text":"Description : Discourse is a versatile forum software ideal for creating and managing online communities. Its significance in development lies in its ability to foster engagement and provide a platform for discussions. Objective : Deploying Discourse using Docker enhances application scalability, security, and ease of maintenance.","title":"Introduction"},{"location":"tools/Discourse/examples/#prerequisites","text":"Docker and Docker Compose installed. Basic knowledge of Docker and YAML syntax.","title":"Prerequisites"},{"location":"tools/Discourse/examples/#project-structure","text":"Source : Discourse Docker Compose Example Directory Layout : postgresql : PostgreSQL container configuration. redis : Redis container configuration. discourse : Main Discourse application container setup. sidekiq : Sidekiq container for background jobs.","title":"Project Structure"},{"location":"tools/Discourse/examples/#step-by-step-setup-and-configuration","text":"Download or Clone the Docker Compose YAML File : Obtain the Docker Compose file for Discourse deployment. Replace Placeholders with Actual Values : [HERE_YOUR_DOMAIN] : Enter the domain where your Discourse instance will be accessible. [HERE_ADMIN_EMAIL] : Provide the email address for the Discourse administrator account. [HERE_ADMIN_USERNAME] : Set the username for the Discourse administrator. [HERE_ADMIN_PASSWORD] : Choose a secure password for the Discourse administrator account. [HERE_SMTP_HOST] : Specify the SMTP server host for sending emails. [HERE_SMTP_PORT] : Enter the port number used by your SMTP server. [HERE_SMTP_USER] : Provide the SMTP username for email authentication. [HERE_SMTP_PASSWORD] : Set the SMTP password for the email account. [HERE_SMTP_PROTOCOL] : Indicate the email protocol used by your SMTP server (e.g., TLS). Each of these placeholders corresponds to essential configuration settings for your Discourse deployment, ensuring proper functionality and access to the application.","title":"Step-by-Step Setup and Configuration"},{"location":"tools/Discourse/examples/#running-the-services","text":"Run docker-compose up to start all services.","title":"Running the Services"},{"location":"tools/Discourse/examples/#accessing-the-application","text":"Access Discourse at the specified domain after successful deployment.","title":"Accessing the Application"},{"location":"tools/Discourse/examples/#additional-configuration-optional","text":"Integrating Keycloak for Single Sign-On (SSO) : To enable SSO with Keycloak in Discourse: cd /opt/bitnami/discourse RAILS_ENV=production bundle exec rake plugin:install repo=https://github.com/discourse/discourse-openid-connect RAILS_ENV=production bundle exec rake assets:precompile This configures Discourse to use Keycloak as the authentication provider, allowing seamless login across connected services. Disabling Email Verification : Given that Keycloak is used for user management, you might want to disable the default email verification sent by Discourse: cd /opt/bitnami/discourse/plugins git clone https://github.com/codergautam/disable-email-verification-discourse.git cd .. RAILS_ENV=production bundle install RAILS_ENV=production bundle exec rake assets:precompile This step ensures that the email verification step is bypassed, streamlining the registration process in a Keycloak-managed environment.","title":"Additional Configuration (Optional)"},{"location":"tools/Discourse/examples/#troubleshooting","text":"Issues with User and Site Configuration : If you encounter problems related to configuring users or site settings in your Docker-deployed Discourse instance, refer to the Bitnami Discourse Container Configuration Guide . This resource provides detailed instructions and examples for configuring various aspects of the Discourse application within a Docker environment. General Docker Deployment Issues : For common Docker-related problems, such as container networking, volume management, or image retrieval issues, consult Docker's official documentation or community forums for guidance and solutions.","title":"Troubleshooting"},{"location":"tools/Discourse/examples/#support-and-additional-resources","text":"Support Channels : Community forums, email support. External Documentation : Docker Documentation , Discourse Official Documentation .","title":"Support and Additional Resources"},{"location":"tools/Discourse/examples/#docker-integration-for-discourse","text":"","title":"Docker Integration for Discourse"},{"location":"tools/Discourse/examples/#introduction_1","text":"This section focuses on integrating the Discourse forum software within a Docker containerized environment, leveraging Docker's capabilities for efficient deployment and management.","title":"Introduction"},{"location":"tools/Discourse/examples/#prerequisites_1","text":"Knowledge of Docker and Docker Compose. Familiarity with YAML syntax.","title":"Prerequisites"},{"location":"tools/Discourse/examples/#project-structure_1","text":"The Docker Compose file defines multiple services: postgresql for the database, redis for caching, discourse for the main application, and sidekiq for background jobs. The volumes and networks sections manage data persistence and inter-container communication.","title":"Project Structure"},{"location":"tools/Discourse/examples/#steps-for-integration","text":"Clone or create a Docker Compose file similar to the provided example. Customize the environment variables and service settings as per your requirements.","title":"Steps for Integration"},{"location":"tools/Discourse/examples/#running-the-example","text":"Execute docker-compose up in the directory containing your Docker Compose file to start the Discourse environment.","title":"Running the Example"},{"location":"tools/Discourse/examples/#accessing-the-application_1","text":"Once the containers are running, access Discourse through the specified domain configured in the DISCOURSE_HOST environment variable of the discourse service. For specific details, always refer to the latest official documentation of the tools involved.","title":"Accessing the Application"},{"location":"tools/TOOLNAME_template/","text":"Tool Name Introduction Description: Provide an overview of the tool's purpose and its importance in modern tech environments. Objective: Explain how integrating this tool can benefit development and enhance application security or functionality. Features of [Tool Name] List of Features: Outline key features and capabilities of the tool, emphasizing how each contributes to its effectiveness and utility in development projects. Use Case Scenario Description: Detail a typical use case scenario, demonstrating how the tool operates within a workflow or system. (if possible) Visual Aid: Include a diagram or flowchart to illustrate the use case, making it easier to understand the tool's application in a real-world scenario. References Description: list here a set of external references where User Manual of further information to tool is provide. It is suggested to include: a) User manual link and b) Link to PDF of description of tool in GREENGAGE catalogue User manual GREENGAGE catalogue entry","title":"Tool Name"},{"location":"tools/TOOLNAME_template/#tool-name","text":"","title":"Tool Name"},{"location":"tools/TOOLNAME_template/#introduction","text":"Description: Provide an overview of the tool's purpose and its importance in modern tech environments. Objective: Explain how integrating this tool can benefit development and enhance application security or functionality.","title":"Introduction"},{"location":"tools/TOOLNAME_template/#features-of-tool-name","text":"List of Features: Outline key features and capabilities of the tool, emphasizing how each contributes to its effectiveness and utility in development projects.","title":"Features of [Tool Name]"},{"location":"tools/TOOLNAME_template/#use-case-scenario","text":"Description: Detail a typical use case scenario, demonstrating how the tool operates within a workflow or system. (if possible) Visual Aid: Include a diagram or flowchart to illustrate the use case, making it easier to understand the tool's application in a real-world scenario.","title":"Use Case Scenario"},{"location":"tools/TOOLNAME_template/#references","text":"Description: list here a set of external references where User Manual of further information to tool is provide. It is suggested to include: a) User manual link and b) Link to PDF of description of tool in GREENGAGE catalogue User manual GREENGAGE catalogue entry","title":"References"},{"location":"tools/TOOLNAME_template/integration/","text":"[Tool Name] Usage & Integration Guide for Third-Party Services Generic usage instructions for [Tool Name] - Instructions on how to use the tool. - In case there is external user manual make a small summary and refer to it. - Otherwise, provide enough explanations and snapshots to let a user get started with the tool usage. Include links to URLs where the tool might be deployed. Introduction explaining the importance of integrating [Tool Name] to make your tool part of the GREENGAGE engine. This implies that your tool has to use the single sign on (SSO) service provided by GREENGAGE. However, you should also describe not only how your tool is integrated with GREENGAGE's SSO but also how other GREENGAGE tools may integrate with your tools. Feel free to add and rename the integrate steps listed below: Step 1: Requesting Access Credentials Instructions on how to request access credentials. Contact details (e.g., email address) for credential requests. Information to be provided in the request (e.g., tool name, description, contact details). Step 2: Receiving Credentials Process description after the request is made. Information on how credentials will be received. Step 3: Configuring Your Service Steps to incorporate the received credentials into the service. Guidelines on setting up the service for proper communication with the [Tool Name]. Step 4: Testing the Integration Instructions for testing the integration to ensure everything is set up correctly. Contact information for support in case of issues during testing. Support and Contact Additional support contact details. Encouragement to reach out with questions or for further assistance. Important Notes: Security reminders and best practices (e.g., keeping credentials confidential). Instructions on what to do if credentials are compromised.","title":"[Tool Name] Usage & Integration Guide for Third-Party Services"},{"location":"tools/TOOLNAME_template/integration/#tool-name-usage-integration-guide-for-third-party-services","text":"Generic usage instructions for [Tool Name] - Instructions on how to use the tool. - In case there is external user manual make a small summary and refer to it. - Otherwise, provide enough explanations and snapshots to let a user get started with the tool usage. Include links to URLs where the tool might be deployed. Introduction explaining the importance of integrating [Tool Name] to make your tool part of the GREENGAGE engine. This implies that your tool has to use the single sign on (SSO) service provided by GREENGAGE. However, you should also describe not only how your tool is integrated with GREENGAGE's SSO but also how other GREENGAGE tools may integrate with your tools. Feel free to add and rename the integrate steps listed below: Step 1: Requesting Access Credentials Instructions on how to request access credentials. Contact details (e.g., email address) for credential requests. Information to be provided in the request (e.g., tool name, description, contact details). Step 2: Receiving Credentials Process description after the request is made. Information on how credentials will be received. Step 3: Configuring Your Service Steps to incorporate the received credentials into the service. Guidelines on setting up the service for proper communication with the [Tool Name]. Step 4: Testing the Integration Instructions for testing the integration to ensure everything is set up correctly. Contact information for support in case of issues during testing. Support and Contact Additional support contact details. Encouragement to reach out with questions or for further assistance. Important Notes: Security reminders and best practices (e.g., keeping credentials confidential). Instructions on what to do if credentials are compromised.","title":"[Tool Name] Usage &amp; Integration Guide for Third-Party Services"},{"location":"tools/TOOLNAME_template/examples/","text":"[Tool Name] Integrating [Tool Name] in [Environment] Introduction Description: Provide an overview of the tool's purpose and significance in development. Objective: Explain how this tool enhances the application's functionality or security. Prerequisites List the prerequisites needed to use the tool effectively. Project Structure Source: Link to the example project. Directory Layout: Describe the structure of the example project. Step-by-Step Setup and Configuration Outline the steps to set up the tool in the chosen environment. Detailed Configuration Steps Provide detailed instructions for each configuration file and setting. Running the Services Instructions for starting the services on different operating systems. Accessing the Application Guide on how to access the application once it is running. Additional Configuration (Optional) Discuss any optional configuration steps for advanced users. Troubleshooting Provide common troubleshooting steps or issues and their solutions. Support and Additional Resources Support Channels: List available support channels. External Documentation: Link to the official documentation or other helpful resources. [Language/Framework Integration] (e.g., Python, Node.js) Introduction Brief introduction to integrating the tool with the specific language or framework. Prerequisites List any prerequisites specific to this language/framework. Project Structure Describe the structure of the example integration project. Steps for Integration Step-by-step guide for integrating the tool into projects using this language/framework. Running the Example Instructions on how to run the example project. Accessing the Application How to access and test the application once it's running. Replace sections marked with [brackets] with the relevant information for your specific tool and environment.","title":"[Tool Name]"},{"location":"tools/TOOLNAME_template/examples/#tool-name","text":"","title":"[Tool Name]"},{"location":"tools/TOOLNAME_template/examples/#integrating-tool-name-in-environment","text":"","title":"Integrating [Tool Name] in [Environment]"},{"location":"tools/TOOLNAME_template/examples/#introduction","text":"Description: Provide an overview of the tool's purpose and significance in development. Objective: Explain how this tool enhances the application's functionality or security.","title":"Introduction"},{"location":"tools/TOOLNAME_template/examples/#prerequisites","text":"List the prerequisites needed to use the tool effectively.","title":"Prerequisites"},{"location":"tools/TOOLNAME_template/examples/#project-structure","text":"Source: Link to the example project. Directory Layout: Describe the structure of the example project.","title":"Project Structure"},{"location":"tools/TOOLNAME_template/examples/#step-by-step-setup-and-configuration","text":"Outline the steps to set up the tool in the chosen environment.","title":"Step-by-Step Setup and Configuration"},{"location":"tools/TOOLNAME_template/examples/#detailed-configuration-steps","text":"Provide detailed instructions for each configuration file and setting.","title":"Detailed Configuration Steps"},{"location":"tools/TOOLNAME_template/examples/#running-the-services","text":"Instructions for starting the services on different operating systems.","title":"Running the Services"},{"location":"tools/TOOLNAME_template/examples/#accessing-the-application","text":"Guide on how to access the application once it is running.","title":"Accessing the Application"},{"location":"tools/TOOLNAME_template/examples/#additional-configuration-optional","text":"Discuss any optional configuration steps for advanced users.","title":"Additional Configuration (Optional)"},{"location":"tools/TOOLNAME_template/examples/#troubleshooting","text":"Provide common troubleshooting steps or issues and their solutions.","title":"Troubleshooting"},{"location":"tools/TOOLNAME_template/examples/#support-and-additional-resources","text":"Support Channels: List available support channels. External Documentation: Link to the official documentation or other helpful resources.","title":"Support and Additional Resources"},{"location":"tools/TOOLNAME_template/examples/#languageframework-integration-eg-python-nodejs","text":"","title":"[Language/Framework Integration] (e.g., Python, Node.js)"},{"location":"tools/TOOLNAME_template/examples/#introduction_1","text":"Brief introduction to integrating the tool with the specific language or framework.","title":"Introduction"},{"location":"tools/TOOLNAME_template/examples/#prerequisites_1","text":"List any prerequisites specific to this language/framework.","title":"Prerequisites"},{"location":"tools/TOOLNAME_template/examples/#project-structure_1","text":"Describe the structure of the example integration project.","title":"Project Structure"},{"location":"tools/TOOLNAME_template/examples/#steps-for-integration","text":"Step-by-step guide for integrating the tool into projects using this language/framework.","title":"Steps for Integration"},{"location":"tools/TOOLNAME_template/examples/#running-the-example","text":"Instructions on how to run the example project.","title":"Running the Example"},{"location":"tools/TOOLNAME_template/examples/#accessing-the-application_1","text":"How to access and test the application once it's running. Replace sections marked with [brackets] with the relevant information for your specific tool and environment.","title":"Accessing the Application"},{"location":"tools/collaborativeEnvironment/","text":"Collaborative Environment Introduction Description The Collaborative Environment, developed as part of the INTERLINK project, serves as a foundational tool for Citizen Observatories. It leverages cutting-edge frontend technologies to enable effective collaboration and intuitive user interaction. Its modular design, based on functional React components, ensures easy expansion and maintenance, adapting to the evolving needs of Citizen Observatories. Objective Integrating the Collaborative Environment into development projects can significantly enhance team communication, document collaboration, and stakeholder engagement. Its capabilities in process planning and promoting loyalty through incentives and rewards make it an invaluable asset for co-production in diverse fields. Features of Collaborative Environment List of Features: Internationalization Support : Utilizes \"react-i18next\" for advanced translation features, catering to a global user base. Flexible Development : Offers well-documented examples for easy addition of new elements and routes, supporting agile development practices. State Management : Employs Redux for effective global state management and React's Context API for smooth data flow across components. Inclusivity and Diversity : Focus on internationalization promotes access and usability across diverse user groups. Modular Structure : Facilitates easy expansion and maintenance, adapting to changing requirements. Efficient Data Handling : Uses a microservices architecture with RESTful interfaces, ensuring scalability and performance. Security Features : Includes authentication through OpenID Connect (OIDC) and secure data transfer protocols. Gamification Elements : Incorporates a gamification engine to incentivize user participation and engagement. Use Case Scenario Description A typical use case for the Collaborative Environment involves setting up and managing a co-production process within a Citizen Observatory. Users can create organizations, form teams, and develop processes, each with clearly defined phases, objectives, and tasks. The platform allows for resource management, tracking progress, and engaging communities effectively. It also supports integration with external tools, enhancing its utility in diverse scenarios. References User manual of Collaborative Environment Documentation in GREENGAGE catalogue","title":"Collaborative Environment"},{"location":"tools/collaborativeEnvironment/#collaborative-environment","text":"","title":"Collaborative Environment"},{"location":"tools/collaborativeEnvironment/#introduction","text":"","title":"Introduction"},{"location":"tools/collaborativeEnvironment/#description","text":"The Collaborative Environment, developed as part of the INTERLINK project, serves as a foundational tool for Citizen Observatories. It leverages cutting-edge frontend technologies to enable effective collaboration and intuitive user interaction. Its modular design, based on functional React components, ensures easy expansion and maintenance, adapting to the evolving needs of Citizen Observatories.","title":"Description"},{"location":"tools/collaborativeEnvironment/#objective","text":"Integrating the Collaborative Environment into development projects can significantly enhance team communication, document collaboration, and stakeholder engagement. Its capabilities in process planning and promoting loyalty through incentives and rewards make it an invaluable asset for co-production in diverse fields.","title":"Objective"},{"location":"tools/collaborativeEnvironment/#features-of-collaborative-environment","text":"","title":"Features of Collaborative Environment"},{"location":"tools/collaborativeEnvironment/#list-of-features","text":"Internationalization Support : Utilizes \"react-i18next\" for advanced translation features, catering to a global user base. Flexible Development : Offers well-documented examples for easy addition of new elements and routes, supporting agile development practices. State Management : Employs Redux for effective global state management and React's Context API for smooth data flow across components. Inclusivity and Diversity : Focus on internationalization promotes access and usability across diverse user groups. Modular Structure : Facilitates easy expansion and maintenance, adapting to changing requirements. Efficient Data Handling : Uses a microservices architecture with RESTful interfaces, ensuring scalability and performance. Security Features : Includes authentication through OpenID Connect (OIDC) and secure data transfer protocols. Gamification Elements : Incorporates a gamification engine to incentivize user participation and engagement.","title":"List of Features:"},{"location":"tools/collaborativeEnvironment/#use-case-scenario","text":"","title":"Use Case Scenario"},{"location":"tools/collaborativeEnvironment/#description_1","text":"A typical use case for the Collaborative Environment involves setting up and managing a co-production process within a Citizen Observatory. Users can create organizations, form teams, and develop processes, each with clearly defined phases, objectives, and tasks. The platform allows for resource management, tracking progress, and engaging communities effectively. It also supports integration with external tools, enhancing its utility in diverse scenarios.","title":"Description"},{"location":"tools/collaborativeEnvironment/#references","text":"User manual of Collaborative Environment Documentation in GREENGAGE catalogue","title":"References"},{"location":"tools/collaborativeEnvironment/integration/","text":"Collaborative Environment Usage Guide for Third-Party Services This guide explains the process for integrating third-party services into the Collaborative Environment, a platform centralizing organizational management and enhancing collaborative workflows. Initial Login and registration Create an account either by registering with an email or using a Google account at Collaborative Environment > Login. Organization and Team Setup After logging in, navigate to Organizations tab > Create new organization. Fill in the necessary details to create your organization. To create a team, ensure you have an existing organization. In the Organizations tab, select your organization and click on Create new team. Define your team, stakeholders, and invite members. Process Creation and Configuration From the dashboard, click on Go to process list then on Create new process and fill in the required fields. Configure your process by selecting it from the process list and accessing the Overview section. This includes setting up various sections like Front Page, Overview, Resources, Guide, Leaderboard, Workplan, Team, and Settings. Define specific elements like Type, Schema, Organizations, Resources, Permissions, Rewards, and Finish in the roadmap for the process. Development and Completion of the Co-Production Process Begin the co-production process by selecting tasks from the Guide section. Manage phases, objectives, and tasks, including descriptions, statuses, planning times, and permissions. Once the process is complete, change the \"State of the project\" to \"Finished\" in Settings. Support and Contact For additional support or assistance with the integration process, please refer to the Collaborative Environment\u2019s online resources, or contact via https://github.com/Greengage-project/interlink-project/issues . (Technical Issues) Important Notes: Adhere to security best practices throughout the integration process. Regularly update and maintain your service for compatibility with the Collaborative Environment. For further assistance or troubleshooting, refer to the Collaborative Environment\u2019s resources or community forums. By following these steps, third-party services can be effectively integrated into the Collaborative Environment, leveraging its capabilities to improve organizational efficiency and collaboration. References Further details about the usage of this tool are available in the following references: - Collaborative Environment's Usage Manual","title":"Collaborative Environment Usage Guide for Third-Party Services"},{"location":"tools/collaborativeEnvironment/integration/#collaborative-environment-usage-guide-for-third-party-services","text":"This guide explains the process for integrating third-party services into the Collaborative Environment, a platform centralizing organizational management and enhancing collaborative workflows. Initial Login and registration Create an account either by registering with an email or using a Google account at Collaborative Environment > Login. Organization and Team Setup After logging in, navigate to Organizations tab > Create new organization. Fill in the necessary details to create your organization. To create a team, ensure you have an existing organization. In the Organizations tab, select your organization and click on Create new team. Define your team, stakeholders, and invite members. Process Creation and Configuration From the dashboard, click on Go to process list then on Create new process and fill in the required fields. Configure your process by selecting it from the process list and accessing the Overview section. This includes setting up various sections like Front Page, Overview, Resources, Guide, Leaderboard, Workplan, Team, and Settings. Define specific elements like Type, Schema, Organizations, Resources, Permissions, Rewards, and Finish in the roadmap for the process. Development and Completion of the Co-Production Process Begin the co-production process by selecting tasks from the Guide section. Manage phases, objectives, and tasks, including descriptions, statuses, planning times, and permissions. Once the process is complete, change the \"State of the project\" to \"Finished\" in Settings. Support and Contact For additional support or assistance with the integration process, please refer to the Collaborative Environment\u2019s online resources, or contact via https://github.com/Greengage-project/interlink-project/issues . (Technical Issues) Important Notes: Adhere to security best practices throughout the integration process. Regularly update and maintain your service for compatibility with the Collaborative Environment. For further assistance or troubleshooting, refer to the Collaborative Environment\u2019s resources or community forums. By following these steps, third-party services can be effectively integrated into the Collaborative Environment, leveraging its capabilities to improve organizational efficiency and collaboration.","title":"Collaborative Environment Usage Guide for Third-Party Services"},{"location":"tools/collaborativeEnvironment/integration/#references","text":"Further details about the usage of this tool are available in the following references: - Collaborative Environment's Usage Manual","title":"References"},{"location":"tools/collaborativeEnvironment/examples/","text":"Collaborative Environment Installation Guide Deploying Collaborative Environment This guide provides detailed instructions for setting up the Collaborative Environment using Docker and Docker Compose. Prerequisites Docker and Docker Compose installed. Node.js and npm installed for Node.js integration. Python and pip installed for Python integration. Basic understanding of the Collaborative Environment, Docker, and the chosen programming language (Node.js or Python). Project Structure The project structure includes various components like Docker configurations, backend services, interlinkers, and frontend applications. The main components are located in different repositories, which can be cloned for development purposes. Collaborative Environment/ \u251c\u2500\u2500 docker/ # Docker configurations for various services \u251c\u2500\u2500 docs/ # Documentation files \u251c\u2500\u2500 envs/ # Environment configurations \u251c\u2500\u2500 Makefile # Makefile for automating setup and deployment ... Step 1: Cloning Repositories and Setting Up the Environment Use the Makefile to clone all necessary components: make setup This will clone repositories such as: Frontend application Backend services (auth, catalogue, coproduction, logging) Interlinkers (survey, googledrive, ceditor) Gamification service Step 2: Building and Starting Services Build the containers for all services: make build Start the services, set up the environment, and seed the database: make up Step 3: Accessing the Application Once all services are up and running, access the Collaborative Environment's frontend through the configured local or development URL. Additional Commands Stopping Services : To stop all containers and remove volumes, use make down . Pulling Latest Changes : To update all components, use make pull . Restarting Services : To restart containers, apply migrations and seed data, use make restart . Environment Variables Setup : To configure environment variables, use make envVariables . Troubleshooting Common issues can arise during the setup, such as Docker network conflicts or missing dependencies. Refer to the README.md files in each repository for specific troubleshooting steps. Support and Additional Resources For further assistance, refer to the detailed documentation in the docs/ directory or visit the official Collaborative Environment Documentation . Additional support can be obtained through community forums or by contacting the Collaborative Environment support team.","title":"Collaborative Environment Installation Guide"},{"location":"tools/collaborativeEnvironment/examples/#collaborative-environment-installation-guide","text":"","title":"Collaborative Environment Installation Guide"},{"location":"tools/collaborativeEnvironment/examples/#deploying-collaborative-environment","text":"This guide provides detailed instructions for setting up the Collaborative Environment using Docker and Docker Compose.","title":"Deploying Collaborative Environment"},{"location":"tools/collaborativeEnvironment/examples/#prerequisites","text":"Docker and Docker Compose installed. Node.js and npm installed for Node.js integration. Python and pip installed for Python integration. Basic understanding of the Collaborative Environment, Docker, and the chosen programming language (Node.js or Python).","title":"Prerequisites"},{"location":"tools/collaborativeEnvironment/examples/#project-structure","text":"The project structure includes various components like Docker configurations, backend services, interlinkers, and frontend applications. The main components are located in different repositories, which can be cloned for development purposes. Collaborative Environment/ \u251c\u2500\u2500 docker/ # Docker configurations for various services \u251c\u2500\u2500 docs/ # Documentation files \u251c\u2500\u2500 envs/ # Environment configurations \u251c\u2500\u2500 Makefile # Makefile for automating setup and deployment ...","title":"Project Structure"},{"location":"tools/collaborativeEnvironment/examples/#step-1-cloning-repositories-and-setting-up-the-environment","text":"Use the Makefile to clone all necessary components: make setup This will clone repositories such as: Frontend application Backend services (auth, catalogue, coproduction, logging) Interlinkers (survey, googledrive, ceditor) Gamification service","title":"Step 1: Cloning Repositories and Setting Up the Environment"},{"location":"tools/collaborativeEnvironment/examples/#step-2-building-and-starting-services","text":"Build the containers for all services: make build Start the services, set up the environment, and seed the database: make up","title":"Step 2: Building and Starting Services"},{"location":"tools/collaborativeEnvironment/examples/#step-3-accessing-the-application","text":"Once all services are up and running, access the Collaborative Environment's frontend through the configured local or development URL.","title":"Step 3: Accessing the Application"},{"location":"tools/collaborativeEnvironment/examples/#additional-commands","text":"Stopping Services : To stop all containers and remove volumes, use make down . Pulling Latest Changes : To update all components, use make pull . Restarting Services : To restart containers, apply migrations and seed data, use make restart . Environment Variables Setup : To configure environment variables, use make envVariables .","title":"Additional Commands"},{"location":"tools/collaborativeEnvironment/examples/#troubleshooting","text":"Common issues can arise during the setup, such as Docker network conflicts or missing dependencies. Refer to the README.md files in each repository for specific troubleshooting steps.","title":"Troubleshooting"},{"location":"tools/collaborativeEnvironment/examples/#support-and-additional-resources","text":"For further assistance, refer to the detailed documentation in the docs/ directory or visit the official Collaborative Environment Documentation . Additional support can be obtained through community forums or by contacting the Collaborative Environment support team.","title":"Support and Additional Resources"},{"location":"tools/datahub/","text":"DataHub DataHub Description Introduction Description DataHub is an open-source metadata platform designed to democratise data discovery. It acts as a centralised system for companies and teams to track, manage, and find data within their organisation. Its primary aim is to streamline the process of accessing and understanding data assets, making it easier for teams to derive insights and make informed decisions. Objective The primary objective of DataHub is to facilitate better data discovery and management. By offering a unified view of an organisation's data assets, it aims to enhance collaboration, data governance, and compliance, while reducing the time and effort spent in locating and understanding data resources. Features of Datahub Metadata Management: DataHub catalogs metadata from various data sources, offering insights into data quality, usage, and lineage. Search and Discovery: It provides powerful search capabilities, allowing users to quickly find datasets and understand their context. Data Lineage: Visual representation of data lineage helps in understanding how data is transformed and where it flows within the organisation. Data Observability: Monitors data health, alerting users to issues related to data quality and freshness. Access Control: Supports granular access control to manage who can view or edit different data assets. Extensibility: Offers APIs and an extensible architecture, allowing for customisation and integration with other tools and systems. Use Case Scenario DataHub is employed to bridge the gap between citisens, scientists, and open data portals. Pilots have a wealth of open data available \u2013 traffic patterns, pollution levels, public space usage, and more. With DataHub, the city creates a centralised, user-friendly platform where all this data is cataloged and made easily searchable. Citisens and local scientists can access this data to understand various aspects of the city's dynamics. For instance, a group of citisens concerned about air quality uses DataHub to find and analyse pollution data. Furthermore, DataHub can be used to track the lineage of data, allowing users to understand how data is transformed and where it flows within the organisation. This helps in ensuring data quality and compliance with regulations such as GDPR. References Documentation GREENGAGE catalogue entry","title":"DataHub"},{"location":"tools/datahub/#datahub","text":"","title":"DataHub"},{"location":"tools/datahub/#datahub-description","text":"","title":"DataHub Description"},{"location":"tools/datahub/#introduction","text":"","title":"Introduction"},{"location":"tools/datahub/#description","text":"DataHub is an open-source metadata platform designed to democratise data discovery. It acts as a centralised system for companies and teams to track, manage, and find data within their organisation. Its primary aim is to streamline the process of accessing and understanding data assets, making it easier for teams to derive insights and make informed decisions.","title":"Description"},{"location":"tools/datahub/#objective","text":"The primary objective of DataHub is to facilitate better data discovery and management. By offering a unified view of an organisation's data assets, it aims to enhance collaboration, data governance, and compliance, while reducing the time and effort spent in locating and understanding data resources.","title":"Objective"},{"location":"tools/datahub/#features-of-datahub","text":"Metadata Management: DataHub catalogs metadata from various data sources, offering insights into data quality, usage, and lineage. Search and Discovery: It provides powerful search capabilities, allowing users to quickly find datasets and understand their context. Data Lineage: Visual representation of data lineage helps in understanding how data is transformed and where it flows within the organisation. Data Observability: Monitors data health, alerting users to issues related to data quality and freshness. Access Control: Supports granular access control to manage who can view or edit different data assets. Extensibility: Offers APIs and an extensible architecture, allowing for customisation and integration with other tools and systems.","title":"Features of Datahub"},{"location":"tools/datahub/#use-case-scenario","text":"DataHub is employed to bridge the gap between citisens, scientists, and open data portals. Pilots have a wealth of open data available \u2013 traffic patterns, pollution levels, public space usage, and more. With DataHub, the city creates a centralised, user-friendly platform where all this data is cataloged and made easily searchable. Citisens and local scientists can access this data to understand various aspects of the city's dynamics. For instance, a group of citisens concerned about air quality uses DataHub to find and analyse pollution data. Furthermore, DataHub can be used to track the lineage of data, allowing users to understand how data is transformed and where it flows within the organisation. This helps in ensuring data quality and compliance with regulations such as GDPR.","title":"Use Case Scenario"},{"location":"tools/datahub/#references","text":"Documentation GREENGAGE catalogue entry","title":"References"},{"location":"tools/datahub/usage/","text":"DataHub Usage Guide To provide a centralised location for all data assets, we are deploying DataHub in a centralised manner. This means that all data assets will be catalogued in a single instance of DataHub. This usage guide provide instructions on how to use DataHub. Step 1: Requesting Credentials Access to DataHub using your Google account or the account created for Greengage project and registered in Keycloak. Send an email to ruben.sanchez@deusto.es with the subject line: \"[GREENGAGE] Request for rights in DataHub\" including the following information: Tool name. Contact name:(for technical propose) Contact email:(for technical propose) Once you have the credentials and the right access rights you can start using DataHub. Data Discovery and Search You can search for any asset located in DataHub using the search bar located on the main page or at the top of any other page within DataHub. The search bar, as shown in the figure below, enables users to quickly find relevant datasets, metrics, and dashboards. Metadata Management DataHub offers comprehensive metadata management capabilities. It aggregates and manages metadata from a variety of data sources, maintaining details about datasets, schemas, data lineage, and ownership. This feature is illustrated in the Metadata Management interface, providing a clear overview of all metadata assets. Data Lineage and Governance DataHub visualises data lineage, showing how data flows through various systems. This feature is crucial for understanding the transformation and lifecycle of data, aiding in governance and compliance. The Data Lineage interface provides a graphical representation of these data flows. Collaboration and Documentation The platform enhances collaboration among data teams by allowing for the documentation and sharing of insights about datasets. The Collaboration and Documentation interface facilitates this exchange, fostering a data-informed culture.","title":"DataHub Usage Guide"},{"location":"tools/datahub/usage/#datahub-usage-guide","text":"To provide a centralised location for all data assets, we are deploying DataHub in a centralised manner. This means that all data assets will be catalogued in a single instance of DataHub. This usage guide provide instructions on how to use DataHub. Step 1: Requesting Credentials Access to DataHub using your Google account or the account created for Greengage project and registered in Keycloak. Send an email to ruben.sanchez@deusto.es with the subject line: \"[GREENGAGE] Request for rights in DataHub\" including the following information: Tool name. Contact name:(for technical propose) Contact email:(for technical propose) Once you have the credentials and the right access rights you can start using DataHub. Data Discovery and Search You can search for any asset located in DataHub using the search bar located on the main page or at the top of any other page within DataHub. The search bar, as shown in the figure below, enables users to quickly find relevant datasets, metrics, and dashboards. Metadata Management DataHub offers comprehensive metadata management capabilities. It aggregates and manages metadata from a variety of data sources, maintaining details about datasets, schemas, data lineage, and ownership. This feature is illustrated in the Metadata Management interface, providing a clear overview of all metadata assets. Data Lineage and Governance DataHub visualises data lineage, showing how data flows through various systems. This feature is crucial for understanding the transformation and lifecycle of data, aiding in governance and compliance. The Data Lineage interface provides a graphical representation of these data flows. Collaboration and Documentation The platform enhances collaboration among data teams by allowing for the documentation and sharing of insights about datasets. The Collaboration and Documentation interface facilitates this exchange, fostering a data-informed culture.","title":"DataHub Usage Guide"},{"location":"tools/datahub/examples/","text":"DataHub example of Integration Creating an Ingestion Source Introduction In this example we will show how to create an ingestion source that pulls data from the desired platform to DataHub. Prerequisites Access to DataHub Access to the platform to be integrated with DataHub Check if the platform to be integrated with DataHub is already supported by Datahub. List of ingestion sources Creating the recipe 1) Access to DataHub 2) Go to the Ingestion view and click in create new source. 3) Select the platform to be integrated with DataHub. 4) Fill the form with the information of the platform to be integrated with DataHub. Some of the platforms have forms to be fulfilled with the information of the platform to be integrated with DataHub. In other cases, the information is provided in a yaml style as shown in the following figure: 5) Schedule the ingestion of data as shown in the following figure: 6) Click in Save & Run source and the ingestion will start. Adding metadata to the datasets 1) Access to DataHub 2) Access to the desired dataset 3) You can edit the metadata of the dataset in the column shown at the right side of the following figure and descriptions , tags and glossary terms can be added to the dataset fields: 4) In the documenation tab, the documentation of the dataset can be added directly in formatted text or via a link to the documentation of it. 5) The lineage tab allows to edit the lineage of the dataset. 6) The queries tab allows to add the most common queries conducted to the dataset. However, this queries cannot be executed from DataHub.","title":"DataHub example of Integration"},{"location":"tools/datahub/examples/#datahub-example-of-integration","text":"","title":"DataHub example of Integration"},{"location":"tools/datahub/examples/#creating-an-ingestion-source","text":"","title":"Creating an Ingestion Source"},{"location":"tools/datahub/examples/#introduction","text":"In this example we will show how to create an ingestion source that pulls data from the desired platform to DataHub.","title":"Introduction"},{"location":"tools/datahub/examples/#prerequisites","text":"Access to DataHub Access to the platform to be integrated with DataHub Check if the platform to be integrated with DataHub is already supported by Datahub. List of ingestion sources","title":"Prerequisites"},{"location":"tools/datahub/examples/#creating-the-recipe","text":"1) Access to DataHub 2) Go to the Ingestion view and click in create new source. 3) Select the platform to be integrated with DataHub. 4) Fill the form with the information of the platform to be integrated with DataHub. Some of the platforms have forms to be fulfilled with the information of the platform to be integrated with DataHub. In other cases, the information is provided in a yaml style as shown in the following figure: 5) Schedule the ingestion of data as shown in the following figure: 6) Click in Save & Run source and the ingestion will start.","title":"Creating the recipe"},{"location":"tools/datahub/examples/#adding-metadata-to-the-datasets","text":"1) Access to DataHub 2) Access to the desired dataset 3) You can edit the metadata of the dataset in the column shown at the right side of the following figure and descriptions , tags and glossary terms can be added to the dataset fields: 4) In the documenation tab, the documentation of the dataset can be added directly in formatted text or via a link to the documentation of it. 5) The lineage tab allows to edit the lineage of the dataset. 6) The queries tab allows to add the most common queries conducted to the dataset. However, this queries cannot be executed from DataHub.","title":"Adding metadata to the datasets"},{"location":"tools/keycloak/","text":"Keycloak Introduction: In today's rapidly evolving tech landscape, ensuring the security of your applications is paramount. As you venture into developing or enhancing your application, integrating a robust authentication and authorization mechanism is crucial. This tutorial aims to guide you through setting up a development environment by integrating Keycloak, an open-source Identity and Access Management solution. Whether you're working on a brand new project or improving an existing one, following this guide will provide a solid foundation for securing your application using Keycloak. Features Keycloak: Single Sign-On (SSO) & Sign-Out: Keycloak facilitates SSO and Single Sign-Out, allowing users to log in once and access various applications without needing to log in again. Standards-Based: Keycloak supports standard protocols for authentication and authorization such as OpenID Connect, OAuth 2.0, and SAML 2.0, ensuring compatibility with various systems. User Federation: It allows user federation with a variety of sources including LDAP and Active Directory, enabling you to manage users and their roles centrally. Customizable: Keycloak is highly customizable, offering a broad range of options to tailor authentication and authorization flows to meet your specific requirements. Scalable: It is designed to be scalable, catering to both small applications and large enterprise solutions with millions of users. Role-Based Access Control (RBAC): Keycloak provides fine-grained authorization and role management, enabling you to control who has access to what in your applications. Multi-factor Authentication (MFA): It supports multi-factor authentication, enhancing security by requiring users to provide multiple forms of identification before gaining access. Social Login: Keycloak supports social logins, allowing users to log in using their social media accounts which can enhance user experience and increase adoption rates. Easy to Integrate: With a variety of client adapters and libraries, integrating Keycloak with your application is straightforward regardless of the technology stack you're using. Support and Community: Being an open-source project, Keycloak has a vibrant community and a wealth of resources available to help you throughout your integration journey. Use case The following diagram show the sequences for an user auythentication procress using Keycloak as an OpenID Connect (OIDC) provider The User initiates the process by requesting a page from the Application. The Application then redirects the User to a browser for authentication. The browser initiates an Authentication Request to Keycloak. Keycloak presents a Login Page to the User. The User enters their credentials into the login page. The credentials are submitted back to Keycloak for validation. Upon successful validation, Keycloak redirects the User back to the Application. The Application makes a Token Request to Keycloak. Keycloak validates the request and, if valid, returns a Token to the Application. With the Token, the User is granted access to the requested page within the Application.","title":"Keycloak"},{"location":"tools/keycloak/#keycloak","text":"","title":"Keycloak"},{"location":"tools/keycloak/#introduction","text":"In today's rapidly evolving tech landscape, ensuring the security of your applications is paramount. As you venture into developing or enhancing your application, integrating a robust authentication and authorization mechanism is crucial. This tutorial aims to guide you through setting up a development environment by integrating Keycloak, an open-source Identity and Access Management solution. Whether you're working on a brand new project or improving an existing one, following this guide will provide a solid foundation for securing your application using Keycloak.","title":"Introduction:"},{"location":"tools/keycloak/#features-keycloak","text":"Single Sign-On (SSO) & Sign-Out: Keycloak facilitates SSO and Single Sign-Out, allowing users to log in once and access various applications without needing to log in again. Standards-Based: Keycloak supports standard protocols for authentication and authorization such as OpenID Connect, OAuth 2.0, and SAML 2.0, ensuring compatibility with various systems. User Federation: It allows user federation with a variety of sources including LDAP and Active Directory, enabling you to manage users and their roles centrally. Customizable: Keycloak is highly customizable, offering a broad range of options to tailor authentication and authorization flows to meet your specific requirements. Scalable: It is designed to be scalable, catering to both small applications and large enterprise solutions with millions of users. Role-Based Access Control (RBAC): Keycloak provides fine-grained authorization and role management, enabling you to control who has access to what in your applications. Multi-factor Authentication (MFA): It supports multi-factor authentication, enhancing security by requiring users to provide multiple forms of identification before gaining access. Social Login: Keycloak supports social logins, allowing users to log in using their social media accounts which can enhance user experience and increase adoption rates. Easy to Integrate: With a variety of client adapters and libraries, integrating Keycloak with your application is straightforward regardless of the technology stack you're using. Support and Community: Being an open-source project, Keycloak has a vibrant community and a wealth of resources available to help you throughout your integration journey.","title":"Features Keycloak:"},{"location":"tools/keycloak/#use-case","text":"The following diagram show the sequences for an user auythentication procress using Keycloak as an OpenID Connect (OIDC) provider The User initiates the process by requesting a page from the Application. The Application then redirects the User to a browser for authentication. The browser initiates an Authentication Request to Keycloak. Keycloak presents a Login Page to the User. The User enters their credentials into the login page. The credentials are submitted back to Keycloak for validation. Upon successful validation, Keycloak redirects the User back to the Application. The Application makes a Token Request to Keycloak. Keycloak validates the request and, if valid, returns a Token to the Application. With the Token, the User is granted access to the requested page within the Application.","title":"Use case"},{"location":"tools/keycloak/integration/","text":"Keycloak Integration Guide for Third-Party Services To facilitate a centralized user management system, we are utilizing Keycloak as our identity and access management solution. To integrate your service with our Keycloak system, please follow the instructions below to request the necessary credentials. Step 1: Requesting Client Credentials Send an email to greengage.admin@deusto.es with the subject line: \"[GREENGAGE] Request for Keycloak Client Credentials\". In the body of the email, please provide the following information: Tool name: Description: (no more than 200 characters): Contact name:(for technical propose) Contact email:(for technical propose) Step 2: Receiving Your Credentials Upon receipt of your request, we will review the information and create a unique Client ID and Secret for your service. We will then send you an email with your Client ID and Secret, which you will use to configure the integration on your side. Step 3: Configuring Your Service Once you have received your Client ID and Secret, incorporate these credentials into your service\u2019s authentication module. Ensure that your service is set to communicate with our Keycloak server using the OpenID Connect protocol. Step 4: Testing the Integration After configuring your service, conduct thorough testing to ensure that the authentication process works correctly. If you encounter any issues during testing, please reach out to us via email for support. If you have any questions about Keycloak integration, please contact us at greengage.admin@deusto.es with the subject \"[Greengage] Keycloak support request\" Important Notes: Keep your Client Secret confidential. Do not share it with unauthorized personnel or services. If you believe your Client Secret has been compromised, contact us immediately to issue a new one. Adhere to all security best practices when implementing the integration to safeguard user data.","title":"Keycloak Integration Guide for Third-Party Services"},{"location":"tools/keycloak/integration/#keycloak-integration-guide-for-third-party-services","text":"To facilitate a centralized user management system, we are utilizing Keycloak as our identity and access management solution. To integrate your service with our Keycloak system, please follow the instructions below to request the necessary credentials. Step 1: Requesting Client Credentials Send an email to greengage.admin@deusto.es with the subject line: \"[GREENGAGE] Request for Keycloak Client Credentials\". In the body of the email, please provide the following information: Tool name: Description: (no more than 200 characters): Contact name:(for technical propose) Contact email:(for technical propose) Step 2: Receiving Your Credentials Upon receipt of your request, we will review the information and create a unique Client ID and Secret for your service. We will then send you an email with your Client ID and Secret, which you will use to configure the integration on your side. Step 3: Configuring Your Service Once you have received your Client ID and Secret, incorporate these credentials into your service\u2019s authentication module. Ensure that your service is set to communicate with our Keycloak server using the OpenID Connect protocol. Step 4: Testing the Integration After configuring your service, conduct thorough testing to ensure that the authentication process works correctly. If you encounter any issues during testing, please reach out to us via email for support. If you have any questions about Keycloak integration, please contact us at greengage.admin@deusto.es with the subject \"[Greengage] Keycloak support request\" Important Notes: Keep your Client Secret confidential. Do not share it with unauthorized personnel or services. If you believe your Client Secret has been compromised, contact us immediately to issue a new one. Adhere to all security best practices when implementing the integration to safeguard user data.","title":"Keycloak Integration Guide for Third-Party Services"},{"location":"tools/keycloak/examples/","text":"Examples This section includes examples in different programming languages on how integration with the provided KeyCloak can be performed. Nodejs Prerequisites Docker and Docker Compose installed. Node.js and npm installed. Basic understanding of Keycloak, Docker, and Node.js. Project Structure Source: example example/ |-- docker-compose.yml |-- realm-export.json |-- keycloak.json |-- package.json |-- index.js |-- run.sh (for Linux/macOS) |-- run.bat (for Windows) Step 1: Setting Up Keycloak docker-compose.yml : This file contains the configuration to run a Keycloak container. Make sure the docker-compose.yml file is set up as provided in your project. realm-export.json : This file should be configured according to your Keycloak realm requirements. It contains realm, client, user, and role configurations. Step 2: Setting Up Node.js Application package.json : This file contains your project metadata and dependencies. Ensure express , express-session , and keycloak-connect dependencies are listed. index.js : This file contains your Express application setup. It sets up routes for login, logout, and the home page which displays the JWT. keycloak.json : This file contains the Keycloak client configuration. Update the realm , resource , and credentials fields with your Keycloak configuration. Installing Dependencies : Run the following command to install the necessary packages as listed in your package.json : npm i Step 3: Running the Services Linux/macOS : Ensure run.sh is executable: chmod +x run.sh . Execute run.sh to start the services: ./run.sh . Windows : Double-click run.bat or run it in the command prompt to start the services. Accessing the Application Navigate to localhost:3000/auth to log in using Keycloak. Once logged in, navigate to localhost:3000 to view the JWT and its decoded payload. To logout, navigate to localhost:3000/logout . Step 1: Install and Setup Keycloak Download and install Keycloak from the official website . Start Keycloak by navigating to the bin directory of your Keycloak installation and executing the standalone.sh (for Linux/macOS) or standalone.bat (for Windows) script. Access the Keycloak Admin Console at http://localhost:8080/auth and complete the initial setup. Create an admin user for managing Keycloak. Step 2: Create a Realm and a Client (OpenID Connect) Create a New Realm: Navigate to the Keycloak Admin Console. Click on \"Add realm\" to create a new realm. Enter the required details for your realm and save. Register a Client: Navigate to Clients and click Create . Provide a Client ID , and select the Client Protocol as openid-connect . Select the Client Access Type as confidential if your client is a web application that can secure the client secret. Otherwise, select public if your client is a native app or a JavaScript app running in the browser. Set Standard Flow Enabled to ON if you want to use the Authorization Code Flow which is recommended for most scenarios. Configure OpenID Connect Protocol: For each client, you can tailor what claims and assertions are stored in the OIDC token by creating and configuring protocol mappers. You may need to set up JSON mapping for certain claim keys in your application to handle roles or other claims passed by Keycloak. Client Adapters: Install a Keycloak Adapter in your application environment to communicate and be secured by Keycloak. Keycloak provides adapters for different platforms, and there are also third-party adapters available. Test Your Setup: At this point, it would be prudent to test your setup by attempting to authenticate using OpenID Connect. There are various grant types supported by Keycloak for authenticating users including authorization code, implicit, and client credentials. Additional Configuration (Optional): Depending on your application's requirements, you might need to configure additional settings in Keycloak or in your application. For instance, you might need to set up user roles, groups, and permissions, or configure multi-factor authentication. Documentations and Tutorials: There are various resources available that provide step-by-step guides or tutorials on integrating OpenID Connect with Keycloak, including the Keycloak official documentation . This extended step should provide a more thorough understanding of how to integrate OpenID Connect with Keycloak. However, the exact steps might vary based on your application's technology stack and your specific requirements. Step 3: Configure your System For a JavaScript application, you could use the Keycloak JavaScript adapter. npm install keycloak-js or a middleware as keycloak-connect npm install keycloak-connect Configure the library with the details of your Keycloak realm and client. // Example configuration for a JavaScript application const keycloak = Keycloak({ url: \"http://localhost:8080/auth\", realm: \"<your-realm>\", clientId: \"<your-client-id>\", }); Step 4: Integrate Authentication Use the library to add authentication to your system. For a web application, this would typically involve redirecting unauthenticated users to the Keycloak login page, and handling the tokens returned by Keycloak upon successful authentication. // Example integration for a JavaScript application keycloak .init({ onLoad: \"login-required\" }) .then((authenticated) => { console.log(authenticated ? \"Authenticated\" : \"Not authenticated\"); }) .catch((error) => { console.error(\"Failed to initialize authentication\", error); }); Step 5: Integrate Authorization Use the tokens obtained during authentication to make authorized requests to your system's backend, and to check the user's roles and permissions. // Example authorization check in a JavaScript application if (keycloak.hasRealmRole(\"admin\")) { console.log(\"User is an admin\"); } This tutorial provides a high-level overview of the steps involved in integrating Keycloak with your system. The exact steps and code may vary depending on the specifics of your system and the programming languages and frameworks you are using. Python Prerequisites Docker and Docker Compose installed. Python and pip installed. Basic understanding of Keycloak, Docker, Flask, and Python. Access the application over HTTPS and accept the self-signed certificate warning: \"The certificate is not trusted because it is self-signed.\" Project Structure Source: example example/ |-- docker-compose.yml |-- realm-export.json |-- client_secrets.json |-- requirements.txt |-- index.py |-- run.sh (for Linux/macOS) |-- run.bat (for Windows) Step 1: Setting Up Keycloak docker-compose.yml : This file contains the configuration to run a Keycloak container. Ensure it's set up as provided in your project. realm-export.json : Configure this file according to your Keycloak realm requirements. It contains realm, client, user, and role configurations. Step 2: Setting Up Python Flask Application requirements.txt : Lists the necessary Python packages, including Flask and Flask-OIDC. client_secrets.json : Contains Keycloak client configuration. Update the client_id , client_secret , and URLs according to your Keycloak setup. index.py : Contains your Flask application. Sets up routes for login, logout, and home page displaying user information. Step 3: Installing Dependencies Run the following command to install necessary Python packages: pip install -r requirements.txt Step 4: Running the Services Linux/macOS : Make run.sh executable: chmod +x run.sh . Execute run.sh to start the services: ./run.sh . Windows : Execute run.bat to start the services. Accessing the Application Navigate to https://localhost:3000/ (accept the self-signed certificate warning). Use the Keycloak authentication process to log in. Once logged in, user information will be displayed on the home page. To logout, navigate to https://localhost:3000/logout . Important Notes Ensure all URLs in client_secrets.json and realm-export.json match your Keycloak configuration. Remember to access the application via HTTPS and accept the browser warning about the self-signed certificate. Modify the index.py file to suit your application's specific Flask and OIDC needs. For more information you can visit the official documentation in https://www.keycloak.org/documentation","title":"Examples"},{"location":"tools/keycloak/examples/#examples","text":"This section includes examples in different programming languages on how integration with the provided KeyCloak can be performed.","title":"Examples"},{"location":"tools/keycloak/examples/#nodejs","text":"","title":"Nodejs"},{"location":"tools/keycloak/examples/#prerequisites","text":"Docker and Docker Compose installed. Node.js and npm installed. Basic understanding of Keycloak, Docker, and Node.js.","title":"Prerequisites"},{"location":"tools/keycloak/examples/#project-structure","text":"Source: example example/ |-- docker-compose.yml |-- realm-export.json |-- keycloak.json |-- package.json |-- index.js |-- run.sh (for Linux/macOS) |-- run.bat (for Windows)","title":"Project Structure"},{"location":"tools/keycloak/examples/#step-1-setting-up-keycloak","text":"docker-compose.yml : This file contains the configuration to run a Keycloak container. Make sure the docker-compose.yml file is set up as provided in your project. realm-export.json : This file should be configured according to your Keycloak realm requirements. It contains realm, client, user, and role configurations.","title":"Step 1: Setting Up Keycloak"},{"location":"tools/keycloak/examples/#step-2-setting-up-nodejs-application","text":"package.json : This file contains your project metadata and dependencies. Ensure express , express-session , and keycloak-connect dependencies are listed. index.js : This file contains your Express application setup. It sets up routes for login, logout, and the home page which displays the JWT. keycloak.json : This file contains the Keycloak client configuration. Update the realm , resource , and credentials fields with your Keycloak configuration. Installing Dependencies : Run the following command to install the necessary packages as listed in your package.json : npm i","title":"Step 2: Setting Up Node.js Application"},{"location":"tools/keycloak/examples/#step-3-running-the-services","text":"Linux/macOS : Ensure run.sh is executable: chmod +x run.sh . Execute run.sh to start the services: ./run.sh . Windows : Double-click run.bat or run it in the command prompt to start the services.","title":"Step 3: Running the Services"},{"location":"tools/keycloak/examples/#accessing-the-application","text":"Navigate to localhost:3000/auth to log in using Keycloak. Once logged in, navigate to localhost:3000 to view the JWT and its decoded payload. To logout, navigate to localhost:3000/logout .","title":"Accessing the Application"},{"location":"tools/keycloak/examples/#step-1-install-and-setup-keycloak","text":"Download and install Keycloak from the official website . Start Keycloak by navigating to the bin directory of your Keycloak installation and executing the standalone.sh (for Linux/macOS) or standalone.bat (for Windows) script. Access the Keycloak Admin Console at http://localhost:8080/auth and complete the initial setup. Create an admin user for managing Keycloak.","title":"Step 1: Install and Setup Keycloak"},{"location":"tools/keycloak/examples/#step-2-create-a-realm-and-a-client-openid-connect","text":"Create a New Realm: Navigate to the Keycloak Admin Console. Click on \"Add realm\" to create a new realm. Enter the required details for your realm and save. Register a Client: Navigate to Clients and click Create . Provide a Client ID , and select the Client Protocol as openid-connect . Select the Client Access Type as confidential if your client is a web application that can secure the client secret. Otherwise, select public if your client is a native app or a JavaScript app running in the browser. Set Standard Flow Enabled to ON if you want to use the Authorization Code Flow which is recommended for most scenarios. Configure OpenID Connect Protocol: For each client, you can tailor what claims and assertions are stored in the OIDC token by creating and configuring protocol mappers. You may need to set up JSON mapping for certain claim keys in your application to handle roles or other claims passed by Keycloak. Client Adapters: Install a Keycloak Adapter in your application environment to communicate and be secured by Keycloak. Keycloak provides adapters for different platforms, and there are also third-party adapters available. Test Your Setup: At this point, it would be prudent to test your setup by attempting to authenticate using OpenID Connect. There are various grant types supported by Keycloak for authenticating users including authorization code, implicit, and client credentials. Additional Configuration (Optional): Depending on your application's requirements, you might need to configure additional settings in Keycloak or in your application. For instance, you might need to set up user roles, groups, and permissions, or configure multi-factor authentication. Documentations and Tutorials: There are various resources available that provide step-by-step guides or tutorials on integrating OpenID Connect with Keycloak, including the Keycloak official documentation . This extended step should provide a more thorough understanding of how to integrate OpenID Connect with Keycloak. However, the exact steps might vary based on your application's technology stack and your specific requirements.","title":"Step 2: Create a Realm and a Client (OpenID Connect)"},{"location":"tools/keycloak/examples/#step-3-configure-your-system","text":"For a JavaScript application, you could use the Keycloak JavaScript adapter. npm install keycloak-js or a middleware as keycloak-connect npm install keycloak-connect Configure the library with the details of your Keycloak realm and client. // Example configuration for a JavaScript application const keycloak = Keycloak({ url: \"http://localhost:8080/auth\", realm: \"<your-realm>\", clientId: \"<your-client-id>\", });","title":"Step 3: Configure your System"},{"location":"tools/keycloak/examples/#step-4-integrate-authentication","text":"Use the library to add authentication to your system. For a web application, this would typically involve redirecting unauthenticated users to the Keycloak login page, and handling the tokens returned by Keycloak upon successful authentication. // Example integration for a JavaScript application keycloak .init({ onLoad: \"login-required\" }) .then((authenticated) => { console.log(authenticated ? \"Authenticated\" : \"Not authenticated\"); }) .catch((error) => { console.error(\"Failed to initialize authentication\", error); });","title":"Step 4: Integrate Authentication"},{"location":"tools/keycloak/examples/#step-5-integrate-authorization","text":"Use the tokens obtained during authentication to make authorized requests to your system's backend, and to check the user's roles and permissions. // Example authorization check in a JavaScript application if (keycloak.hasRealmRole(\"admin\")) { console.log(\"User is an admin\"); } This tutorial provides a high-level overview of the steps involved in integrating Keycloak with your system. The exact steps and code may vary depending on the specifics of your system and the programming languages and frameworks you are using.","title":"Step 5: Integrate Authorization"},{"location":"tools/keycloak/examples/#python","text":"","title":"Python"},{"location":"tools/keycloak/examples/#prerequisites_1","text":"Docker and Docker Compose installed. Python and pip installed. Basic understanding of Keycloak, Docker, Flask, and Python. Access the application over HTTPS and accept the self-signed certificate warning: \"The certificate is not trusted because it is self-signed.\"","title":"Prerequisites"},{"location":"tools/keycloak/examples/#project-structure_1","text":"Source: example example/ |-- docker-compose.yml |-- realm-export.json |-- client_secrets.json |-- requirements.txt |-- index.py |-- run.sh (for Linux/macOS) |-- run.bat (for Windows)","title":"Project Structure"},{"location":"tools/keycloak/examples/#step-1-setting-up-keycloak_1","text":"docker-compose.yml : This file contains the configuration to run a Keycloak container. Ensure it's set up as provided in your project. realm-export.json : Configure this file according to your Keycloak realm requirements. It contains realm, client, user, and role configurations.","title":"Step 1: Setting Up Keycloak"},{"location":"tools/keycloak/examples/#step-2-setting-up-python-flask-application","text":"requirements.txt : Lists the necessary Python packages, including Flask and Flask-OIDC. client_secrets.json : Contains Keycloak client configuration. Update the client_id , client_secret , and URLs according to your Keycloak setup. index.py : Contains your Flask application. Sets up routes for login, logout, and home page displaying user information.","title":"Step 2: Setting Up Python Flask Application"},{"location":"tools/keycloak/examples/#step-3-installing-dependencies","text":"Run the following command to install necessary Python packages: pip install -r requirements.txt","title":"Step 3: Installing Dependencies"},{"location":"tools/keycloak/examples/#step-4-running-the-services","text":"Linux/macOS : Make run.sh executable: chmod +x run.sh . Execute run.sh to start the services: ./run.sh . Windows : Execute run.bat to start the services.","title":"Step 4: Running the Services"},{"location":"tools/keycloak/examples/#accessing-the-application_1","text":"Navigate to https://localhost:3000/ (accept the self-signed certificate warning). Use the Keycloak authentication process to log in. Once logged in, user information will be displayed on the home page. To logout, navigate to https://localhost:3000/logout .","title":"Accessing the Application"},{"location":"tools/keycloak/examples/#important-notes","text":"Ensure all URLs in client_secrets.json and realm-export.json match your Keycloak configuration. Remember to access the application via HTTPS and accept the browser warning about the self-signed certificate. Modify the index.py file to suit your application's specific Flask and OIDC needs. For more information you can visit the official documentation in https://www.keycloak.org/documentation","title":"Important Notes"},{"location":"tools/mode/","text":"MODE Introduction MODE software technology uses smartphones to automatically track the distances travelled and transport modes used, thus enabling software developers, system integrators and transport operators to design innovative mobility services. MODE provides the missing link for the development of innovative mobility services: automatic, reliable recognition of the means of transport used, and travelled routes using smartphones. Features of MODE MODE is a trip recording and mode detection framework. It consists of two main software components: 1. A data recording module for iOS and Android Apps 2. A backend data analysis module The above figure shows a high-level overview and demonstrates how to embed the MODE components into a system (AIT components are highlighted in blue, other components must be provided by a partner). Client-side tracking and data recording functionality can be added to an existing App by including the libmode library. Once the library is configured and activated (see Frontend ), it tracks user activity and records relevant movement data. The library transmits completed trips to a backend server using REST calls. On the server side, the transmitted trip data is processed using the AIT JMDCA library. The JMDCA library is written in Java and provides core data structures, processing routines and analysis algorithms which allow application programmers to easily analyse trips and perform mode detection on the recorded data (JMDCA API, see section Backend ). To deliver high-quality results, the JMDCA mode detection algorithm requires access to a third-party public transit routing engine (such as HERE or Google routing). The result of using MODE are trips. These trips can include multiple segments (\"activities\") which make up a trip. E.g. if a user walks to his or her car, then drives to a transit station and then takes the train to reach the destination it would yield three activities, each with: Start and end times The travel mode (e.g. bike, car, public transport, walking, etc.) The probability of the \"correctness\" (e.g. it could be \"LOW; Reason: Bad GPS signal\") Use Case Scenario The main usage of MODE is to be integrated in smartphone apps to track user travel behavior. E.g. it was used in apps which incentivize \"green\" transport modes where you start the app and if it detects that you did the trip via bicycle or public transport you could earn \"tokens\" which could be spent on culture events.","title":"MODE"},{"location":"tools/mode/#mode","text":"","title":"MODE"},{"location":"tools/mode/#introduction","text":"MODE software technology uses smartphones to automatically track the distances travelled and transport modes used, thus enabling software developers, system integrators and transport operators to design innovative mobility services. MODE provides the missing link for the development of innovative mobility services: automatic, reliable recognition of the means of transport used, and travelled routes using smartphones.","title":"Introduction"},{"location":"tools/mode/#features-of-mode","text":"MODE is a trip recording and mode detection framework. It consists of two main software components: 1. A data recording module for iOS and Android Apps 2. A backend data analysis module The above figure shows a high-level overview and demonstrates how to embed the MODE components into a system (AIT components are highlighted in blue, other components must be provided by a partner). Client-side tracking and data recording functionality can be added to an existing App by including the libmode library. Once the library is configured and activated (see Frontend ), it tracks user activity and records relevant movement data. The library transmits completed trips to a backend server using REST calls. On the server side, the transmitted trip data is processed using the AIT JMDCA library. The JMDCA library is written in Java and provides core data structures, processing routines and analysis algorithms which allow application programmers to easily analyse trips and perform mode detection on the recorded data (JMDCA API, see section Backend ). To deliver high-quality results, the JMDCA mode detection algorithm requires access to a third-party public transit routing engine (such as HERE or Google routing). The result of using MODE are trips. These trips can include multiple segments (\"activities\") which make up a trip. E.g. if a user walks to his or her car, then drives to a transit station and then takes the train to reach the destination it would yield three activities, each with: Start and end times The travel mode (e.g. bike, car, public transport, walking, etc.) The probability of the \"correctness\" (e.g. it could be \"LOW; Reason: Bad GPS signal\")","title":"Features of MODE"},{"location":"tools/mode/#use-case-scenario","text":"The main usage of MODE is to be integrated in smartphone apps to track user travel behavior. E.g. it was used in apps which incentivize \"green\" transport modes where you start the app and if it detects that you did the trip via bicycle or public transport you could earn \"tokens\" which could be spent on culture events.","title":"Use Case Scenario"},{"location":"tools/mode/integration/","text":"MODE Integration Guide for Third-Party Services Since MODE is a distributed system with multiple components, it can (or must) be implemented at various levels, mostly in the frontend (Android, iOS apps) and the backend (server) which processes the trips. Frontend The MODE smartphone libraries ( libmode for Android and iOS ) must be integrated in a host app which takes care about user authentication, and starting/stopping the MODE service. While MODE can run continuously in the background, it can adversely affect battery consumption (especially on iOS devices). It should therefore only be started when the user agrees to track a trip. When MODE is running, it will detect when activities start or end (e.g. due to GPS signals, or when it detects, that the phone is not moving anymore). Once an activity end, the binary data of the trip is sent to a REST server in the background, which does the processing. Backend The backend is provided as a Docker image which opens the rest service on port 8080. Its sole purpose is to listen to the binary data from the frontend, process the data, and return JSON output of trips. If advanced logic is needed, like storing the returned trips in a database, this must be done by some kind of middleware which details are yet to be determined, depending on the concrete use cases of MODE within the project. Testing Testing the setup with real data can be cumbersome, as someone would need to walk/drive around to see the data pushed from the frontend to the backend. Therefore we provide some sample data which can be sent to the backend to be processed using a simple test script . Note that the backend server requires an API key which must be changed in the test script and is provided on demand . Output If the front end and backend works correctly, the backend will spit out the resulting trips as a JSON file : { \"message\": \"success\", \"activities\": [ { \"begin\": \"2020-07-15T13:20:41.059Z\", \"end\": \"2020-07-15T13:45:37.059Z\", \"mode\": \"WALK\", \"modeDetails\": \" | Sensor mode Walk | GPSQuality(AVERAGE : time gap)\", \"distanceMeters\": 1253.2470899042082, \"route\": \"ue`eHkddcB??????????rCrAj@Vj@Vh@Th@Vp@Xj@Xp@Xh@Tl@Vl@XJDp@Xn@Xp@Vp@XJDl@Tn@Rp@RRB@@D@?@?@???@???@@????????@???A?????????????????????????????????@????????????????????????Nh@??@E??@??@@B@B@B@@ACAAAC?AM]EICGCE?A???@?@BFBH@DP`A@F?D???B????A@A???EA??A??A????A@??????A????AA????CAI@??A@????A????????@?????B@B?D?B?F@D?B?H@H?F?D@B?@?@????C?CAC?CAGAGCG?CAA?AAA@??ABABCBABAB?@?@?BBBBD@D@F?DCDCBCBEBEBC@EBEBC@EBEBC@EBC@CBCBCBCBCBEBC@EBE@C@C@ABCBCBEBCBE@E@G@G@EBEBEBGBEBG@GBG@G?G?E?ECCEAEAEAECEECECEEECEECECCCCECCECCACACAEACAECECEEECEECCEEECECECECECEEECECECEAECEAECGCGCEEECE?C@CBE@C?CCECC?E?CBCFADCFCDABC@A@A?@A????A?A??@A???@A??@A??????@??A?@@?@??????A?A?AA???????????A?@???????@??@???@A??@?A???A@A?A??????@A??A??@A@???@???@?B????@??A?AA????A?????????A???@?????@?@?@?@?@A??@??????A???@@?A?A???????????A???A?A??????????@??@???A??@A???A@?A??@?@A@?@?@????@????A?????A?A?A?????A??????A????DA??????A??@????A???@??@?????@??A?????????@??????A?????CAAA?AAAA@???@A?????????@@@@?@????@@???????????????A?????????A???AA????????A?????A???@@@??@A?????A?@?????@??A?A@A@A@????@I@????B?@?A@??@?@ABCBA@A???\" }, { \"begin\": \"2020-07-15T13:45:38.059Z\", \"end\": \"2020-07-15T13:51:20.059Z\", \"mode\": \"BUS\", \"modeDetails\": \"Line 69A (Hauptbahnhof - Simmering) (BUS) from Am Kanal to Arsenalsteg | Sensor mode Bus routed to Bus | GPSQuality(GOOD)\", \"distanceMeters\": 2143.843162687046, \"route\": \"ch_eHk_dcB????yA`ByAxA??gBbB??}AlA??{BrA??cBp@uBn@??cBZ??s@`C????????????????????AnD??ElC??EjCGlC??EtB??EtBGtB??IxD??KpC????????????????????A`D??IdE????c@xC??_AlB??CzC??m@lC??_AbB????????????qA`C????wBxD??_BfC??wAzB????}A~B????????????????iAbB??gAdB??gB`C??q@~@s@~@s@~@KW\" }, { \"begin\": \"2020-07-15T13:51:29.059Z\", \"end\": \"2020-07-15T14:32:45.059Z\", \"mode\": \"WALK\", \"modeDetails\": \" | Sensor mode Walk | GPSQuality(BAD : large time gap)\", \"distanceMeters\": 1435.4269300101091, \"route\": \"mhaeHq{_cBE^@?AAACEAEAE?E?C?CACCCAC?E?CACCAG?CBEBABA@CBCBAB?@D?@?BCFA@C@CBAB?B@B@@@A@C@C@A@@@B?DABAB?B?@@@??@?BABA@?@?@B?D?D@D@F@D@D@F@F@D@FBD@F@FBH@FBHBFBFBFBHBF@HBF@DBF@FBDBDBDBFBD@FBD@D?B?@???????A??????????????@@?B@DBDBF@DBFBDBFBD@FBDBFBFBDBDBDBFBDBFBFBDBFBD@F@FBFBDB@DAD?DAB@D@B@@@A@CBC@CB?BDBFBF@FBD@BDBB@F@F@H?H@H@H@HBHBF@DBF@DBFBBDDDBDBDBDDBDBFBDDFBD@FBFBD@FBFBDBFBDDDDFDFBDDDBDBDBBBBBDBBBBD@B@DABABEBCBCBE@G@G@G@GBE@EBG@E?G@G?E@G?E@GBE@IBGBGDGBGBEBG@E@E@G@E@G@G@G@E@G@G?E@G?G@EBEBEBEBAD?D@FBDBBDDD@F@B@D?D@B?B@B@B??DJ?@@@?@B@LLDBFB@@@?@@@???@@@?@@@???@@@???@@@?@?@@??@@@?@?@@??@?@@@@DBB@NZ?@?@@@?@@??@?@?@@??@??@B??????????????AA???????A????EM??AC????AC?A???A????CG?AAC????????CI?AAA?A???????A??A????A??AE?????AA????A?????AAA?AAC?????A??????AAAE???A????M_@AACEAA??A??AAA??AAA?AA??MKAACCCACCECCCCACCEECCEAE?E@EDEDEFCDAHCHAHAHAH?FAF?DADCBADADAF?FAHAFAFAFAF?H?FAF?FAFCFCDCFCDCDCDEDCBEDCBCDAFCF?FAF@F?FAD?DADAFADCBADCDCDCDAFCDAFADAFCDCDCDCBADADADCDADCDADCFAH?HAF?HAF?F?H?F@FAD?FCFADAHAFAFAFAHAFADCFAF?D?F?D?D?FADADAFAFCFCHAFCFCHCDCFCDEBEDEBEDC@E?EACCEECACCCACCCCCCACCCCCCECCACCCACACA@A@A@?@?@?@?@?????????A??A?A?A?A?@@@?????????@?????@A@ABU??EH?@A@?@ABEJ?@ABA@CHW|@K^ADAH?@?????C?????A???@?????A?E?A?C?????J?@????A?@????????@???A????????????\" } ] } The format uses the encoded polyline algorithm for encoding the polylines of the trips ( sample code ). We will, however, for the project also try to output GeoJSON files directly, which might be easier to consume. Support and Contact For any questions, contact Martin Stubenschrott directly at martin.stubenschrott@ait.ac.at Important Notes You might get an API key for third-party routing services (e.g. Google Router). Make sure to keep that confident and only use it for testing and sparsingly. Overuse of the API key may result in high fees, as only about 20.000 requests/per month are free! For production, we must use a different key being used for billing!","title":"MODE Integration Guide for Third-Party Services"},{"location":"tools/mode/integration/#mode-integration-guide-for-third-party-services","text":"Since MODE is a distributed system with multiple components, it can (or must) be implemented at various levels, mostly in the frontend (Android, iOS apps) and the backend (server) which processes the trips.","title":"MODE Integration Guide for Third-Party Services"},{"location":"tools/mode/integration/#frontend","text":"The MODE smartphone libraries ( libmode for Android and iOS ) must be integrated in a host app which takes care about user authentication, and starting/stopping the MODE service. While MODE can run continuously in the background, it can adversely affect battery consumption (especially on iOS devices). It should therefore only be started when the user agrees to track a trip. When MODE is running, it will detect when activities start or end (e.g. due to GPS signals, or when it detects, that the phone is not moving anymore). Once an activity end, the binary data of the trip is sent to a REST server in the background, which does the processing.","title":"Frontend"},{"location":"tools/mode/integration/#backend","text":"The backend is provided as a Docker image which opens the rest service on port 8080. Its sole purpose is to listen to the binary data from the frontend, process the data, and return JSON output of trips. If advanced logic is needed, like storing the returned trips in a database, this must be done by some kind of middleware which details are yet to be determined, depending on the concrete use cases of MODE within the project.","title":"Backend"},{"location":"tools/mode/integration/#testing","text":"Testing the setup with real data can be cumbersome, as someone would need to walk/drive around to see the data pushed from the frontend to the backend. Therefore we provide some sample data which can be sent to the backend to be processed using a simple test script . Note that the backend server requires an API key which must be changed in the test script and is provided on demand .","title":"Testing"},{"location":"tools/mode/integration/#output","text":"If the front end and backend works correctly, the backend will spit out the resulting trips as a JSON file : { \"message\": \"success\", \"activities\": [ { \"begin\": \"2020-07-15T13:20:41.059Z\", \"end\": \"2020-07-15T13:45:37.059Z\", \"mode\": \"WALK\", \"modeDetails\": \" | Sensor mode Walk | GPSQuality(AVERAGE : time gap)\", \"distanceMeters\": 1253.2470899042082, \"route\": \"ue`eHkddcB??????????rCrAj@Vj@Vh@Th@Vp@Xj@Xp@Xh@Tl@Vl@XJDp@Xn@Xp@Vp@XJDl@Tn@Rp@RRB@@D@?@?@???@???@@????????@???A?????????????????????????????????@????????????????????????Nh@??@E??@??@@B@B@B@@ACAAAC?AM]EICGCE?A???@?@BFBH@DP`A@F?D???B????A@A???EA??A??A????A@??????A????AA????CAI@??A@????A????????@?????B@B?D?B?F@D?B?H@H?F?D@B?@?@????C?CAC?CAGAGCG?CAA?AAA@??ABABCBABAB?@?@?BBBBD@D@F?DCDCBCBEBEBC@EBEBC@EBEBC@EBC@CBCBCBCBCBEBC@EBE@C@C@ABCBCBEBCBE@E@G@G@EBEBEBGBEBG@GBG@G?G?E?ECCEAEAEAECEECECEEECEECECCCCECCECCACACAEACAECECEEECEECCEEECECECECECEEECECECEAECEAECGCGCEEECE?C@CBE@C?CCECC?E?CBCFADCFCDABC@A@A?@A????A?A??@A???@A??@A??????@??A?@@?@??????A?A?AA???????????A?@???????@??@???@A??@?A???A@A?A??????@A??A??@A@???@???@?B????@??A?AA????A?????????A???@?????@?@?@?@?@A??@??????A???@@?A?A???????????A???A?A??????????@??@???A??@A???A@?A??@?@A@?@?@????@????A?????A?A?A?????A??????A????DA??????A??@????A???@??@?????@??A?????????@??????A?????CAAA?AAAA@???@A?????????@@@@?@????@@???????????????A?????????A???AA????????A?????A???@@@??@A?????A?@?????@??A?A@A@A@????@I@????B?@?A@??@?@ABCBA@A???\" }, { \"begin\": \"2020-07-15T13:45:38.059Z\", \"end\": \"2020-07-15T13:51:20.059Z\", \"mode\": \"BUS\", \"modeDetails\": \"Line 69A (Hauptbahnhof - Simmering) (BUS) from Am Kanal to Arsenalsteg | Sensor mode Bus routed to Bus | GPSQuality(GOOD)\", \"distanceMeters\": 2143.843162687046, \"route\": \"ch_eHk_dcB????yA`ByAxA??gBbB??}AlA??{BrA??cBp@uBn@??cBZ??s@`C????????????????????AnD??ElC??EjCGlC??EtB??EtBGtB??IxD??KpC????????????????????A`D??IdE????c@xC??_AlB??CzC??m@lC??_AbB????????????qA`C????wBxD??_BfC??wAzB????}A~B????????????????iAbB??gAdB??gB`C??q@~@s@~@s@~@KW\" }, { \"begin\": \"2020-07-15T13:51:29.059Z\", \"end\": \"2020-07-15T14:32:45.059Z\", \"mode\": \"WALK\", \"modeDetails\": \" | Sensor mode Walk | GPSQuality(BAD : large time gap)\", \"distanceMeters\": 1435.4269300101091, \"route\": \"mhaeHq{_cBE^@?AAACEAEAE?E?C?CACCCAC?E?CACCAG?CBEBABA@CBCBAB?@D?@?BCFA@C@CBAB?B@B@@@A@C@C@A@@@B?DABAB?B?@@@??@?BABA@?@?@B?D?D@D@F@D@D@F@F@D@FBD@F@FBH@FBHBFBFBFBHBF@HBF@DBF@FBDBDBDBFBD@FBD@D?B?@???????A??????????????@@?B@DBDBF@DBFBDBFBD@FBDBFBFBDBDBDBFBDBFBFBDBFBD@F@FBFBDB@DAD?DAB@D@B@@@A@CBC@CB?BDBFBF@FBD@BDBB@F@F@H?H@H@H@HBHBF@DBF@DBFBBDDDBDBDBDDBDBFBDDFBD@FBFBD@FBFBDBFBDDDDFDFBDDDBDBDBBBBBDBBBBD@B@DABABEBCBCBE@G@G@G@GBE@EBG@E?G@G?E@G?E@GBE@IBGBGDGBGBEBG@E@E@G@E@G@G@G@E@G@G?E@G?G@EBEBEBEBAD?D@FBDBBDDD@F@B@D?D@B?B@B@B??DJ?@@@?@B@LLDBFB@@@?@@@???@@@?@@@???@@@???@@@?@?@@??@@@?@?@@??@?@@@@DBB@NZ?@?@@@?@@??@?@?@@??@??@B??????????????AA???????A????EM??AC????AC?A???A????CG?AAC????????CI?AAA?A???????A??A????A??AE?????AA????A?????AAA?AAC?????A??????AAAE???A????M_@AACEAA??A??AAA??AAA?AA??MKAACCCACCECCCCACCEECCEAE?E@EDEDEFCDAHCHAHAHAH?FAF?DADCBADADAF?FAHAFAFAFAF?H?FAF?FAFCFCDCFCDCDCDEDCBEDCBCDAFCF?FAF@F?FAD?DADAFADCBADCDCDCDAFCDAFADAFCDCDCDCBADADADCDADCDADCFAH?HAF?HAF?F?H?F@FAD?FCFADAHAFAFAFAHAFADCFAF?D?F?D?D?FADADAFAFCFCHAFCFCHCDCFCDEBEDEBEDC@E?EACCEECACCCACCCCCCACCCCCCECCACCCACACA@A@A@?@?@?@?@?????????A??A?A?A?A?@@@?????????@?????@A@ABU??EH?@A@?@ABEJ?@ABA@CHW|@K^ADAH?@?????C?????A???@?????A?E?A?C?????J?@????A?@????????@???A????????????\" } ] } The format uses the encoded polyline algorithm for encoding the polylines of the trips ( sample code ). We will, however, for the project also try to output GeoJSON files directly, which might be easier to consume.","title":"Output"},{"location":"tools/mode/integration/#support-and-contact","text":"For any questions, contact Martin Stubenschrott directly at martin.stubenschrott@ait.ac.at","title":"Support and Contact"},{"location":"tools/mode/integration/#important-notes","text":"You might get an API key for third-party routing services (e.g. Google Router). Make sure to keep that confident and only use it for testing and sparsingly. Overuse of the API key may result in high fees, as only about 20.000 requests/per month are free! For production, we must use a different key being used for billing!","title":"Important Notes"},{"location":"tools/mode/examples/android/","text":"MODE Integration Guide for data collection on Android devices The MODE smartphone library ( libmode ) provides a complete, out-of-the-box solution for mobility data collection via smartphones. It is built as an autonomous component that can easily be integrated into third party apps. For the standard use-case (trip data collection) the library can be integrated into existing Apps with little effort: The most recent version of the MODE library will be provided by AIT upon request . You will also be granted Git access to https://gitlab.ait.ac.at/energy/commons/smartsurvey/android/smartsurvey_demo_app/ which hosts a very basic demo app, showing how to use the MODE library. Step 1: Add the library to the App as a build dependency The library can be added to the App by adding a corresponding dependency to the build. The following example shows how to add libmode to the build.gradle of an Android App. We will provide the library with enough meta-data according to the Maven standard. When putting the library into your ~/.m2/repository/ folder, you can add the dependency like this: dependencies { implementation 'at.ac.ait.mode:libmode:2.0.6' } Of course, you can also just copy the libmode.jar file to a libs folder in your project, and add the dependency like this: dependencies { implementation fileTree(dir: 'libs', include: ['*.jar']) } Step 2: Initialize the library and start data collection The library needs to be initialized before it can start recording data: private void initLibMode() { Context ctx = getApplicationContext(); Mode.initialize(ctx); // Optional: allow data uploading via cellular network (otherwise data only gets uploaded when on WiFi) Mode.getConfigurationEntry(Configuration.CONFIG_METERED_UPLOADING_ALLOWED).setBooleanValue(ctx, true); // set Backend Server Mode.setBaseServerURL(\"http://localhost:8080\"); // Set OAUTH2 token manually // This will be sent to the backend service on each request Mode.setAuthorizationToken(\"MY_OAUTH2_TOKEN\"); // Start Mode.startSurvey(); } Note that we need to ask first for permission to access the location. ( ANDROID_PERMISSION_ACCESS_FINE_LOCATION ). Only when this is granted, we should initialize the library as in the example above. The provided demo app shows how to handle this. Support and Contact For any questions, contact Martin Stubenschrott directly at martin.stubenschrott@ait.ac.at","title":"MODE Integration Guide for data collection on Android devices"},{"location":"tools/mode/examples/android/#mode-integration-guide-for-data-collection-on-android-devices","text":"The MODE smartphone library ( libmode ) provides a complete, out-of-the-box solution for mobility data collection via smartphones. It is built as an autonomous component that can easily be integrated into third party apps. For the standard use-case (trip data collection) the library can be integrated into existing Apps with little effort: The most recent version of the MODE library will be provided by AIT upon request . You will also be granted Git access to https://gitlab.ait.ac.at/energy/commons/smartsurvey/android/smartsurvey_demo_app/ which hosts a very basic demo app, showing how to use the MODE library. Step 1: Add the library to the App as a build dependency The library can be added to the App by adding a corresponding dependency to the build. The following example shows how to add libmode to the build.gradle of an Android App. We will provide the library with enough meta-data according to the Maven standard. When putting the library into your ~/.m2/repository/ folder, you can add the dependency like this: dependencies { implementation 'at.ac.ait.mode:libmode:2.0.6' } Of course, you can also just copy the libmode.jar file to a libs folder in your project, and add the dependency like this: dependencies { implementation fileTree(dir: 'libs', include: ['*.jar']) } Step 2: Initialize the library and start data collection The library needs to be initialized before it can start recording data: private void initLibMode() { Context ctx = getApplicationContext(); Mode.initialize(ctx); // Optional: allow data uploading via cellular network (otherwise data only gets uploaded when on WiFi) Mode.getConfigurationEntry(Configuration.CONFIG_METERED_UPLOADING_ALLOWED).setBooleanValue(ctx, true); // set Backend Server Mode.setBaseServerURL(\"http://localhost:8080\"); // Set OAUTH2 token manually // This will be sent to the backend service on each request Mode.setAuthorizationToken(\"MY_OAUTH2_TOKEN\"); // Start Mode.startSurvey(); } Note that we need to ask first for permission to access the location. ( ANDROID_PERMISSION_ACCESS_FINE_LOCATION ). Only when this is granted, we should initialize the library as in the example above. The provided demo app shows how to handle this.","title":"MODE Integration Guide for data collection on Android devices"},{"location":"tools/mode/examples/android/#support-and-contact","text":"For any questions, contact Martin Stubenschrott directly at martin.stubenschrott@ait.ac.at","title":"Support and Contact"},{"location":"tools/mode/examples/backend/","text":"MODE Integration Guide for backend data analysis We provide a basic REST entpoint as a Docker image which listens on an endpoint /analyze . The input is binary movement data gathered from the smartphone apps (see here and here ). Output is a JSON string of the taken activities. Using the image Step 1: Copy exported image to server Use your tool of choice. Step 2: Import image on server docker load -i modesrv-1.0.0.tar.gz Step 3: Create Docker container # docker stop modesrv # docker rm modesrv docker create --name modesrv -p 8080:8080 --restart=unless-stopped ait/modesrv:1.0.0 Step 4: Run Docker image and follow its logs docker start modesrv docker logs -f modesrv Step 5: Test the image Follow these instructions to send example data to localhost:8080 and see some JSON output. Important Notes The MODE backend service uses third-party routing services (e.g. Google Router). At this stage, a sample API key is hardcoded in the project. Make sure to keep that key confident and only use it for testing and sparsingly. Overuse of the API key may result in high fees, as only about 20.000 requests/per month are free! For production, we must use a different key being used for billing. This will be set via an ENV variable to the Docker image.","title":"MODE Integration Guide for backend data analysis"},{"location":"tools/mode/examples/backend/#mode-integration-guide-for-backend-data-analysis","text":"We provide a basic REST entpoint as a Docker image which listens on an endpoint /analyze . The input is binary movement data gathered from the smartphone apps (see here and here ). Output is a JSON string of the taken activities.","title":"MODE Integration Guide for backend data analysis"},{"location":"tools/mode/examples/backend/#using-the-image","text":"Step 1: Copy exported image to server Use your tool of choice. Step 2: Import image on server docker load -i modesrv-1.0.0.tar.gz Step 3: Create Docker container # docker stop modesrv # docker rm modesrv docker create --name modesrv -p 8080:8080 --restart=unless-stopped ait/modesrv:1.0.0 Step 4: Run Docker image and follow its logs docker start modesrv docker logs -f modesrv Step 5: Test the image Follow these instructions to send example data to localhost:8080 and see some JSON output.","title":"Using the image"},{"location":"tools/mode/examples/backend/#important-notes","text":"The MODE backend service uses third-party routing services (e.g. Google Router). At this stage, a sample API key is hardcoded in the project. Make sure to keep that key confident and only use it for testing and sparsingly. Overuse of the API key may result in high fees, as only about 20.000 requests/per month are free! For production, we must use a different key being used for billing. This will be set via an ENV variable to the Docker image.","title":"Important Notes"},{"location":"tools/mode/examples/ios/","text":"MODE Integration Guide for data collection on iOS devices The MODE smartphone library ( libmode ) provides a complete, out-of-the-box solution for mobility data collection via smartphones. It is built as an autonomous component that can easily be integrated into third party apps. For the standard use-case (trip data collection) the library can be integrated into existing Apps with little effort: The most recent version of the MODE library will be provided by AIT upon request . You will also be granted Git access to https://gitlab.ait.ac.at/energy/commons/smartsurvey/ios/SmartSurveyDemoApp/ which hosts a very basic demo app, showing how to use the MODE library. Step 1: Add the library to the App as a build dependency To make it easy to use libmode, we have published the library on our Gitlab in https://gitlab.ait.ac.at/energy/commons/smartsurvey/ios/Libmode.git You can then add the swift package as a dependency in XCode. Step 2: Initialize the library and start data collection The library needs to be initialized before it can start recording data: func initLibMode() { // Allow uploading of trips also on cellular connection (default is Wi-Fi only) UserDefaultsStore.cellularUploadAllowed = true // Set the OAUTH2 token and the server address where trips are uploaded and analyzed. Mode.setAuthorizationToken(token: \"myuser:abcdef123456\") // Define where the backend server is located, waiting for the trips to be // uploaded // // By default, access to non-https is disabled by iOS. For development purposes one can change // Info.plist like described here, but that might not allow the app to be // accepted in the app store, so only use during development or install a local // SSL certificate: // https://stackoverflow.com/questions/63597760/not-allowed-to-make-request-to-local-host-swift Mode.setBaseServerURL(surveyUrl: \"http://localhost:8080\") // This call starts recording trip data. Once it has detected a trip, it is uploaded and analyzed Mode.startSurvey() } Also make sure that you request background location access in the manifest of the project. Support and Contact For any questions, contact Martin Stubenschrott directly at martin.stubenschrott@ait.ac.at","title":"MODE Integration Guide for data collection on iOS devices"},{"location":"tools/mode/examples/ios/#mode-integration-guide-for-data-collection-on-ios-devices","text":"The MODE smartphone library ( libmode ) provides a complete, out-of-the-box solution for mobility data collection via smartphones. It is built as an autonomous component that can easily be integrated into third party apps. For the standard use-case (trip data collection) the library can be integrated into existing Apps with little effort: The most recent version of the MODE library will be provided by AIT upon request . You will also be granted Git access to https://gitlab.ait.ac.at/energy/commons/smartsurvey/ios/SmartSurveyDemoApp/ which hosts a very basic demo app, showing how to use the MODE library. Step 1: Add the library to the App as a build dependency To make it easy to use libmode, we have published the library on our Gitlab in https://gitlab.ait.ac.at/energy/commons/smartsurvey/ios/Libmode.git You can then add the swift package as a dependency in XCode. Step 2: Initialize the library and start data collection The library needs to be initialized before it can start recording data: func initLibMode() { // Allow uploading of trips also on cellular connection (default is Wi-Fi only) UserDefaultsStore.cellularUploadAllowed = true // Set the OAUTH2 token and the server address where trips are uploaded and analyzed. Mode.setAuthorizationToken(token: \"myuser:abcdef123456\") // Define where the backend server is located, waiting for the trips to be // uploaded // // By default, access to non-https is disabled by iOS. For development purposes one can change // Info.plist like described here, but that might not allow the app to be // accepted in the app store, so only use during development or install a local // SSL certificate: // https://stackoverflow.com/questions/63597760/not-allowed-to-make-request-to-local-host-swift Mode.setBaseServerURL(surveyUrl: \"http://localhost:8080\") // This call starts recording trip data. Once it has detected a trip, it is uploaded and analyzed Mode.startSurvey() } Also make sure that you request background location access in the manifest of the project.","title":"MODE Integration Guide for data collection on iOS devices"},{"location":"tools/mode/examples/ios/#support-and-contact","text":"For any questions, contact Martin Stubenschrott directly at martin.stubenschrott@ait.ac.at","title":"Support and Contact"},{"location":"tools/sensorsIntegration/","text":"MQTT Introduction MQTT is one of the most wide IoT protocols used both due to its scalable capabilities and the number of platforms for Smart Cities accepting this protocol. For that reason, MQTT has been introduced in the Smart Spot to be a protocol to report values to 3rd party platforms. Nowadays, it can be used only to report values, while managing it will be carried out using the original Smart Spot protocol, which is LwM2M. The management can be used both from the platform provided by Libelium so called Homard. This version of MQTT also includes significant security and reliability improvements compared to earlier versions. For example, authentication and authorization mechanisms were added to enable safer and more controlled communication between devices. Session management was also improved, and a \"last will and testament\" mechanism was added to ensure that devices disconnect properly in case of a network failure. Overall, MQTT v3.1.1 is considered a mature and stable version of the MQTT protocol and is widely used in a variety of IoT applications, such as environmental monitoring, vehicle telemetry, asset tracking, and more. Features of MQTT Supported Security MQTT v3.1.1 supports the two main security mechanisms to ensure secure communication between devices. These include: User and password-based authentication: MQTT devices can be authenticated using user and password credentials stored on the MQTT server. 1.2 Digital certificates: MQTT devices can be authenticated using digital certificates, which provides greater security and mutual authentication between devices. Both of them can be configured using the remote device management platform Homard provided by Libelium. MQTT Ports MQTT uses by default two different ports for communication: TCP port 1883: This is the default port used for unencrypted connection. MQTT clients connect to the MQTT server using this port. The communication between the client and the server is performed without encryption, meaning that the exchanged data is not protected. TCP port 8883: This is the port used for encrypted connection. MQTT clients can connect to the MQTT server using this port and establish a secure connection over TLS (Transport Layer Security). TLS provides an additional layer of security to MQTT communication, protecting the exchanged data between the client and the server It is important to note that default ports may vary depending on the MQTT provider or the MQTT server configuration. Therefore, the MQTT server documentation should be consulted to confirm the ports used in MQTT communication and ensure that clients are properly configured to connect to the server. Configuration The vertical data concept involves grouping and organising the data reported by the device into different verticals depending on the transmitted data set. The grouping of this data aligns with the definition of the official Data Models of the FIWARE platform described on the website https://smartdatamodels.org/. Given the versatility of the Smart Spot to be configurable at the hardware level during the order with different types of sensors, not all data verticals have to be active on the device. The device transmits data using the necessary verticals depending on the hardware configuration of the device. Additionally, it is possible to enable or disable specific verticals separately through the Homard device remote platform as indicated in the next section. The definition of a vertical at MQTT integration level is mainly composed of two elements: Topic: The topic for publishing each data set of the vertical will be composed based on a default prefix or a topic prefix specified by the client through the Homard device remote platform. The topic prefix must have the following format /{base_apikey}/{device_id} <br> where 'base_apikey' is usually an identifying key for a set of devices and 'device_id' is a specific name for the device. If a topic prefix is not configured, the device will use a default one. This topic prefix will be internally modified by the SmartSpot depending on the vertical. Specifically, the SmartSpot will add both the \u2018/attrs\u2019 postfix corresponding to the one required by the FIWARE IoT Agents to receive data and the 'lower_vertical_suffix' and 'upper_vertical_suffix' fields within the topic prefix to separate the data into different topics, one for each vertical, resulting in the final topic being composed in the following way: /{base_apikey}{lower_vertical_suffix}/{device_id}_{upper_vertical_suffix}/attrs <br> The following table illustrates an example of the prefix formation according to the publication vertical. Message: The data published through JSON-encoded text messages on each of the specified topics contain at least the data from the different sensors included in your Smart Spot device for each vertical, and may also include extra information regarding how the device is configured. This extra information is provided as it may be useful to the user and may enable device configuration via MQTT protocol in the future, although this feature is not currently implemented. The following table illustrate the information that can be included in each vertical depending on your device's configuration. The example message will be: ```{ \"TimeInstant\": \"2023-03-20T13:50:00Z\", \"period\": 5, \"status\": \"connected\", \"no2-a4\": 0.640869140625, \"ox-a431\": 28.586013793945312, \"co-a4\": 91.760452270507812, \"so2-a4\": 0.927800178527832, \"pm10\": 49.334125518798828, \"pm2\": 15.33404541015625, \"pm1\": 2.1444158554077148 } ``` It is important to note that if a sensor is not connected or not working properly, the field is still included in the transmitted message, but its value is set to 'null'. The reasons for carrying out this procedure are mainly to: Help identify device malfunctions for maintenance tasks. Prevent the introduction of '0' or repetitive non-real values that can cause confusion when data persists in databases or is represented in graphs.","title":"MQTT"},{"location":"tools/sensorsIntegration/#mqtt","text":"","title":"MQTT"},{"location":"tools/sensorsIntegration/#introduction","text":"MQTT is one of the most wide IoT protocols used both due to its scalable capabilities and the number of platforms for Smart Cities accepting this protocol. For that reason, MQTT has been introduced in the Smart Spot to be a protocol to report values to 3rd party platforms. Nowadays, it can be used only to report values, while managing it will be carried out using the original Smart Spot protocol, which is LwM2M. The management can be used both from the platform provided by Libelium so called Homard. This version of MQTT also includes significant security and reliability improvements compared to earlier versions. For example, authentication and authorization mechanisms were added to enable safer and more controlled communication between devices. Session management was also improved, and a \"last will and testament\" mechanism was added to ensure that devices disconnect properly in case of a network failure. Overall, MQTT v3.1.1 is considered a mature and stable version of the MQTT protocol and is widely used in a variety of IoT applications, such as environmental monitoring, vehicle telemetry, asset tracking, and more.","title":"Introduction"},{"location":"tools/sensorsIntegration/#features-of-mqtt","text":"Supported Security MQTT v3.1.1 supports the two main security mechanisms to ensure secure communication between devices. These include: User and password-based authentication: MQTT devices can be authenticated using user and password credentials stored on the MQTT server. 1.2 Digital certificates: MQTT devices can be authenticated using digital certificates, which provides greater security and mutual authentication between devices. Both of them can be configured using the remote device management platform Homard provided by Libelium. MQTT Ports MQTT uses by default two different ports for communication: TCP port 1883: This is the default port used for unencrypted connection. MQTT clients connect to the MQTT server using this port. The communication between the client and the server is performed without encryption, meaning that the exchanged data is not protected. TCP port 8883: This is the port used for encrypted connection. MQTT clients can connect to the MQTT server using this port and establish a secure connection over TLS (Transport Layer Security). TLS provides an additional layer of security to MQTT communication, protecting the exchanged data between the client and the server It is important to note that default ports may vary depending on the MQTT provider or the MQTT server configuration. Therefore, the MQTT server documentation should be consulted to confirm the ports used in MQTT communication and ensure that clients are properly configured to connect to the server. Configuration The vertical data concept involves grouping and organising the data reported by the device into different verticals depending on the transmitted data set. The grouping of this data aligns with the definition of the official Data Models of the FIWARE platform described on the website https://smartdatamodels.org/. Given the versatility of the Smart Spot to be configurable at the hardware level during the order with different types of sensors, not all data verticals have to be active on the device. The device transmits data using the necessary verticals depending on the hardware configuration of the device. Additionally, it is possible to enable or disable specific verticals separately through the Homard device remote platform as indicated in the next section. The definition of a vertical at MQTT integration level is mainly composed of two elements: Topic: The topic for publishing each data set of the vertical will be composed based on a default prefix or a topic prefix specified by the client through the Homard device remote platform. The topic prefix must have the following format /{base_apikey}/{device_id} <br> where 'base_apikey' is usually an identifying key for a set of devices and 'device_id' is a specific name for the device. If a topic prefix is not configured, the device will use a default one. This topic prefix will be internally modified by the SmartSpot depending on the vertical. Specifically, the SmartSpot will add both the \u2018/attrs\u2019 postfix corresponding to the one required by the FIWARE IoT Agents to receive data and the 'lower_vertical_suffix' and 'upper_vertical_suffix' fields within the topic prefix to separate the data into different topics, one for each vertical, resulting in the final topic being composed in the following way: /{base_apikey}{lower_vertical_suffix}/{device_id}_{upper_vertical_suffix}/attrs <br> The following table illustrates an example of the prefix formation according to the publication vertical. Message: The data published through JSON-encoded text messages on each of the specified topics contain at least the data from the different sensors included in your Smart Spot device for each vertical, and may also include extra information regarding how the device is configured. This extra information is provided as it may be useful to the user and may enable device configuration via MQTT protocol in the future, although this feature is not currently implemented. The following table illustrate the information that can be included in each vertical depending on your device's configuration. The example message will be: ```{ \"TimeInstant\": \"2023-03-20T13:50:00Z\", \"period\": 5, \"status\": \"connected\", \"no2-a4\": 0.640869140625, \"ox-a431\": 28.586013793945312, \"co-a4\": 91.760452270507812, \"so2-a4\": 0.927800178527832, \"pm10\": 49.334125518798828, \"pm2\": 15.33404541015625, \"pm1\": 2.1444158554077148 } ``` It is important to note that if a sensor is not connected or not working properly, the field is still included in the transmitted message, but its value is set to 'null'. The reasons for carrying out this procedure are mainly to: Help identify device malfunctions for maintenance tasks. Prevent the introduction of '0' or repetitive non-real values that can cause confusion when data persists in databases or is represented in graphs.","title":"Features of MQTT"},{"location":"tools/sensorsIntegration/integration/","text":"Libelium IoT sensors Usage & Integration Guide for Third-Party Services Generic usage instructions for SmartSpot and other kind of sensors Instructions on how to use the tool. The user's manual is available at this link The importance of using MQTT connectors in the project is highlighted by the need to establish efficient and standardized communication in an environment where various technologies converge. MQTT (Message Queuing Telemetry Transport) stands out as a lightweight and robust protocol that enables real-time data transmission between devices, ensuring seamless connectivity in heterogeneous environments. In the context of air quality monitoring, data standardization through MQTT facilitates the integration of devices from different manufacturers and technologies, promoting interoperability and simplifying the development of IoT architecture. The efficiency of MQTT is particularly evident in the effective management of messages, minimizing bandwidth and optimizing the transmission of critical information about sensors. Thus, the choice of MQTT connectors not only improves the coherence and speed of communication but also establishes a solid foundation for scalability and future system expansion, ensuring robustness and reliability in projects that require efficient real-time data management. Step 1: Requesting Access Credentials Instructions on how to request access credentials. To get started with your MQTT broker, please contact a.pujante@libelium.com Step 2: Receiving Credentials Once you have your MQTT connector, the next step is to integrate it into your platform or architecture in order to be able to receive the data on it Step 4: Testing the Integration To check the integration, it will be necessary to see if the data is arriving correctly to the platform and can be displayed correctly.. Support and Contact Contact for support, issues and information: a.pujante@libelium.com, e.illueca@libelium.com and aj.jara@libelium.com","title":"Libelium IoT sensors Usage & Integration Guide for Third-Party Services"},{"location":"tools/sensorsIntegration/integration/#libelium-iot-sensors-usage-integration-guide-for-third-party-services","text":"Generic usage instructions for SmartSpot and other kind of sensors Instructions on how to use the tool. The user's manual is available at this link The importance of using MQTT connectors in the project is highlighted by the need to establish efficient and standardized communication in an environment where various technologies converge. MQTT (Message Queuing Telemetry Transport) stands out as a lightweight and robust protocol that enables real-time data transmission between devices, ensuring seamless connectivity in heterogeneous environments. In the context of air quality monitoring, data standardization through MQTT facilitates the integration of devices from different manufacturers and technologies, promoting interoperability and simplifying the development of IoT architecture. The efficiency of MQTT is particularly evident in the effective management of messages, minimizing bandwidth and optimizing the transmission of critical information about sensors. Thus, the choice of MQTT connectors not only improves the coherence and speed of communication but also establishes a solid foundation for scalability and future system expansion, ensuring robustness and reliability in projects that require efficient real-time data management. Step 1: Requesting Access Credentials Instructions on how to request access credentials. To get started with your MQTT broker, please contact a.pujante@libelium.com Step 2: Receiving Credentials Once you have your MQTT connector, the next step is to integrate it into your platform or architecture in order to be able to receive the data on it Step 4: Testing the Integration To check the integration, it will be necessary to see if the data is arriving correctly to the platform and can be displayed correctly.. Support and Contact Contact for support, issues and information: a.pujante@libelium.com, e.illueca@libelium.com and aj.jara@libelium.com","title":"Libelium IoT sensors Usage &amp; Integration Guide for Third-Party Services"},{"location":"tools/superset/","text":"Apache Superset Introduction Apache Superset is a sophisticated, open-source data exploration and visualisation platform designed to transform the way analysts and businesses interact with their data. As a versatile web application, it empowers users with tools for deep data exploration and the creation of beautifully rendered visualisations and dashboards. Superset is renowned for its user-friendly interface, democratising data analysis by enabling users of all skill levels to engage with complex datasets, design compelling visual narratives, and derive actionable insights. Offering a rich array of features, including a variety of chart types, interactive dashboards, and an integrated SQL editor, Apache Superset stands out as a comprehensive solution for modern data analytics and business intelligence needs. Features of Superset Data Exploration: Superset allows users to explore datasets through simple, intuitive, and interactive visualisations. It offers a variety of chart types and the ability to dive deep into the details of the data. Easy Visualisation Creation: Users can easily create and share visualisations without the need for programming. It includes a rich set of visualisations like line charts, bar charts, and scatter plots, among others. Interactive Dashboards: Superset enables the creation of dynamic dashboards that are interactive and customisable. These dashboards can be shared with others and include various visualisations for comprehensive data analysis. SQL IDE: It comes with an integrated SQL IDE, which allows users to write, run, and visualise the results of SQL queries in a user-friendly environment. Security and Integration: Apache Superset offers robust security features, including role-based permission control. It can be integrated with most SQL-based data sources, including traditional databases and newer SQL-speaking data engines. Scalability and Performance: It is designed to be highly scalable, easily handling large-scale data exploration jobs. It also provides features for optimising query performance. Open Source and Extensible: Being open source allows for customisation and extension, enabling developers to contribute to its features or adapt it to specific needs. Community-Driven: Apache Superset is backed by a strong community, which contributes to its continuous development and improvement. Use Case Scenario Context: In an urban citizen observatory, members of the community actively participate in monitoring and analysing environmental conditions. One of the critical concerns in many cities is air quality, which directly impacts public health and quality of life. Objective: To monitor, analyse, and visualise air quality data across different parts of the city to identify pollution hotspots, understand temporal trends, and engage the public in environmental awareness and decision-making processes. Implementation Using Apache Superset: 1) Data Collection: - Deploy HOPU sensors across the city to measure air quality indicators like PM2.5, PM10, NO2, CO2, and Ozone. - Sensors transmit data to a centralised database, storing time-stamped readings and location data. 2) Data Integration with Apache Superset: - Connect Superset to the database where sensor data is stored. - Ensure data is updated in real-time or at regular intervals for accurate analysis. 3) Exploratory Data Analysis: - Use Superset\u2019s SQL IDE to query specific aspects of the data, like identifying times of day with peak pollution levels. - Create visualisations to explore correlations, such as between traffic intensity and NO2 levels. 4) Dashboard Creation: - Develop an interactive dashboard in Superset, displaying various visualisations: - Maps showing real-time pollution levels across different city areas. - Line charts depicting temporal trends in air quality. - Bar charts comparing air quality on weekdays vs weekends. - Include filters to allow users to select specific dates, times, or areas for detailed analysis. 5) Data-Driven Decisions: - Use insights from the Superset dashboard to guide community actions and policy recommendations. - For example, advocate for traffic reduction measures in areas with consistently high pollution levels. References Apache Superset documentation GREENGAGE catalogue entry","title":"Apache Superset"},{"location":"tools/superset/#apache-superset","text":"","title":"Apache Superset"},{"location":"tools/superset/#introduction","text":"Apache Superset is a sophisticated, open-source data exploration and visualisation platform designed to transform the way analysts and businesses interact with their data. As a versatile web application, it empowers users with tools for deep data exploration and the creation of beautifully rendered visualisations and dashboards. Superset is renowned for its user-friendly interface, democratising data analysis by enabling users of all skill levels to engage with complex datasets, design compelling visual narratives, and derive actionable insights. Offering a rich array of features, including a variety of chart types, interactive dashboards, and an integrated SQL editor, Apache Superset stands out as a comprehensive solution for modern data analytics and business intelligence needs.","title":"Introduction"},{"location":"tools/superset/#features-of-superset","text":"Data Exploration: Superset allows users to explore datasets through simple, intuitive, and interactive visualisations. It offers a variety of chart types and the ability to dive deep into the details of the data. Easy Visualisation Creation: Users can easily create and share visualisations without the need for programming. It includes a rich set of visualisations like line charts, bar charts, and scatter plots, among others. Interactive Dashboards: Superset enables the creation of dynamic dashboards that are interactive and customisable. These dashboards can be shared with others and include various visualisations for comprehensive data analysis. SQL IDE: It comes with an integrated SQL IDE, which allows users to write, run, and visualise the results of SQL queries in a user-friendly environment. Security and Integration: Apache Superset offers robust security features, including role-based permission control. It can be integrated with most SQL-based data sources, including traditional databases and newer SQL-speaking data engines. Scalability and Performance: It is designed to be highly scalable, easily handling large-scale data exploration jobs. It also provides features for optimising query performance. Open Source and Extensible: Being open source allows for customisation and extension, enabling developers to contribute to its features or adapt it to specific needs. Community-Driven: Apache Superset is backed by a strong community, which contributes to its continuous development and improvement.","title":"Features of Superset"},{"location":"tools/superset/#use-case-scenario","text":"","title":"Use Case Scenario"},{"location":"tools/superset/#context","text":"In an urban citizen observatory, members of the community actively participate in monitoring and analysing environmental conditions. One of the critical concerns in many cities is air quality, which directly impacts public health and quality of life.","title":"Context:"},{"location":"tools/superset/#objective","text":"To monitor, analyse, and visualise air quality data across different parts of the city to identify pollution hotspots, understand temporal trends, and engage the public in environmental awareness and decision-making processes.","title":"Objective:"},{"location":"tools/superset/#implementation-using-apache-superset","text":"1) Data Collection: - Deploy HOPU sensors across the city to measure air quality indicators like PM2.5, PM10, NO2, CO2, and Ozone. - Sensors transmit data to a centralised database, storing time-stamped readings and location data. 2) Data Integration with Apache Superset: - Connect Superset to the database where sensor data is stored. - Ensure data is updated in real-time or at regular intervals for accurate analysis. 3) Exploratory Data Analysis: - Use Superset\u2019s SQL IDE to query specific aspects of the data, like identifying times of day with peak pollution levels. - Create visualisations to explore correlations, such as between traffic intensity and NO2 levels. 4) Dashboard Creation: - Develop an interactive dashboard in Superset, displaying various visualisations: - Maps showing real-time pollution levels across different city areas. - Line charts depicting temporal trends in air quality. - Bar charts comparing air quality on weekdays vs weekends. - Include filters to allow users to select specific dates, times, or areas for detailed analysis. 5) Data-Driven Decisions: - Use insights from the Superset dashboard to guide community actions and policy recommendations. - For example, advocate for traffic reduction measures in areas with consistently high pollution levels.","title":"Implementation Using Apache Superset:"},{"location":"tools/superset/#references","text":"Apache Superset documentation GREENGAGE catalogue entry","title":"References"},{"location":"tools/superset/usage/","text":"Apache Superset Usage Guide Generic usage instructions for Apache Superset - Instructions on how to use the tool. - In case there is external user manual make a small summary and refer to it. - Otherwise, provide enough explanations and snapshots to let a user get started with the tool usage. Include links to URLs where the tool might be deployed. Introduction explaining the importance of integrating [Tool Name] to make your tool part of the GREENGAGE engine. This implies that your tool has to use the single sign on (SSO) service provided by GREENGAGE. However, you should also describe not only how your tool is integrated with GREENGAGE's SSO but also how other GREENGAGE tools may integrate with your tools. Feel free to add and rename the integrate steps listed below: Step 1: Log In - Access to the Superset deploy of Greengage. - First of all, you should login using your Greengage login in the keycloak authentication webpage that will appear. - You will reach to the main view of the tool as shown in the figure below. Step 2: Creating datasets Click on the DATASETS button at the top-left menu and click on the +DATASET button at the top-right part of the view that you have arrived. Introduce the database, schema and table that you want to import from the possible sources and click CREATE DATASET AND CREATE CHART button. NOTE: In case that you want to connect Apache Superset to a new database or source contact with ruben.sanchez@deusto.es Step 3: Creating charts Click on the CHARTS button at the top-left menu and click on the +CHART button at the top-right part of the view that you have arrived. For the chart creation you should select a dataset and a chart type to show the data. Once in the chart editing view, you should add a name to the chart. In this view you have two columns: the left one has all the rows from the dataset yout selected and the right one has the settings for the chart. Once you have selected the right settings, click on the CREATE CHART button at the bottom of the second column. Note that to upload the chart you will have to click again in a button that updates it in the same position as the previous one. Step 4: Creating dashboards Click on the DASHBOARDS button at the top-left menu and click on the +DASHBOARD button at the top-right part of the view that you have arrived. Once in the dashboard creation view, you should add a name to the dashboard. In this view you can add already created charts that appear in the right side or create new charts. Furthermore, you can add new Layout elements such as headers, text or dividers. Extra: Executing SQL queries to imported datasets Accessing to the SQL view using the button at the top left menu you can access a SQL statement executor against the datasets imported in Superset.","title":"Apache Superset Usage Guide"},{"location":"tools/superset/usage/#apache-superset-usage-guide","text":"Generic usage instructions for Apache Superset - Instructions on how to use the tool. - In case there is external user manual make a small summary and refer to it. - Otherwise, provide enough explanations and snapshots to let a user get started with the tool usage. Include links to URLs where the tool might be deployed. Introduction explaining the importance of integrating [Tool Name] to make your tool part of the GREENGAGE engine. This implies that your tool has to use the single sign on (SSO) service provided by GREENGAGE. However, you should also describe not only how your tool is integrated with GREENGAGE's SSO but also how other GREENGAGE tools may integrate with your tools. Feel free to add and rename the integrate steps listed below: Step 1: Log In - Access to the Superset deploy of Greengage. - First of all, you should login using your Greengage login in the keycloak authentication webpage that will appear. - You will reach to the main view of the tool as shown in the figure below. Step 2: Creating datasets Click on the DATASETS button at the top-left menu and click on the +DATASET button at the top-right part of the view that you have arrived. Introduce the database, schema and table that you want to import from the possible sources and click CREATE DATASET AND CREATE CHART button. NOTE: In case that you want to connect Apache Superset to a new database or source contact with ruben.sanchez@deusto.es Step 3: Creating charts Click on the CHARTS button at the top-left menu and click on the +CHART button at the top-right part of the view that you have arrived. For the chart creation you should select a dataset and a chart type to show the data. Once in the chart editing view, you should add a name to the chart. In this view you have two columns: the left one has all the rows from the dataset yout selected and the right one has the settings for the chart. Once you have selected the right settings, click on the CREATE CHART button at the bottom of the second column. Note that to upload the chart you will have to click again in a button that updates it in the same position as the previous one. Step 4: Creating dashboards Click on the DASHBOARDS button at the top-left menu and click on the +DASHBOARD button at the top-right part of the view that you have arrived. Once in the dashboard creation view, you should add a name to the dashboard. In this view you can add already created charts that appear in the right side or create new charts. Furthermore, you can add new Layout elements such as headers, text or dividers. Extra: Executing SQL queries to imported datasets Accessing to the SQL view using the button at the top left menu you can access a SQL statement executor against the datasets imported in Superset.","title":"Apache Superset Usage Guide"},{"location":"tools/superset/examples/","text":"Apache Superset Apache Superset cannot be integrated with third party tools, however new datasources can be integrated into Superset. Integrating new datasources to Apache Superset Apache Superset can ingest data from more than 40 sources. Some of the integrations have built-in configuration wizards, as seen in the image below. However, others, only request the connection URL as stated in Superset documentation Once you have added the details for the connection, Apache Superset will create a connection test and provide you with feedback of it. If the connection is successful, the new datasets are ready to be used.","title":"Apache Superset"},{"location":"tools/superset/examples/#apache-superset","text":"Apache Superset cannot be integrated with third party tools, however new datasources can be integrated into Superset.","title":"Apache Superset"},{"location":"tools/superset/examples/#integrating-new-datasources-to-apache-superset","text":"Apache Superset can ingest data from more than 40 sources. Some of the integrations have built-in configuration wizards, as seen in the image below. However, others, only request the connection URL as stated in Superset documentation Once you have added the details for the connection, Apache Superset will create a connection test and provide you with feedback of it. If the connection is successful, the new datasets are ready to be used.","title":"Integrating new datasources to Apache Superset"},{"location":"tools/wordpress/","text":"WordPress Introduction Description WordPress is a versatile Content Management System (CMS). It initially gained popularity for blog creation but has since evolved into a key tool for developing a wide range of websites, including commercial ones. Its flexibility and user-friendly interface make it a go-to solution for website creation. Objective The primary objective of WordPress is to simplify the process of website creation and management. Its user-friendly design, combined with powerful features, makes it an ideal choice for users ranging from novices to seasoned web developers. WordPress aims to enhance website functionality, improve user experience, and provide robust website management tools. Features of WordPress List of Features Easy Installation, Update, and Customization : Streamlines the setup and maintenance process. Automatic System Updates : Ensures the CMS stays current and secure. Multiple Authors and User Roles : Allows for collaborative work with various permission levels. Support for Multiple Blogs : Facilitates the management of several blogs within a single site. Static Page Creation Capability : Enhances the flexibility of content management. Additional Features : Includes categorization of articles and pages, password protection, WYSIWYG editor, email publishing, import options from various platforms, auto-save drafts, comment and communication tools, permalink support, content and comment distribution via RSS and Atom, link management, multimedia file management, plugin and widget support, integrated search functionality, and theme creation system. Use Case Scenario Description A typical use case for WordPress involves creating and managing a professional blog or a commercial website. In this scenario, WordPress serves as the backbone, offering tools for content creation, theme customization, and audience engagement through comments and social media integrations. The scenario demonstrates WordPress' versatility in handling diverse web development needs. References Description For comprehensive guides and technical details about WordPress, the following resources are available: Support guides : https://wordpress.org/documentation/support-guides/ - Provides step-by-step instructions for using WordPress effectively.","title":"WordPress"},{"location":"tools/wordpress/#wordpress","text":"","title":"WordPress"},{"location":"tools/wordpress/#introduction","text":"","title":"Introduction"},{"location":"tools/wordpress/#description","text":"WordPress is a versatile Content Management System (CMS). It initially gained popularity for blog creation but has since evolved into a key tool for developing a wide range of websites, including commercial ones. Its flexibility and user-friendly interface make it a go-to solution for website creation.","title":"Description"},{"location":"tools/wordpress/#objective","text":"The primary objective of WordPress is to simplify the process of website creation and management. Its user-friendly design, combined with powerful features, makes it an ideal choice for users ranging from novices to seasoned web developers. WordPress aims to enhance website functionality, improve user experience, and provide robust website management tools.","title":"Objective"},{"location":"tools/wordpress/#features-of-wordpress","text":"","title":"Features of WordPress"},{"location":"tools/wordpress/#list-of-features","text":"Easy Installation, Update, and Customization : Streamlines the setup and maintenance process. Automatic System Updates : Ensures the CMS stays current and secure. Multiple Authors and User Roles : Allows for collaborative work with various permission levels. Support for Multiple Blogs : Facilitates the management of several blogs within a single site. Static Page Creation Capability : Enhances the flexibility of content management. Additional Features : Includes categorization of articles and pages, password protection, WYSIWYG editor, email publishing, import options from various platforms, auto-save drafts, comment and communication tools, permalink support, content and comment distribution via RSS and Atom, link management, multimedia file management, plugin and widget support, integrated search functionality, and theme creation system.","title":"List of Features"},{"location":"tools/wordpress/#use-case-scenario","text":"","title":"Use Case Scenario"},{"location":"tools/wordpress/#description_1","text":"A typical use case for WordPress involves creating and managing a professional blog or a commercial website. In this scenario, WordPress serves as the backbone, offering tools for content creation, theme customization, and audience engagement through comments and social media integrations. The scenario demonstrates WordPress' versatility in handling diverse web development needs.","title":"Description"},{"location":"tools/wordpress/#references","text":"","title":"References"},{"location":"tools/wordpress/#description_2","text":"For comprehensive guides and technical details about WordPress, the following resources are available: Support guides : https://wordpress.org/documentation/support-guides/ - Provides step-by-step instructions for using WordPress effectively.","title":"Description"},{"location":"tools/wordpress/integration/","text":"Integration WordPress + Keycloak Integration Introduction Integrating Keycloak with WordPress enhances security by implementing Single Sign-On (SSO). This integration allows users to access WordPress using Keycloak credentials, streamlining the login process and improving security. Step 1: Integrating Keycloak in WordPress Access WordPress Admin : Log into wp-admin in WordPress using credentials defined in docker-compose ( WORDPRESS_EMAIL , WORDPRESS_PASSWORD ). Install OAuth SSO Plugin : Go to 'Plugins' > 'Add New'. Search for \"OAuth Single Sign On \u2013 SSO (OAuth Client)\" and click on \"Install Now\". Activate and Configure : After installation, click \"Activate\". You\u2019ll be redirected to the configuration page. Follow the steps in the \"Setup Wizard\". For more details on Keycloak integration, visit the Keycloak WordPress Integration Documentation . Step 2: Configuring Keycloak with WordPress Ensure you have a clientid and secret from Keycloak. Follow the instructions provided in the setup wizard to integrate these credentials into WordPress. WordPress + Discourse Integration Introduction Integrating Discourse with WordPress allows seamless connectivity between WordPress posts and Discourse discussions. This integration enhances community engagement by linking WordPress content with Discourse forums. Step 1: Setting Up API Key in Discourse Admin Access : Log into Discourse as an admin. Create API Key : Navigate to /admin/api/keys . Create an API Key (User Level: \"All Users\", Scope: \"Global\") and save it for later use in WordPress. Step 2: Integrating Discourse in WordPress Access WordPress Admin : Log into wp-admin using docker-compose credentials ( WORDPRESS_EMAIL , WORDPRESS_PASSWORD ). Install WP Discourse Plugin : Go to 'Plugins' > 'Add New'. Search for \"WP Discourse\" and click \"Install Now\". Activate and Configure : After installation, click \"Activate\". A \"Discourse\" option will appear in the sidebar. Configure Plugin Settings : Set the 'Discourse URL'. Use the API Key created in Discourse. In the \"Publishing\" section, enable relevant options. In the \"Commenting\" tab, configure Discourse comments settings. In the \"Webhook\" tab, enable \"Sync Comment Data\". For more information on configuring the WP Discourse plugin, visit the Discourse WordPress Plugin Guide . Important Notes: Ensure to securely store and manage your API keys and credentials. Regularly update your integration plugins to maintain security and functionality. In case of any security concerns or compromised credentials, contact the respective support teams immediately.","title":"Integration"},{"location":"tools/wordpress/integration/#integration","text":"","title":"Integration"},{"location":"tools/wordpress/integration/#wordpress-keycloak-integration","text":"","title":"WordPress + Keycloak Integration"},{"location":"tools/wordpress/integration/#introduction","text":"Integrating Keycloak with WordPress enhances security by implementing Single Sign-On (SSO). This integration allows users to access WordPress using Keycloak credentials, streamlining the login process and improving security.","title":"Introduction"},{"location":"tools/wordpress/integration/#step-1-integrating-keycloak-in-wordpress","text":"Access WordPress Admin : Log into wp-admin in WordPress using credentials defined in docker-compose ( WORDPRESS_EMAIL , WORDPRESS_PASSWORD ). Install OAuth SSO Plugin : Go to 'Plugins' > 'Add New'. Search for \"OAuth Single Sign On \u2013 SSO (OAuth Client)\" and click on \"Install Now\". Activate and Configure : After installation, click \"Activate\". You\u2019ll be redirected to the configuration page. Follow the steps in the \"Setup Wizard\". For more details on Keycloak integration, visit the Keycloak WordPress Integration Documentation .","title":"Step 1: Integrating Keycloak in WordPress"},{"location":"tools/wordpress/integration/#step-2-configuring-keycloak-with-wordpress","text":"Ensure you have a clientid and secret from Keycloak. Follow the instructions provided in the setup wizard to integrate these credentials into WordPress.","title":"Step 2: Configuring Keycloak with WordPress"},{"location":"tools/wordpress/integration/#wordpress-discourse-integration","text":"","title":"WordPress + Discourse Integration"},{"location":"tools/wordpress/integration/#introduction_1","text":"Integrating Discourse with WordPress allows seamless connectivity between WordPress posts and Discourse discussions. This integration enhances community engagement by linking WordPress content with Discourse forums.","title":"Introduction"},{"location":"tools/wordpress/integration/#step-1-setting-up-api-key-in-discourse","text":"Admin Access : Log into Discourse as an admin. Create API Key : Navigate to /admin/api/keys . Create an API Key (User Level: \"All Users\", Scope: \"Global\") and save it for later use in WordPress.","title":"Step 1: Setting Up API Key in Discourse"},{"location":"tools/wordpress/integration/#step-2-integrating-discourse-in-wordpress","text":"Access WordPress Admin : Log into wp-admin using docker-compose credentials ( WORDPRESS_EMAIL , WORDPRESS_PASSWORD ). Install WP Discourse Plugin : Go to 'Plugins' > 'Add New'. Search for \"WP Discourse\" and click \"Install Now\". Activate and Configure : After installation, click \"Activate\". A \"Discourse\" option will appear in the sidebar. Configure Plugin Settings : Set the 'Discourse URL'. Use the API Key created in Discourse. In the \"Publishing\" section, enable relevant options. In the \"Commenting\" tab, configure Discourse comments settings. In the \"Webhook\" tab, enable \"Sync Comment Data\". For more information on configuring the WP Discourse plugin, visit the Discourse WordPress Plugin Guide . Important Notes: Ensure to securely store and manage your API keys and credentials. Regularly update your integration plugins to maintain security and functionality. In case of any security concerns or compromised credentials, contact the respective support teams immediately.","title":"Step 2: Integrating Discourse in WordPress"},{"location":"tools/wordpress/examples/","text":"WordPress, Keycloak, and Discourse Integration Guide Introduction This guide details the process of deploying WordPress, Keycloak, and Discourse using Docker, and integrating them for a seamless user experience. This setup utilizes Keycloak for Single Sign-On (SSO) across WordPress and Discourse, enhancing security and streamlining user management. Prerequisites Docker and Docker Compose installed. Familiarity with Docker, YAML syntax, and basic network configurations. Access to a SMTP service for email functionalities. Docker Compose Configuration The provided docker-compose.yml file outlines the configuration for deploying WordPress, Keycloak, and Discourse. The structure includes: WordPress Service : Utilizes the bitnami/wordpress image, configured for web content management. MariaDB Service : A database for WordPress. Keycloak Database : PostgreSQL database dedicated to Keycloak. Keycloak Service : Manages SSO and user authentication. Discourse Services : Includes postgresqldiscourse , redis , discourse , and sidekiq for the forum platform. Deploying the Services Download or Clone the Docker Compose File : Obtain the provided docker-compose.yml . Configure Environment Variables : Replace placeholders in the file with actual values for SMTP, database credentials, and other settings. Run the Services : Execute docker-compose up in the directory containing the Docker Compose file. Access Services : WordPress: Accessible at http://localhost:9000 . Discourse: Accessible at http://localhost:3000 . Keycloak: Accessible at http://localhost:8080 . Integrating Keycloak with WordPress and Discourse WordPress + Keycloak Install OAuth SSO Plugin in WordPress : Access WordPress admin panel. Navigate to 'Plugins' > 'Add New'. Search and install \"OAuth Single Sign On \u2013 SSO (OAuth Client)\". Configure Keycloak Integration : Use Keycloak's clientid and secret . Follow the setup wizard in WordPress for Keycloak integration. Discourse + Keycloak Enable Keycloak SSO in Discourse : Access the Discourse container's command line. Install the OpenID Connect plugin and configure it to use Keycloak as the SSO provider. Configure API Key in Discourse : Generate an API key in Discourse admin panel for integration purposes. WordPress + Discourse Install WP Discourse Plugin in WordPress : Access WordPress admin panel. Navigate to 'Plugins' > 'Add New'. Search and install \"WP Discourse\". Configure Discourse Integration : Use the Discourse API key. Set up publishing and commenting settings in WordPress to synchronize with Discourse. Additional Configuration and Troubleshooting Email Setup : Ensure SMTP settings are correctly configured in both WordPress and Discourse for email functionalities. Security and Data Backup : Regularly update services and back up data. Troubleshooting : Consult the official documentation of WordPress, Keycloak, and Discourse for specific issues. Support and Additional Resources Community Forums : Engage with community forums for WordPress, Keycloak, and Discourse for support. External Documentation : Refer to the official documentation of each tool for detailed guides and updates. Note : Always ensure that you are working with the latest versions of the software and following the best practices for security and maintenance.","title":"WordPress, Keycloak, and Discourse Integration Guide"},{"location":"tools/wordpress/examples/#wordpress-keycloak-and-discourse-integration-guide","text":"","title":"WordPress, Keycloak, and Discourse Integration Guide"},{"location":"tools/wordpress/examples/#introduction","text":"This guide details the process of deploying WordPress, Keycloak, and Discourse using Docker, and integrating them for a seamless user experience. This setup utilizes Keycloak for Single Sign-On (SSO) across WordPress and Discourse, enhancing security and streamlining user management.","title":"Introduction"},{"location":"tools/wordpress/examples/#prerequisites","text":"Docker and Docker Compose installed. Familiarity with Docker, YAML syntax, and basic network configurations. Access to a SMTP service for email functionalities.","title":"Prerequisites"},{"location":"tools/wordpress/examples/#docker-compose-configuration","text":"The provided docker-compose.yml file outlines the configuration for deploying WordPress, Keycloak, and Discourse. The structure includes: WordPress Service : Utilizes the bitnami/wordpress image, configured for web content management. MariaDB Service : A database for WordPress. Keycloak Database : PostgreSQL database dedicated to Keycloak. Keycloak Service : Manages SSO and user authentication. Discourse Services : Includes postgresqldiscourse , redis , discourse , and sidekiq for the forum platform.","title":"Docker Compose Configuration"},{"location":"tools/wordpress/examples/#deploying-the-services","text":"Download or Clone the Docker Compose File : Obtain the provided docker-compose.yml . Configure Environment Variables : Replace placeholders in the file with actual values for SMTP, database credentials, and other settings. Run the Services : Execute docker-compose up in the directory containing the Docker Compose file. Access Services : WordPress: Accessible at http://localhost:9000 . Discourse: Accessible at http://localhost:3000 . Keycloak: Accessible at http://localhost:8080 .","title":"Deploying the Services"},{"location":"tools/wordpress/examples/#integrating-keycloak-with-wordpress-and-discourse","text":"","title":"Integrating Keycloak with WordPress and Discourse"},{"location":"tools/wordpress/examples/#wordpress-keycloak","text":"Install OAuth SSO Plugin in WordPress : Access WordPress admin panel. Navigate to 'Plugins' > 'Add New'. Search and install \"OAuth Single Sign On \u2013 SSO (OAuth Client)\". Configure Keycloak Integration : Use Keycloak's clientid and secret . Follow the setup wizard in WordPress for Keycloak integration.","title":"WordPress + Keycloak"},{"location":"tools/wordpress/examples/#discourse-keycloak","text":"Enable Keycloak SSO in Discourse : Access the Discourse container's command line. Install the OpenID Connect plugin and configure it to use Keycloak as the SSO provider. Configure API Key in Discourse : Generate an API key in Discourse admin panel for integration purposes.","title":"Discourse + Keycloak"},{"location":"tools/wordpress/examples/#wordpress-discourse","text":"Install WP Discourse Plugin in WordPress : Access WordPress admin panel. Navigate to 'Plugins' > 'Add New'. Search and install \"WP Discourse\". Configure Discourse Integration : Use the Discourse API key. Set up publishing and commenting settings in WordPress to synchronize with Discourse.","title":"WordPress + Discourse"},{"location":"tools/wordpress/examples/#additional-configuration-and-troubleshooting","text":"Email Setup : Ensure SMTP settings are correctly configured in both WordPress and Discourse for email functionalities. Security and Data Backup : Regularly update services and back up data. Troubleshooting : Consult the official documentation of WordPress, Keycloak, and Discourse for specific issues.","title":"Additional Configuration and Troubleshooting"},{"location":"tools/wordpress/examples/#support-and-additional-resources","text":"Community Forums : Engage with community forums for WordPress, Keycloak, and Discourse for support. External Documentation : Refer to the official documentation of each tool for detailed guides and updates. Note : Always ensure that you are working with the latest versions of the software and following the best practices for security and maintenance.","title":"Support and Additional Resources"}]}